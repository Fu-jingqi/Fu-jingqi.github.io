<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ResNet：残差网络</title>
    <link href="/2024/08/01/ResNet%EF%BC%9A%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/08/01/ResNet%EF%BC%9A%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h3 id="什么是resnet">什么是ResNet</h3><p>ResNet，即残差网络（ResidualNetwork），是一种深度学习架构，它通过引入残差学习框架解决了随着网络层数增加而训练难度增大的问题。ResNet由KaimingHe等人在2015年提出，并在多个视觉识别任务中取得了突破性的成绩。</p><p>Resnet在cnn图像方面有着非常突出的表现，它利用 shortcut短路连接，解决了深度网络中模型退化的问题。</p><p><imgsrc="https://pic.imgdb.cn/item/66ab71dbd9c307b7e94d77aa.png" /></p><h3 id="网络中的亮点">网络中的亮点：</h3><ul><li><p>超深的网络结构（突破1000层）</p></li><li><p>提出residual（残差结构）模块</p></li><li><p>使用Batch Normalization加速训练（丢弃dropout)</p></li></ul><h3 id="采用残差结构的原因">采用残差结构的原因</h3><p>在ResNet提出之前，所有的神经网络都是通过卷积层和池化层的叠加组成的。然而实验中发现并不是一味的堆叠卷积层和池化层，网络的识别效果就会有显著提升，此时会出现两种问题：</p><p><imgsrc="https://pic.imgdb.cn/item/66ab7222d9c307b7e94dab39.png" /></p><h4 id="梯度消失或爆炸的问题">梯度消失或爆炸的问题</h4><blockquote><p>梯度消失：若每一层的误差梯度小于1，反向传播时，网络越深，梯度越趋近于0梯度爆炸：若每一层的误差梯度大于1，反向传播时，网络越深，梯度越来越大</p></blockquote><h4 id="退化问题">退化问题</h4><p>随着层数的增加，预测效果反而越来越差.</p><p><imgsrc="https://pic.imgdb.cn/item/66ab724ad9c307b7e94dcca1.png" /></p><h4 id="解决方法">解决方法</h4><ul><li>梯度消失或梯度爆炸问题在数据的预处理以及在网络中使用 BN（BatchNormalization）层来解决。</li><li>深层网络中的退化问题通过残差网络 (ResNets)来减轻。ResNet论文提出了residual结构（残差结构）来<strong>减轻</strong>退化问题，下图是使用residual结构的卷积网络，可以看到随着网络的不断加深，效果并没有变差，而是变的更好了。（虚线是trainerror，实线是test error）</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66ab73f1d9c307b7e94f1a56.png" /></p><h3 id="batch-normalization">Batch Normalization</h3><p>Batch Normalization的目的是使我们的一批（Batch）featuremap满足均值为0，方差为1的分布规律。</p><p>我们在图像预处理过程中通常会对图像进行标准化处理，这样能够加速网络的收敛，如下图所示，对于Conv1来说输入的就是满足某一分布的特征矩阵，但对于Conv2而言输入的featuremap就不一定满足某一分布规律了（注意这里所说满足某一分布规律并不是指某一个featuremap的数据要满足分布规律，理论上是指整个训练样本集所对应featuremap的数据要满足分布规律）。而我们BatchNormalization的目的就是使我们的featuremap满足均值为0，方差为1的分布规律。</p><p><imgsrc="https://pic.imgdb.cn/item/66ab7f21d9c307b7e959dadb.png" /></p><p><imgsrc="https://pic.imgdb.cn/item/66ab7f7fd9c307b7e95a216e.png" /></p><h3 id="resnet的网络结构">ResNet的网络结构</h3><p>ResNet block有两种，一种左侧两层的 <strong>BasicBlock</strong>结构，一种是右侧三层的 <strong>Bottleneck</strong>结构，即将两个33的卷积层替换为<code>1 * 1+3 * 3 + 1 * 1</code>，它通过<code>1 * 1 conv</code>来巧妙地缩减或扩张<code>feature</code><code>map</code>维度，从而使得我们的3 * 3conv的filters数目不受上一层输入的影响，它的输出也不会影响到下一层。</p><p><imgsrc="https://pic.imgdb.cn/item/66ab74d9d9c307b7e94feabe.png" /></p><ul><li>左图参数：3 * 3 * 256 * 256 + 3 * 3 * 256 * 256 = 1179648</li><li>右图参数：1 * 1 * 256 * 64 + 3 * 3 * 64 * 64 + 1 * 1 * 64 * 256 =69,632</li><li>参数大大减小，搭建深层次网络时，采用三层的残差结构（bottleneck）。</li><li>先降后升为了主分支上输出的特征矩阵和捷径分支上输出的特征矩阵形状相同，以便进行加法操作。</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66ab7b65d9c307b7e956983d.png" /></p><p>分为实线和虚线分支的原因是：在<code>conv3_x</code>、<code>conv4_x</code>、<code>conv5_x</code>网络中的第一个残差块是虚线的，其余的是实线的，为了起到承上启下的作用，虚线块需要确保特征矩阵等规模主要有两个区别： * 主分支步距 + 1 * 子分支增加一个1 * 1的卷积核</p><h4 id="basicblock-结构"><strong>BasicBlock</strong> 结构</h4><p><imgsrc="https://pic.imgdb.cn/item/66ab7970d9c307b7e954fba2.png" /></p><ul><li>右图输入: [56,56,64]</li><li>实线: [56,56,64] -&gt; [28,28,128] -&gt; [28,28,128]</li><li>虚线: [28 , 28 , 128]</li><li>输出: [28 , 28 , 128]</li></ul><h4 id="bottleneck-结构"><strong>bottleneck</strong> 结构</h4><p><imgsrc="https://pic.imgdb.cn/item/66ab7970d9c307b7e954fb9e.png" /></p><ul><li>右图输入: [56,56,256]</li><li>实线: [56,56,256] -&gt; [56,56,128] -&gt; [28,28,128] -&gt;[28,28,512]</li><li>虚线: [28 , 28 , 512]</li><li>输出: [28 , 28 , 512]</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66ab7ea4d9c307b7e95977ff.png" /></p><h3 id="resnet模型构建">ResNet模型构建</h3><h4 id="basicblock">BasicBlock</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicBlock</span>(nn.Module):<br>    expansion = <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channel, out_channel, stride=<span class="hljs-number">1</span>, downsample=<span class="hljs-literal">None</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(BasicBlock, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,<br>                               kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.bn1 = nn.BatchNorm2d(out_channel)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,<br>                               kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.bn2 = nn.BatchNorm2d(out_channel)<br>        <span class="hljs-variable language_">self</span>.downsample = downsample<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        identity = x<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            identity = <span class="hljs-variable language_">self</span>.downsample(x)<br><br>        out = <span class="hljs-variable language_">self</span>.conv1(x)<br>        out = <span class="hljs-variable language_">self</span>.bn1(out)<br>        out = <span class="hljs-variable language_">self</span>.relu(out)<br><br>        out = <span class="hljs-variable language_">self</span>.conv2(out)<br>        out = <span class="hljs-variable language_">self</span>.bn2(out)<br><br>        out += identity<br>        out = <span class="hljs-variable language_">self</span>.relu(out)<br><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><h4 id="bottleneck">Bottleneck</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Bottleneck</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。</span><br><span class="hljs-string">    但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2，</span><br><span class="hljs-string">    这么做的好处是能够在top1上提升大概0.5%的准确率。</span><br><span class="hljs-string">    可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    expansion = <span class="hljs-number">4</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channel, out_channel, stride=<span class="hljs-number">1</span>, downsample=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 groups=<span class="hljs-number">1</span>, width_per_group=<span class="hljs-number">64</span></span>):<br>        <span class="hljs-built_in">super</span>(Bottleneck, <span class="hljs-variable language_">self</span>).__init__()<br><br>        width = <span class="hljs-built_in">int</span>(out_channel * (width_per_group / <span class="hljs-number">64.</span>)) * groups<br><br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,<br>                               kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># squeeze channels</span><br>        <span class="hljs-variable language_">self</span>.bn1 = nn.BatchNorm2d(width)<br>        <span class="hljs-comment"># -----------------------------------------</span><br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,<br>                               kernel_size=<span class="hljs-number">3</span>, stride=stride, bias=<span class="hljs-literal">False</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.bn2 = nn.BatchNorm2d(width)<br>        <span class="hljs-comment"># -----------------------------------------</span><br>        <span class="hljs-variable language_">self</span>.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*<span class="hljs-variable language_">self</span>.expansion,<br>                               kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)  <span class="hljs-comment"># unsqueeze channels</span><br>        <span class="hljs-variable language_">self</span>.bn3 = nn.BatchNorm2d(out_channel*<span class="hljs-variable language_">self</span>.expansion)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.downsample = downsample<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        identity = x<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            identity = <span class="hljs-variable language_">self</span>.downsample(x)<br><br>        out = <span class="hljs-variable language_">self</span>.conv1(x)<br>        out = <span class="hljs-variable language_">self</span>.bn1(out)<br>        out = <span class="hljs-variable language_">self</span>.relu(out)<br><br>        out = <span class="hljs-variable language_">self</span>.conv2(out)<br>        out = <span class="hljs-variable language_">self</span>.bn2(out)<br>        out = <span class="hljs-variable language_">self</span>.relu(out)<br><br>        out = <span class="hljs-variable language_">self</span>.conv3(out)<br>        out = <span class="hljs-variable language_">self</span>.bn3(out)<br><br>        out += identity<br>        out = <span class="hljs-variable language_">self</span>.relu(out)<br><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><h4 id="resnet">ResNet</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResNet</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 block,</span><br><span class="hljs-params">                 blocks_num,</span><br><span class="hljs-params">                 num_classes=<span class="hljs-number">1000</span>,</span><br><span class="hljs-params">                 include_top=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">                 groups=<span class="hljs-number">1</span>,</span><br><span class="hljs-params">                 width_per_group=<span class="hljs-number">64</span></span>):<br>        <span class="hljs-built_in">super</span>(ResNet, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.include_top = include_top<br>        <span class="hljs-variable language_">self</span>.in_channel = <span class="hljs-number">64</span><br><br>        <span class="hljs-variable language_">self</span>.groups = groups<br>        <span class="hljs-variable language_">self</span>.width_per_group = width_per_group<br><br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-variable language_">self</span>.in_channel, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>,<br>                               padding=<span class="hljs-number">3</span>, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.bn1 = nn.BatchNorm2d(<span class="hljs-variable language_">self</span>.in_channel)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.maxpool = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.layer1 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">64</span>, blocks_num[<span class="hljs-number">0</span>])<br>        <span class="hljs-variable language_">self</span>.layer2 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">128</span>, blocks_num[<span class="hljs-number">1</span>], stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.layer3 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">256</span>, blocks_num[<span class="hljs-number">2</span>], stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.layer4 = <span class="hljs-variable language_">self</span>._make_layer(block, <span class="hljs-number">512</span>, blocks_num[<span class="hljs-number">3</span>], stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.include_top:<br>            <span class="hljs-variable language_">self</span>.avgpool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># output size = (1, 1)</span><br>            <span class="hljs-variable language_">self</span>.fc = nn.Linear(<span class="hljs-number">512</span> * block.expansion, num_classes)<br><br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layer</span>(<span class="hljs-params">self, block, channel, block_num, stride=<span class="hljs-number">1</span></span>):<br>        downsample = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> <span class="hljs-variable language_">self</span>.in_channel != channel * block.expansion:<br>            downsample = nn.Sequential(<br>                nn.Conv2d(<span class="hljs-variable language_">self</span>.in_channel, channel * block.expansion, kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),<br>                nn.BatchNorm2d(channel * block.expansion))<br><br>        layers = []<br>        layers.append(block(<span class="hljs-variable language_">self</span>.in_channel,<br>                            channel,<br>                            downsample=downsample,<br>                            stride=stride,<br>                            groups=<span class="hljs-variable language_">self</span>.groups,<br>                            width_per_group=<span class="hljs-variable language_">self</span>.width_per_group))<br>        <span class="hljs-variable language_">self</span>.in_channel = channel * block.expansion<br><br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, block_num):<br>            layers.append(block(<span class="hljs-variable language_">self</span>.in_channel,<br>                                channel,<br>                                groups=<span class="hljs-variable language_">self</span>.groups,<br>                                width_per_group=<span class="hljs-variable language_">self</span>.width_per_group))<br><br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.conv1(x)<br>        x = <span class="hljs-variable language_">self</span>.bn1(x)<br>        x = <span class="hljs-variable language_">self</span>.relu(x)<br>        x = <span class="hljs-variable language_">self</span>.maxpool(x)<br><br>        x = <span class="hljs-variable language_">self</span>.layer1(x)<br>        x = <span class="hljs-variable language_">self</span>.layer2(x)<br>        x = <span class="hljs-variable language_">self</span>.layer3(x)<br>        x = <span class="hljs-variable language_">self</span>.layer4(x)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.include_top:<br>            x = <span class="hljs-variable language_">self</span>.avgpool(x)<br>            x = torch.flatten(x, <span class="hljs-number">1</span>)<br>            x = <span class="hljs-variable language_">self</span>.fc(x)<br><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LSTM：长短期记忆网络</title>
    <link href="/2024/08/01/LSTM%EF%BC%9A%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/08/01/LSTM%EF%BC%9A%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h3 id="长短期记忆网络-lstm">长短期记忆网络 LSTM</h3><p>长短期记忆网络（<code>Long Short-Term Memory</code>，简称<code>LSTM</code>）是一种特殊的递归神经网络（<code>RNN</code>），它被设计用来解决传统RNN在处理长序列数据时遇到的梯度消失或梯度爆炸问题。LSTM通过引入三个门控机制来控制信息的流动，从而能够学习到长期依赖关系。</p><p>LSTM的关键组成部分包括：</p><ol type="1"><li><p><strong>遗忘门（ForgetGate）</strong>：决定从上一个状态中丢弃哪些信息。遗忘门通过sigmoid函数输出0到1之间的值，表示保留信息的程度。</p></li><li><p><strong>输入门</strong>：由两个部分组成，一个是更新门（UpdateGate），它决定哪些新信息将被存储在单元状态中；另一个是候选状态（CandidateState），它通过tanh函数生成新的候选值，这些值将被加入到当前单元状态中。</p></li><li><p><strong>输出门（OutputGate）</strong>：决定下一个隐藏状态的值，这通常用于预测下一个时间步的输出。</p></li></ol><p><code>LSTM</code>单元的工作原理可以概括为以下几个步骤：</p><ul><li>遗忘门决定从细胞状态中丢弃哪些信息。</li><li>输入门的更新门决定哪些新信息将被加入到细胞状态中，而候选状态生成这些新信息。</li><li>更新后的细胞状态通过细胞状态向量和隐藏状态向量更新。</li><li>输出门决定隐藏状态的输出，这通常用于下一个时间步的预测。</li></ul><p><code>LSTM</code>因其能够处理长序列数据而广泛应用于自然语言处理、语音识别、时间序列预测等领域。</p><p>传统的循环神经网络（<code>RNN</code>）虽然建立了不同时刻隐藏层之间的关系，实现了记忆的效果，但只是基于前一时刻，是一种<code>short</code><code>memory</code>。为了实现长期记忆，RNN开始引入一种门电路结构来“记笔记”,从而实现长期记忆。</p><ul><li>我们先看RNN的平面结构，如下图所示：</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66aaffb4d9c307b7e9e3e7c1.png" /></p><ul><li>和RNN相比，<code>LSTM</code>增加了一条新的时间链，记录长期记忆（<code>Long Term Memory</code>），我们这里用<code>C</code>表示，同时它增加了两条链之间的关系，见下图所示</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66ab00a0d9c307b7e9e49d51.png" /></p><ul><li>新增加的链条<code>C</code>就是相当于记事本。以<code>t</code>时刻为例，和<code>RNN</code>相比,<code>t</code>时刻的隐层输入除了来自输入<spanclass="math inline">\(x_t\)</span>和<spanclass="math inline">\(s_{t-1}\)</span>,还包含记事<spanclass="math inline">\(c_t\)</span>的信息，如下图所示：</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66ab01a7d9c307b7e9e55d06.png" /></p><ul><li>让我们把<span class="math inline">\(S_t\)</span>和<spanclass="math inline">\(c_t\)</span>之间的关联放大看清楚：一条线拆成三条线：</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66ab0236d9c307b7e9e5c628.png" /></p><p><imgsrc="https://pic.imgdb.cn/item/66ab029ad9c307b7e9e61825.png" /></p><p>将删除和增加操作结合起来得到输出：</p><p><span class="math display">\[ c_t = f_1 * c_{t-1} + f2 \]</span></p><p>而<spanclass="math inline">\(c_t\)</span>除了会被向下传递，还会用来更新当前短期记忆<spanclass="math inline">\(s_t\)</span>,最后可以计算输出得到<spanclass="math inline">\(y_t\)</span>，同时保持短期记忆链<spanclass="math inline">\(s\)</span>和长期记忆链<spanclass="math inline">\(c\)</span>并相互更新.</p><p>聊聊LSTM的梯度消失与梯度爆炸 LSTM的梯度消失首先明确，真正意义上来说，LSTM是不会梯度消失的(解决了RNN的问题，所以为啥呢？)。</p><p>LSTM的梯度消失与MLP或者CNN中梯度消失不一样。MLP/CNN中不同层有不同的参数，梯度各算各的；RNN中同样的权重在各时间步共享参数，最终的梯度是各时间步梯度之和。即使梯度越传越弱，远距离的梯度接近消失，近距离的梯度还在，所以总的梯度之和不会消失。进一步说，LSTM的梯度由RNN结构的梯度和线性变换函数的梯度组成。即使RNN的梯度接近0，线性变换函数的梯度就是其斜率，是一个常数，LSTM的梯度趋向于一个常数，这就解决了梯度消失的问题。LSTM所谓的梯度消失：梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。</p><h3id="一个简单且熟悉的例子使用lstm预测大气臭氧浓度">一个简单且熟悉的例子：使用LSTM预测大气臭氧浓度</h3><ul><li>数据预处理</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> xgboost <span class="hljs-keyword">as</span> xgb<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader,Dataset<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split,GridSearchCV<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error,mean_absolute_error,r2_score<br><br>data=pd.read_csv(<span class="hljs-string">&quot;./datasets/datasets_beijing.csv&quot;</span>)<br>data[<span class="hljs-string">&quot;AQI&quot;</span>]=data[<span class="hljs-string">&quot;AQI&quot;</span>].apply(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">int</span>(<span class="hljs-built_in">str</span>(x).replace(<span class="hljs-string">&quot;—&quot;</span>,<span class="hljs-string">&quot;0&quot;</span>)))<br>data[<span class="hljs-string">&quot;CO&quot;</span>]=data[<span class="hljs-string">&quot;CO&quot;</span>].apply(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">float</span>(<span class="hljs-built_in">str</span>(x).replace(<span class="hljs-string">&quot;—&quot;</span>,<span class="hljs-string">&quot;0&quot;</span>)))<br>data[<span class="hljs-string">&quot;CO_24h&quot;</span>]=data[<span class="hljs-string">&quot;CO_24h&quot;</span>].apply(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">float</span>(<span class="hljs-built_in">str</span>(x).replace(<span class="hljs-string">&quot;—&quot;</span>,<span class="hljs-string">&quot;0&quot;</span>)))<br>data[<span class="hljs-string">&quot;NO2&quot;</span>]=data[<span class="hljs-string">&quot;NO2&quot;</span>].apply(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">float</span>(<span class="hljs-built_in">str</span>(x).replace(<span class="hljs-string">&quot;—&quot;</span>,<span class="hljs-string">&quot;0&quot;</span>)))<br>data[<span class="hljs-string">&quot;PM10&quot;</span>]=data[<span class="hljs-string">&quot;PM10&quot;</span>].apply(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">float</span>(<span class="hljs-built_in">str</span>(x).replace(<span class="hljs-string">&quot;—&quot;</span>,<span class="hljs-string">&quot;0&quot;</span>)))<br>data[<span class="hljs-string">&quot;PM2_5&quot;</span>]=data[<span class="hljs-string">&quot;PM2_5&quot;</span>].apply(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">float</span>(<span class="hljs-built_in">str</span>(x).replace(<span class="hljs-string">&quot;—&quot;</span>,<span class="hljs-string">&quot;0&quot;</span>)))<br>data[<span class="hljs-string">&quot;SO2&quot;</span>]=data[<span class="hljs-string">&quot;SO2&quot;</span>].apply(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">float</span>(<span class="hljs-built_in">str</span>(x).replace(<span class="hljs-string">&quot;—&quot;</span>,<span class="hljs-string">&quot;0&quot;</span>)))<br>data[<span class="hljs-string">&quot;O3&quot;</span>]=data[<span class="hljs-string">&quot;O3&quot;</span>].apply(<span class="hljs-keyword">lambda</span> x:<span class="hljs-built_in">float</span>(<span class="hljs-built_in">str</span>(x).replace(<span class="hljs-string">&quot;—&quot;</span>,<span class="hljs-string">&quot;0&quot;</span>)))<br><br>datasets=data.loc[:,[<span class="hljs-string">&#x27;AQI&#x27;</span>, <span class="hljs-string">&#x27;CO&#x27;</span>, <span class="hljs-string">&#x27;CO_24h&#x27;</span>, <span class="hljs-string">&#x27;Latitude&#x27;</span>, <span class="hljs-string">&#x27;Longitude&#x27;</span>, <span class="hljs-string">&#x27;NO2&#x27;</span>, <span class="hljs-string">&#x27;PM10&#x27;</span>, <span class="hljs-string">&#x27;PM2_5&#x27;</span>, <span class="hljs-string">&#x27;SO2&#x27;</span>,<span class="hljs-string">&#x27;O3&#x27;</span>]].values<br>datasets=StandardScaler().fit_transform(datasets)<br>datasets_lstm,labels=[],[]<br>n_window=<span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(data)-n_window)):<br>    data_temp=datasets[i:i+n_window,:-<span class="hljs-number">1</span>]<br>    data_temp_flatten=data_temp.flatten()<br>    datasets_lstm.append(data_temp)<br>    datasets_xgb.append(data_temp_flatten)<br>    labels.append(datasets[i+n_window,-<span class="hljs-number">1</span>])<br><br>datasets_lstm=np.array(datasets_lstm)<br>labels=np.array(labels)<br><span class="hljs-built_in">print</span>(datasets_lstm.shape,labels.shape)<br><br>datasets_lstm_train,datasets_lstm_test,labels_train,labels_test=train_test_split(datasets_lstm,labels,train_size=<span class="hljs-number">0.8</span>)<br><br></code></pre></td></tr></table></figure><ul><li>构造数据loader</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataGenerator</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,x,y</span>):<br>        <span class="hljs-built_in">super</span>(DataGenerator, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.x,<span class="hljs-variable language_">self</span>.y=x,y<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.y)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, item</span>):<br>        <span class="hljs-keyword">return</span> torch.FloatTensor(<span class="hljs-variable language_">self</span>.x[item,:,:]),torch.FloatTensor([<span class="hljs-variable language_">self</span>.y[item]])<br>        <br>train_loader=DataLoader(DataGenerator(x=datasets_lstm_train,y=labels_train),batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">True</span>)<br>val_loader=DataLoader(DataGenerator(x=datasets_lstm_test,y=labels_test),batch_size=<span class="hljs-number">128</span>,shuffle=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><ul><li>LSTM模型构建</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LstmModel</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(LstmModel, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.lstm1=nn.LSTM(input_size=<span class="hljs-number">9</span>,hidden_size=<span class="hljs-number">16</span>,bidirectional=<span class="hljs-literal">True</span>,batch_first=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.fc=nn.Sequential(<br>            nn.Linear(<span class="hljs-number">32</span>,<span class="hljs-number">128</span>),<br>            nn.LeakyReLU(),<br>            nn.Linear(<span class="hljs-number">128</span>,<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        outputs1,(h1,c1)=<span class="hljs-variable language_">self</span>.lstm1(x)<br>        out1=torch.cat([h1[<span class="hljs-number">0</span>,:,:],h1[<span class="hljs-number">1</span>,:,:]],dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.fc(out1)<br></code></pre></td></tr></table></figure><ul><li>训练模型以及可视化训练效果</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model_name</span>):<br>    device=torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br>    model=LstmModel().to(device)<br>    <br>    optimizer=optim.Adam(model.parameters(),lr=<span class="hljs-number">1e-5</span>)<br>    loss_func=nn.MSELoss()<br>    <br>    scheduler=optim.lr_scheduler.StepLR(optimizer,step_size=<span class="hljs-number">5</span>,gamma=<span class="hljs-number">0.9</span>)<br>    <br>    <span class="hljs-comment"># 训练准确率train_accs和训练损失train_losses</span><br>    train_accs,train_losses=[],[]<br>    val_accs,val_losses=[],[]<br>    min_val_loss=<span class="hljs-number">1000</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">30</span>):<br>        <br>        train_loss = train_one_epoch(model,train_loader,optimizer,scheduler,loss_func,device,epoch)<br>        <br>        val_loss=get_val_result(model,val_loader,loss_func,device)<br>        <br>        train_losses.append(train_loss)<br>        <br>        val_losses.append(val_loss)<br>        <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;model:<span class="hljs-subst">&#123;model_name&#125;</span>,epoch:<span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>,train loss:<span class="hljs-subst">&#123;train_loss&#125;</span>,val loss:<span class="hljs-subst">&#123;val_loss&#125;</span>&quot;</span>)<br>        <br>        <span class="hljs-keyword">if</span> val_loss&lt;min_val_loss:<br>            torch.save(model,<span class="hljs-string">f&quot;models/<span class="hljs-subst">&#123;model_name&#125;</span>_best.pth&quot;</span>)<br>            min_val_loss=val_loss<br>        <span class="hljs-keyword">if</span> (epoch+<span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:<br>            torch.save(model,<span class="hljs-string">f&quot;models/<span class="hljs-subst">&#123;model_name&#125;</span>_epoch<span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>.pth&quot;</span>)<br><br>    plot_acc_loss(train_losses,val_losses,model_name)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_one_epoch</span>(<span class="hljs-params">model,train_loader,optimizer,scheduler,loss_func,device,epoch</span>):<br>    <br>    model.train()<br>    <br>    data_tqdm=tqdm(train_loader)<br>    <br>    losses=[]<br>    <br>    <span class="hljs-keyword">for</span> batch,(x,y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_tqdm):<br>        <span class="hljs-comment"># x_train,y_train=x.to(device),y.to(device)</span><br>        output=model(x_train)<br>        loss=loss_func(output,y_train)<br>        losses.append(loss.item())<br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>        data_tqdm.set_description_str(<span class="hljs-string">f&quot;epoch:<span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>,batch:<span class="hljs-subst">&#123;batch+<span class="hljs-number">1</span>&#125;</span>,loss:<span class="hljs-subst">&#123;loss.item()&#125;</span>,lr:<span class="hljs-subst">&#123;scheduler.get_last_lr()[<span class="hljs-number">0</span>]:<span class="hljs-number">.7</span>f&#125;</span>&quot;</span>)<br>    scheduler.step()<br><br>    losses=<span class="hljs-built_in">float</span>(np.mean(losses))<br><br>    <span class="hljs-keyword">return</span> losses<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_val_result</span>(<span class="hljs-params">model,val_loader,loss_func,device</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    data_tqdm=tqdm(val_loader)<br>    losses=[]<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> x,y <span class="hljs-keyword">in</span> data_tqdm:<br>            x_val,y_val=x.to(device),y.to(device)<br>            output=model(x_val)<br>            loss=loss_func(output,y_val)<br>            losses.append(loss.item())<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">float</span>(np.mean(losses))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_acc_loss</span>(<span class="hljs-params">train_losses,val_losses,model_name</span>):<br>    plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">5</span>))<br>    plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-built_in">len</span>(train_losses)+<span class="hljs-number">1</span>),train_losses,<span class="hljs-string">&quot;r&quot;</span>,label=<span class="hljs-string">&quot;train&quot;</span>)<br>    plt.plot(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-built_in">len</span>(val_losses)+<span class="hljs-number">1</span>),val_losses,<span class="hljs-string">&quot;g&quot;</span>,label=<span class="hljs-string">&quot;val&quot;</span>)<br>    plt.title(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;model_name&#125;</span>_loss-epoch&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;epoch&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>)<br>    plt.legend()<br>    plt.savefig(<span class="hljs-string">f&quot;images/<span class="hljs-subst">&#123;model_name&#125;</span>_epoch_acc_loss.jpg&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>LSTM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN：循环神经网络</title>
    <link href="/2024/08/01/RNN%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/08/01/RNN%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h3 id="rnn-循环神经网络简介">RNN 循环神经网络简介</h3><p>RNN，即循环神经网络（Recurrent NeuralNetwork），是一种适合于序列数据的深度学习模型。RNN的核心特点是它能够处理序列中的前后依赖关系，即当前的输出会作为下一个时间步的输入。这种特性使得RNN在处理时间序列数据、自然语言处理等领域有着广泛的应用。</p><p>RNN的基本结构包括输入层、隐藏层和输出层。隐藏层的神经元会将前一时间步的输出作为当前时间步的输入，形成一种循环结构。RNN有几种变体，例如长短期记忆网络（LSTM）和门控循环单元（GRU），它们都是为了解决RNN在训练过程中可能出现的梯度消失或梯度爆炸问题。</p><h3 id="网络结构的不同之处">网络结构的不同之处</h3><ul><li>简单的神经网络如下图所示：</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66aaf589d9c307b7e9da5a7e.png" /></p><p>通常情况下，深度神经网络都是水平方向延伸的，隐层数量很多，但是没有考虑单个隐层在时间上（时序上）的变化；*而RNN则不同，它关注隐藏每个神经元在时间维度上的不断成长与进步，示意图如下图所示</p><p><imgsrc="https://pic.imgdb.cn/item/66aaf6a3d9c307b7e9db2cae.png" /></p><p>假如我们使用<spanclass="math inline">\(W_s\)</span>来表示层级间的权重矩阵，RNN会假定不同层之间的权重矩阵共享同一个<spanclass="math inline">\(W_s\)</span>，可以有效减小训练轮数</p><p><imgsrc="https://pic.imgdb.cn/item/66aaf7add9c307b7e9dbeb79.png" /></p><p>对于输出<span class="math inline">\(S\)</span>则有:</p><blockquote><p>神经元隐藏层输出矩阵形式：<span class="math inline">\(S = f(W_{in}X +b)\)</span></p></blockquote><blockquote><p>RNN输出矩阵形式：<span class="math inline">\(S_t = f(W_{in}X +W_sS_{t-1} + b)\)</span></p></blockquote><p><imgsrc="https://pic.imgdb.cn/item/66aaf8d2d9c307b7e9dcc6a0.png" /></p><p>RNN使得前后输出具有时序关系，即获得了记忆</p><h3 id="论文中的抽象结构是如何来的">论文中的抽象结构是如何来的</h3><h4 id="抽象结构-1">抽象结构 1</h4><p>在看资料时，我们能看到RNN的一种抽象结构（如下图右边所示），下面我们来看一下抽象结构如何生成的：</p><ul><li><p><imgsrc="https://pic.imgdb.cn/item/66aaf9dcd9c307b7e9dd9ba3.png" /></p></li><li><p><imgsrc="https://pic.imgdb.cn/item/66aafa05d9c307b7e9ddbe0e.png" /></p></li><li><p>上左图中红色输入代表上有图中的<code>X</code>，蓝色圈隐层代表上右图的<code>S</code>,绿色圈输出层代表上右图的<code>o</code>，把不同时间层级间对权重矩阵的更新化成一个大圈表示，就得到右边的简易示意图。</p></li></ul><h4 id="抽象结构-2">抽象结构 2</h4><p>在大量论文、博客中还能观察到另一种RNN结构，如下图所示：</p><p><imgsrc="https://pic.imgdb.cn/item/66aafb3ed9c307b7e9e028b7.png" /></p><p>下面我们来看一下这种抽象结构如何生成的：</p><ul><li><p><imgsrc="https://pic.imgdb.cn/item/66aafb6dd9c307b7e9e05905.png" /></p></li><li><p><imgsrc="https://pic.imgdb.cn/item/66aafb7fd9c307b7e9e06b93.png" /></p></li><li><p><imgsrc="https://pic.imgdb.cn/item/66aafb99d9c307b7e9e07fcf.png" /></p></li></ul><h3 id="rnn-的不足之处">RNN 的不足之处</h3><p>RNN自身的特性使得它天生就是一个记账小能手，它和传统的神经网络一样，也使用误差反向传播加梯度下降来更新权重，只不过在计算隐藏输出时，要引入之前不同时刻的数据，而缺陷在于记忆不够“久远”,通常超过十步就记不住了，而为了解决这个问题，人们提出了长短期记忆网络（<code>LSTM</code>）</p><h3 id="不同输入输出格式的-rnn">不同输入输出格式的 RNN</h3><p>由于时序上的层级结构，使得RNN在输入输出关系上有不同的类型，解决不同的问题。见下图：</p><p><imgsrc="https://pic.imgdb.cn/item/66aafd14d9c307b7e9e1bee3.png" /></p><ul><li><p>one to many:输入可以是一幅图片，但输出可以是一段话或<code>music</code>，看图说话</p></li><li><p>many to one:输入可以是一段话或<code>music</code>，但输出可以是一幅图片</p></li><li><p>n to n: 输入一段话，生成一段代码</p></li><li><p>n to m:</p></li></ul><p><imgsrc="https://pic.imgdb.cn/item/66aafe34d9c307b7e9e2bdc2.png" /></p>]]></content>
    
    
    <categories>
      
      <category>Deep_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GoogLeNet：含并行连结的网络</title>
    <link href="/2024/07/26/GoogLeNet%EF%BC%9A%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/07/26/GoogLeNet%EF%BC%9A%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h3 id="googlenet简介">GoogLeNet简介</h3><p>GoogLeNet在2014年由Google团队提出，斩获当年ImageNet竞赛中ClassificationTask (分类任务) 第一名。</p><h3 id="googlenet的优势">GoogLeNet的优势</h3><ul><li>引入了Inception结构（融合不同尺度的特征信息）</li><li>使用1x1的卷积核进行降维以及映射处理</li><li>添加两个辅助分类器帮助训练</li></ul><p><a href="https://imgse.com/i/pkbHyOU"><imgsrc="https://s21.ax1x.com/2024/07/26/pkbHyOU.png"alt="pkbHyOU.png" /></a></p><ul><li>丢弃全连接层，使用平均池化层（大大减少模型参数)</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66a35673d9c307b7e96112d9.png" /></p><p>常规串行(vgg，AlexNet....) 特殊并行(GoogLeNet)</p><ul><li>1 * 1卷积核</li></ul><p>1 * 1卷积核起到降维的作用,改变的是out_channel，不改变长宽</p><p><imgsrc="https://pic.imgdb.cn/item/66a88f66d9c307b7e9e8482f.png" /></p><p>特征矩阵的深度是由卷积核的个数决定的</p><ul><li>辅助分类器</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66a35a3fd9c307b7e966d22e.png" /></p><ul><li>Inception块</li></ul><p><imgsrc="https://pic.imgdb.cn/item/66a8d29ed9c307b7e9250158.png" /></p><ol type="1"><li>每个分支所得的特征矩阵高和宽必须相同。</li><li>使用不同大小窗口的卷积层，不用考虑卷积层参数（全都要）</li></ol><h3 id="googlenet模型构建">GoogLeNet模型构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># !/usr/bin/env python3</span><br><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># ********************************************************************************************************************</span><br><span class="hljs-comment">#       Created:     2024/07/30</span><br><span class="hljs-comment">#       Filename:    GoogLeNet_model.py</span><br><span class="hljs-comment">#       Email:       72110902110jq@gmail.com</span><br><span class="hljs-comment">#       Create By:   coderfjq</span><br><span class="hljs-comment">#       LastModify:  2024/07/30</span><br><span class="hljs-comment"># ********************************************************************************************************************</span><br><span class="hljs-comment"># This code sucks, you know it and I know it.  </span><br><span class="hljs-comment"># Move on and call me an idiot later.</span><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GoogLeNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">1000</span>, aux_logits=<span class="hljs-literal">True</span>, init_weights=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>(GoogLeNet, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.aux_logits = aux_logits<br><br>        <span class="hljs-variable language_">self</span>.conv1 = BasicConv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>)<br>        <span class="hljs-variable language_">self</span>.maxpool1 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-variable language_">self</span>.conv2 = BasicConv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.conv3 = BasicConv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.maxpool2 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-variable language_">self</span>.inception3a = Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, <span class="hljs-number">96</span>, <span class="hljs-number">128</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)<br>        <span class="hljs-variable language_">self</span>.inception3b = Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">192</span>, <span class="hljs-number">32</span>, <span class="hljs-number">96</span>, <span class="hljs-number">64</span>)<br>        <span class="hljs-variable language_">self</span>.maxpool3 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-variable language_">self</span>.inception4a = Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, <span class="hljs-number">96</span>, <span class="hljs-number">208</span>, <span class="hljs-number">16</span>, <span class="hljs-number">48</span>, <span class="hljs-number">64</span>)<br>        <span class="hljs-variable language_">self</span>.inception4b = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, <span class="hljs-number">112</span>, <span class="hljs-number">224</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        <span class="hljs-variable language_">self</span>.inception4c = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        <span class="hljs-variable language_">self</span>.inception4d = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, <span class="hljs-number">144</span>, <span class="hljs-number">288</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        <span class="hljs-variable language_">self</span>.inception4e = Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br>        <span class="hljs-variable language_">self</span>.maxpool4 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-variable language_">self</span>.inception5a = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br>        <span class="hljs-variable language_">self</span>.inception5b = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">48</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.aux_logits:<br>            <span class="hljs-variable language_">self</span>.aux1 = InceptionAux(<span class="hljs-number">512</span>, num_classes)<br>            <span class="hljs-variable language_">self</span>.aux2 = InceptionAux(<span class="hljs-number">528</span>, num_classes)<br><br>        <span class="hljs-variable language_">self</span>.avgpool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(<span class="hljs-number">0.4</span>)<br>        <span class="hljs-variable language_">self</span>.fc = nn.Linear(<span class="hljs-number">1024</span>, num_classes)<br>        <span class="hljs-keyword">if</span> init_weights:<br>            <span class="hljs-variable language_">self</span>._initialize_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># N x 3 x 224 x 224</span><br>        x = <span class="hljs-variable language_">self</span>.conv1(x)<br>        <span class="hljs-comment"># N x 64 x 112 x 112</span><br>        x = <span class="hljs-variable language_">self</span>.maxpool1(x)<br>        <span class="hljs-comment"># N x 64 x 56 x 56</span><br>        x = <span class="hljs-variable language_">self</span>.conv2(x)<br>        <span class="hljs-comment"># N x 64 x 56 x 56</span><br>        x = <span class="hljs-variable language_">self</span>.conv3(x)<br>        <span class="hljs-comment"># N x 192 x 56 x 56</span><br>        x = <span class="hljs-variable language_">self</span>.maxpool2(x)<br><br>        <span class="hljs-comment"># N x 192 x 28 x 28</span><br>        x = <span class="hljs-variable language_">self</span>.inception3a(x)<br>        <span class="hljs-comment"># N x 256 x 28 x 28</span><br>        x = <span class="hljs-variable language_">self</span>.inception3b(x)<br>        <span class="hljs-comment"># N x 480 x 28 x 28</span><br>        x = <span class="hljs-variable language_">self</span>.maxpool3(x)<br>        <span class="hljs-comment"># N x 480 x 14 x 14</span><br>        x = <span class="hljs-variable language_">self</span>.inception4a(x)<br>        <span class="hljs-comment"># N x 512 x 14 x 14</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.aux_logits:    <span class="hljs-comment"># eval model lose this layer</span><br>            aux1 = <span class="hljs-variable language_">self</span>.aux1(x)<br><br>        x = <span class="hljs-variable language_">self</span>.inception4b(x)<br>        <span class="hljs-comment"># N x 512 x 14 x 14</span><br>        x = <span class="hljs-variable language_">self</span>.inception4c(x)<br>        <span class="hljs-comment"># N x 512 x 14 x 14</span><br>        x = <span class="hljs-variable language_">self</span>.inception4d(x)<br>        <span class="hljs-comment"># N x 528 x 14 x 14</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.aux_logits:    <span class="hljs-comment"># eval model lose this layer</span><br>            aux2 = <span class="hljs-variable language_">self</span>.aux2(x)<br><br>        x = <span class="hljs-variable language_">self</span>.inception4e(x)<br>        <span class="hljs-comment"># N x 832 x 14 x 14</span><br>        x = <span class="hljs-variable language_">self</span>.maxpool4(x)<br>        <span class="hljs-comment"># N x 832 x 7 x 7</span><br>        x = <span class="hljs-variable language_">self</span>.inception5a(x)<br>        <span class="hljs-comment"># N x 832 x 7 x 7</span><br>        x = <span class="hljs-variable language_">self</span>.inception5b(x)<br>        <span class="hljs-comment"># N x 1024 x 7 x 7</span><br><br>        x = <span class="hljs-variable language_">self</span>.avgpool(x)<br>        <span class="hljs-comment"># N x 1024 x 1 x 1</span><br>        x = torch.flatten(x, <span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># N x 1024</span><br>        x = <span class="hljs-variable language_">self</span>.dropout(x)<br>        x = <span class="hljs-variable language_">self</span>.fc(x)<br>        <span class="hljs-comment"># N x 1000 (num_classes)</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.training <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.aux_logits:   <span class="hljs-comment"># eval model lose this layer</span><br>            <span class="hljs-keyword">return</span> x, aux2, aux1<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>                <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>                nn.init.normal_(m.weight, <span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>)<br>                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj</span>):<br>        <span class="hljs-built_in">super</span>(Inception, <span class="hljs-variable language_">self</span>).__init__()<br><br>        <span class="hljs-variable language_">self</span>.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-variable language_">self</span>.branch2 = nn.Sequential(<br>            BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="hljs-number">1</span>),<br>            BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)   <span class="hljs-comment"># 保证输出大小等于输入大小</span><br>        )<br><br>        <span class="hljs-variable language_">self</span>.branch3 = nn.Sequential(<br>            BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="hljs-number">1</span>),<br>            <span class="hljs-comment"># 在官方的实现中，其实是3x3的kernel并不是5x5，这里我也懒得改了，具体可以参考下面的issue</span><br>            <span class="hljs-comment"># Please see https://github.com/pytorch/vision/issues/906 for details.</span><br>            BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)   <span class="hljs-comment"># 保证输出大小等于输入大小</span><br>        )<br><br>        <span class="hljs-variable language_">self</span>.branch4 = nn.Sequential(<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>            BasicConv2d(in_channels, pool_proj, kernel_size=<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        branch1 = <span class="hljs-variable language_">self</span>.branch1(x)<br>        branch2 = <span class="hljs-variable language_">self</span>.branch2(x)<br>        branch3 = <span class="hljs-variable language_">self</span>.branch3(x)<br>        branch4 = <span class="hljs-variable language_">self</span>.branch4(x)<br><br>        outputs = [branch1, branch2, branch3, branch4]<br>        <span class="hljs-keyword">return</span> torch.cat(outputs, <span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 辅助分类器</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InceptionAux</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, num_classes</span>):<br>        <span class="hljs-built_in">super</span>(InceptionAux, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.averagePool = nn.AvgPool2d(kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">3</span>)<br>        <span class="hljs-variable language_">self</span>.conv = BasicConv2d(in_channels, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">1</span>)  <span class="hljs-comment"># output[batch, 128, 4, 4]</span><br><br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">2048</span>, <span class="hljs-number">1024</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">1024</span>, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14</span><br>        x = <span class="hljs-variable language_">self</span>.averagePool(x)<br>        <span class="hljs-comment"># aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4</span><br>        x = <span class="hljs-variable language_">self</span>.conv(x)<br>        <span class="hljs-comment"># N x 128 x 4 x 4</span><br>        x = torch.flatten(x, <span class="hljs-number">1</span>)<br>        x = F.dropout(x, <span class="hljs-number">0.5</span>, training=<span class="hljs-variable language_">self</span>.training)<br>        <span class="hljs-comment"># N x 2048</span><br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x), inplace=<span class="hljs-literal">True</span>)<br>        x = F.dropout(x, <span class="hljs-number">0.5</span>, training=<span class="hljs-variable language_">self</span>.training)<br>        <span class="hljs-comment"># N x 1024</span><br>        x = <span class="hljs-variable language_">self</span>.fc2(x)<br>        <span class="hljs-comment"># N x num_classes</span><br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicConv2d</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(BasicConv2d, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv = nn.Conv2d(in_channels, out_channels, **kwargs)<br>        <span class="hljs-variable language_">self</span>.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.conv(x)<br>        x = <span class="hljs-variable language_">self</span>.relu(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>GoogLeNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VGG：使用块的卷积神经网络</title>
    <link href="/2024/07/25/VGG%EF%BC%9A%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/07/25/VGG%EF%BC%9A%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h3 id="vgg-简介">VGG 简介</h3><p>VGG（Visual GeometryGroup）网络是一种卷积神经网络模型，由牛津大学的视觉几何组和谷歌DeepMind共同提出，它在2014年的ImageNet挑战赛中取得了优异的成绩。VGG网络以其简单而有效的结构而著称，其核心思想是通过堆叠多个小尺寸的卷积核（如3x3）来构建深层网络，从而减少模型的参数数量，同时保持了网络的深度和性能。</p><p><imgsrc="https://pic.imgdb.cn/item/66a8d016d9c307b7e922fa56.png" /></p><ul><li>conv的stride为1，padding为1</li><li>maxpool的size为2，stride为2</li></ul><h4 id="vgg16">VGG16</h4><p>VGG16包含13个卷积层和3个全连接层，因此得名“VGG16”。这些卷积层和全连接层都具有权重系数，而池化层不涉及权重，因此不计入权重层的总数。</p><h3 id="vgg-的优势">VGG 的优势</h3><ul><li>通过堆叠多个3x3的卷积核来替代大尺度卷积核(减少所需参数)</li></ul><p>论文中提到，可以通过堆叠两个3x3的卷积核替代5x5的卷积核，堆叠三个3x3的卷积核替代7x7的卷积核。（拥有相同的感受野）</p><h4 id="感受野">感受野</h4><p>在卷积神经网络中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野(receptivefield)。通俗的解释是，输出featuremap.上的一个单元对应输入层上的区域大小。</p><p><imgsrc="https://pic.imgdb.cn/item/66a8b815d9c307b7e90db6d5.png" /></p><p><imgsrc="https://pic.imgdb.cn/item/66a8bdfed9c307b7e9139c60.png" /></p><p>论文中提到，可以通过堆叠两个3x3的卷积核替代5x5的卷积核，堆叠三个3x3的卷积核替代7x7的卷积核。使用7x7卷积核所需参数，与堆叠三个3x3卷积核所需参数（假设输入输出channel为C)* 7×7×C×C=49C^2 * 3×3×C×C+3×3×C×C+3×3×C×C=27C^2</p><h3 id="vgg模型构建">VGG模型构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># !/usr/bin/env python3</span><br><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># ********************************************************************************************************************</span><br><span class="hljs-comment">#       Created:     2024/07/30</span><br><span class="hljs-comment">#       Filename:    GoogLeNet_model.py</span><br><span class="hljs-comment">#       Email:       72110902110jq@gmail.com</span><br><span class="hljs-comment">#       Create By:   coderfjq</span><br><span class="hljs-comment">#       LastModify:  2024/07/30</span><br><span class="hljs-comment"># ********************************************************************************************************************</span><br><span class="hljs-comment"># This code sucks, you know it and I know it.  </span><br><span class="hljs-comment"># Move on and call me an idiot later.</span><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># official pretrain weights</span><br>model_urls = &#123;<br>    <span class="hljs-string">&#x27;vgg11&#x27;</span>: <span class="hljs-string">&#x27;https://download.pytorch.org/models/vgg11-bbd30ac9.pth&#x27;</span>,<br>    <span class="hljs-string">&#x27;vgg13&#x27;</span>: <span class="hljs-string">&#x27;https://download.pytorch.org/models/vgg13-c768596a.pth&#x27;</span>,<br>    <span class="hljs-string">&#x27;vgg16&#x27;</span>: <span class="hljs-string">&#x27;https://download.pytorch.org/models/vgg16-397923af.pth&#x27;</span>,<br>    <span class="hljs-string">&#x27;vgg19&#x27;</span>: <span class="hljs-string">&#x27;https://download.pytorch.org/models/vgg19-dcbb9e9d.pth&#x27;</span><br>&#125;<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VGG</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, features, num_classes=<span class="hljs-number">1000</span>, init_weights=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>(VGG, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.features = features<br>        <span class="hljs-variable language_">self</span>.classifier = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">512</span>*<span class="hljs-number">7</span>*<span class="hljs-number">7</span>, <span class="hljs-number">4096</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>            nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>),<br>            nn.ReLU(<span class="hljs-literal">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>            nn.Linear(<span class="hljs-number">4096</span>, num_classes)<br>        )<br>        <span class="hljs-keyword">if</span> init_weights:<br>            <span class="hljs-variable language_">self</span>._initialize_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># N x 3 x 224 x 224</span><br>        x = <span class="hljs-variable language_">self</span>.features(x)<br>        <span class="hljs-comment"># N x 512 x 7 x 7</span><br>        x = torch.flatten(x, start_dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># N x 512*7*7</span><br>        x = <span class="hljs-variable language_">self</span>.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                <span class="hljs-comment"># nn.init.kaiming_normal_(m.weight, mode=&#x27;fan_out&#x27;, nonlinearity=&#x27;relu&#x27;)</span><br>                nn.init.xavier_uniform_(m.weight)<br>                <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>                nn.init.xavier_uniform_(m.weight)<br>                <span class="hljs-comment"># nn.init.normal_(m.weight, 0, 0.01)</span><br>                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_features</span>(<span class="hljs-params">cfg: <span class="hljs-built_in">list</span></span>):<br>    layers = []<br>    in_channels = <span class="hljs-number">3</span><br>    <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> cfg:<br>        <span class="hljs-keyword">if</span> v == <span class="hljs-string">&quot;M&quot;</span>:<br>            layers += [nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)]<br>        <span class="hljs-keyword">else</span>:<br>            conv2d = nn.Conv2d(in_channels, v, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>            layers += [conv2d, nn.ReLU(<span class="hljs-literal">True</span>)]<br>            in_channels = v<br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br><br><br>cfgs = &#123;<br>    <span class="hljs-string">&#x27;vgg11&#x27;</span>: [<span class="hljs-number">64</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">128</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>],<br>    <span class="hljs-string">&#x27;vgg13&#x27;</span>: [<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>],<br>    <span class="hljs-string">&#x27;vgg16&#x27;</span>: [<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>],<br>    <span class="hljs-string">&#x27;vgg19&#x27;</span>: [<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>],<br>&#125;<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vgg</span>(<span class="hljs-params">model_name=<span class="hljs-string">&quot;vgg16&quot;</span>, **kwargs</span>):<br>    <span class="hljs-keyword">assert</span> model_name <span class="hljs-keyword">in</span> cfgs, <span class="hljs-string">&quot;Warning: model number &#123;&#125; not in cfgs dict!&quot;</span>.<span class="hljs-built_in">format</span>(model_name)<br>    cfg = cfgs[model_name]<br><br>    model = VGG(make_features(cfg), **kwargs)<br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>VGG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AlexNet：深度卷积神经网络</title>
    <link href="/2024/07/25/AlexNet%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/07/25/AlexNet%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h3 id="alexnet简介">AlexNet简介</h3><p>AlexNet是2012年ISLVRC 2012（ImageNet Large Scale Visual RecognitionChallenge）竞赛的冠军网络，分类准确率由传统的 70%+提升到80%+。它是由Hinton和他的学生AlexKrizhevsky设计的。也是在那年之后，深度学习开始迅速发展。</p><p><a href="https://imgse.com/i/pkbwmVJ"><imgsrc="https://s21.ax1x.com/2024/07/25/pkbwmVJ.png"alt="pkbwmVJ.png" /></a></p><h3 id="alexnet的优势">AlexNet的优势</h3><ul><li><p>论文中采用多个GPU并行训练数据，加快了训练过程</p></li><li><p>使用了 ReLU 激活函数，而不是传统的 Sigmoid 激活函数以及 Tanh激活函数。</p></li><li><p>局部响应归一化（Local Response Normalization, LRN）</p></li></ul><p>首先，局部响应归一化是一种在卷积神经网络中常用的技术，目的是在网络的某些层中减少神经元之间的干扰，从而提高网络的泛化能力。这种技术通过规范化神经元的输出来实现。</p><p>局部响应归一化(Local Response Normalization,LRN)的公式可以表示为：</p><p><span class="math display">\[ a_i = \frac{a_i}{(k + \alpha\sum_{j=\max(0, i - n/2)}^{\min(N-1, i + n/2)} a_j^2)^\beta)}\]</span></p><p>其中： - $ a_i $ 是第 $ i $ 个神经元的输出。 - $ N $ 是神经元的数量。- $ n $ 是归一化的窗口大小，即考虑 $ i $ 周围 $ n/2 $ 个神经元的范围。 -$ k $ 是一个常数，用于控制归一化过程中的偏移量。 - $ $是一个缩放因子，用于控制局部响应的强度。 - $ $是一个指数，用于控制归一化的影响范围，通常取值在0.5到1之间。</p><p>LRN的目的是减少相邻神经元之间的相互影响，通过这种方式，每个神经元的输出都会根据其周围神经元的输出强度进行调整。这有助于网络在训练过程中更加专注于特定的特征，从而提高其泛化能力。</p><p>解释如下： 1. <strong>局部范围</strong>：公式中的求和是从 $ i =max(0, r - n/2) $ 到 $ i = min(N-1, r + n/2) $。这表示在位置 $ (r, c) $处，考虑 $ n $个相邻的卷积核的输出。这样做的目的是使每个神经元的输出不仅依赖于其自身的输入，还受到周围神经元输出的影响。</p><ol start="2" type="1"><li><p><strong>归一化</strong>：通过将原始活动值 $ a_{r,c}^{y} $除以一个归一化因子（$ k + a_{r,c}^{i}$），使得输出值被规范化。这个归一化因子是所有相邻卷积核输出的加权和，其中权重由$ $ 控制。</p></li><li><p><strong>超参数</strong>：$ $, $ $, 和 $ k $ 是超参数。$ $决定了相邻神经元输出的影响程度，$ $ 决定了归一化因子的非线性程度，$ k $则提供了一个基础值，防止除数过小。</p></li><li><p><strong>效果</strong>：论文中提到，这种局部响应归一化可以减少模型的误差率。例如，他们在ImageNet 数据集上使用这种归一化方法，将 top-1 和 top-5的错误率分别降低了 1.4% 和 1.2%。</p></li><li><p><strong>应用时机</strong>：论文中提到这种归一化是在应用 ReLU激活函数之后进行的，并且是在某些层中应用，具体见论文的第 3.5节。</p></li></ol><ul><li>Dropout</li></ul><p>使用 Dropout 的方式在网络正向传播过程中随机失活一部分神经元：</p><p><imgsrc="https://pic.imgdb.cn/item/66a25906d9c307b7e9867a12.png" /></p><h3 id="alexnet卷积过程">AlexNet卷积过程</h3><blockquote><p>经卷积后的矩阵尺寸大小计算公式为： N = (W − F + 2P ) / S + 1</p></blockquote><ol type="1"><li>输入图片大小 W×W</li><li>Filter大小 F×F</li><li>步长 S</li><li>padding的像素数 P</li></ol><p><a href="https://imgse.com/i/pkbwtVH"><imgsrc="https://s21.ax1x.com/2024/07/25/pkbwtVH.png"alt="pkbwtVH.png" /></a></p><blockquote><p><code>Cov 1</code> :</p></blockquote><ul><li><code>kernels</code>: 48 * 2 = 96</li><li><code>kernel_size</code>: 11</li><li><code>padding</code>: [1, 2]（左边上边补充一列padding，右边下边补充两列padding）</li><li><code>stride</code>: 4</li></ul><p><strong>N = (W − F + 2P ) / S + 1 = (224 - 11 + 1 + 2) / 4 + 1 =55</strong></p><blockquote><p>input_size = [224,224,3] output_size = [55,55,96]</p></blockquote><p><a href="https://imgse.com/i/pkbwhR0"><imgsrc="https://s21.ax1x.com/2024/07/25/pkbwhR0.png"alt="pkbwhR0.png" /></a></p><blockquote><p><code>Maxpool 1</code> :</p></blockquote><ul><li><code>kernels</code>: 3</li><li><code>padding</code>: 0</li><li><code>stride</code>: 2</li></ul><p><strong>该池化层使得宽高减半，通道数不变</strong></p><blockquote><p>input_size = [55,55,96] output_size = [27,27,96]</p></blockquote><p><a href="https://imgse.com/i/pkbwHZ4"><imgsrc="https://s21.ax1x.com/2024/07/25/pkbwHZ4.png"alt="pkbwHZ4.png" /></a></p><blockquote><p>Cov 2 :</p></blockquote><ul><li><code>kernels</code>: 128 * 2 = 256</li><li><code>kernel_size</code>: 5</li><li><code>padding</code>: [2, 2]</li><li><code>stride</code>: 1</li></ul><p><strong>N = (W − F + 2P ) / S + 1 = (27 - 5 + 2 * 2) / 1 + 1 =27</strong></p><blockquote><p>input_size = [27,27,96] output_size = [27,27,256]</p></blockquote><p><a href="https://imgse.com/i/pkbwbdJ"><imgsrc="https://s21.ax1x.com/2024/07/25/pkbwbdJ.png"alt="pkbwbdJ.png" /></a></p><blockquote><p><code>Maxpool 2</code> :</p></blockquote><ul><li><code>kernels</code>: 3</li><li><code>padding</code>: 0</li><li><code>stride</code>: 2</li></ul><p><strong>该池化层使得宽高减半，通道数不变</strong></p><blockquote><p>input_size = [27,27,256] output_size = [13,13,256]</p></blockquote><p><a href="https://imgse.com/i/pkbwOiR"><imgsrc="https://s21.ax1x.com/2024/07/25/pkbwOiR.png"alt="pkbwOiR.png" /></a></p><blockquote><p><code>Cov 3</code> :</p></blockquote><ul><li><code>kernels</code>: 192 * 2 = 384</li><li><code>kernel_size</code>: 3</li><li><code>padding</code>: [1, 1]</li><li><code>stride</code>: 1</li></ul><p><strong>N = (W − F + 2P ) / S + 1 = (13 - 3 + 2 * 1) / 1 + 1 =13</strong></p><blockquote><p>input_size = [13,13,256] output_size = [13,13,384]</p></blockquote><p><imgsrc="https://pic.imgdb.cn/item/66a25670d9c307b7e9843a59.png" /></p><blockquote><p><code>Cov 4</code> :</p></blockquote><ul><li><code>kernels</code>: 192 * 2 = 384</li><li><code>kernel_size</code>: 3</li><li><code>padding</code>: [1, 1]</li><li><code>stride</code>: 1</li></ul><p><strong>N = (W − F + 2P ) / S + 1 = (13 - 3 + 2 * 1) / 1 + 1 =13</strong></p><blockquote><p>input_size = [13,13,384] output_size = [13,13,384]</p></blockquote><p><imgsrc="https://pic.imgdb.cn/item/66a256fdd9c307b7e984bd13.png" /></p><blockquote><p><code>Cov 5</code> :</p></blockquote><ul><li><code>kernels</code>: 128 * 2 = 256</li><li><code>kernel_size</code>: 3</li><li><code>padding</code>: [1, 1]</li><li><code>stride</code>: 1</li></ul><p><strong>N = (W − F + 2P ) / S + 1 = (13 - 3 + 2 * 1) / 1 + 1 =13</strong></p><blockquote><p>input_size = [13,13,384] output_size = [13,13,256]</p></blockquote><p><imgsrc="https://pic.imgdb.cn/item/66a25760d9c307b7e9850a81.png" /></p><blockquote><p><code>Maxpool 3</code> :</p></blockquote><ul><li><code>kernels</code>: 3</li><li><code>padding</code>: 0</li><li><code>stride</code>: 2</li></ul><p><strong>该池化层使得宽高减半，通道数不变</strong></p><blockquote><p>input_size = [13,13,256] output_size = [6,6,256]</p></blockquote><h3 id="alexnet模型构建">AlexNet模型构建</h3><p><imgsrc="https://pic.imgdb.cn/item/66af2653d9c307b7e93e973b.png" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># !/usr/bin/env python3</span><br><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># ********************************************************************************************************************</span><br><span class="hljs-comment">#       Created:     2024/07/30</span><br><span class="hljs-comment">#       Filename:    GoogLeNet_model.py</span><br><span class="hljs-comment">#       Email:       72110902110jq@gmail.com</span><br><span class="hljs-comment">#       Create By:   coderfjq</span><br><span class="hljs-comment">#       LastModify:  2024/07/30</span><br><span class="hljs-comment"># ********************************************************************************************************************</span><br><span class="hljs-comment"># This code sucks, you know it and I know it.  </span><br><span class="hljs-comment"># Move on and call me an idiot later.</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AlexNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">1000</span>, init_weights=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>(AlexNet, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.features = nn.Sequential(<br>            <span class="hljs-comment"># nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2),</span><br>            <span class="hljs-comment"># --------------------------------</span><br>            nn.Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">48</span>, kernel_size=<span class="hljs-number">11</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">2</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>            <span class="hljs-comment"># --------------------------------</span><br>            nn.Conv2d(in_channels=<span class="hljs-number">48</span>, out_channels=<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>            <span class="hljs-comment"># --------------------------------</span><br>            nn.Conv2d(in_channels=<span class="hljs-number">128</span>, out_channels=<span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            <span class="hljs-comment"># --------------------------------</span><br>            nn.Conv2d(in_channels=<span class="hljs-number">192</span>, out_channels=<span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            <span class="hljs-comment"># --------------------------------</span><br>            nn.Conv2d(in_channels=<span class="hljs-number">192</span>, out_channels=<span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>        )<br><br>        <span class="hljs-variable language_">self</span>.classifier = nn.Sequential(<br>            nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>            nn.Linear(<span class="hljs-number">128</span> * <span class="hljs-number">6</span> * <span class="hljs-number">6</span>,<span class="hljs-number">2048</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>            nn.Linear(<span class="hljs-number">2048</span>, <span class="hljs-number">2048</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br><br>            nn.Linear(<span class="hljs-number">2048</span>,num_classes)<br>        )<br><br>        <span class="hljs-keyword">if</span> init_weights:<br>            <span class="hljs-variable language_">self</span>._initialize_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = <span class="hljs-variable language_">self</span>.features(x)<br>        x = torch.flatten(x, start_dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># x = x.view(x.shape[0], -1)</span><br>        <span class="hljs-comment"># x = x.reshape(-1, 6 * 6 * 128)</span><br>        x = <span class="hljs-variable language_">self</span>.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>                <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>                nn.init.normal_(m.weight, <span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>)<br>                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h4 id="alxenet模型训练">AlxeNet模型训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> 深度学习基础学习.day05.AlexNet_model <span class="hljs-keyword">import</span> AlexNet<br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms, datasets, utils<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> dataloader, DataLoader<br><br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;using &#123;&#125; device.&quot;</span>.<span class="hljs-built_in">format</span>(device))<br><br>data_transform = &#123;<br><br>    <span class="hljs-string">&quot;train&quot;</span>: transforms.Compose([<br>        <span class="hljs-comment"># 增大训练集数量，实现数据增强</span><br>        <span class="hljs-comment"># RandomResizedCrop :随机裁剪，像素大小为224 * 224</span><br>        <span class="hljs-comment"># RandomHorizontalFlip :水平方向随机翻转</span><br>        transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<br>        transforms.RandomHorizontalFlip(),<br>        transforms.ToTensor(),<br>        transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))<br>        ]),<br><br>    <span class="hljs-string">&quot;val&quot;</span>: transforms.Compose([<br>        transforms.Resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),  <span class="hljs-comment"># cannot 224, must (224, 224)</span><br>        transforms.ToTensor(),<br>        transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))<br>        ])<br>&#125;<br>data_root = os.path.abspath(os.getcwd())  <span class="hljs-comment"># get data root path</span><br>image_path = os.path.join(data_root, <span class="hljs-string">&#x27;data_set&#x27;</span>,<span class="hljs-string">&quot;flower_data&quot;</span>)  <span class="hljs-comment"># flower data set path</span><br><span class="hljs-keyword">assert</span> os.path.exists(image_path), <span class="hljs-string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="hljs-built_in">format</span>(image_path)<br><br>train_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="hljs-string">&quot;train&quot;</span>),<br>                                     transform=data_transform[<span class="hljs-string">&quot;train&quot;</span>])<br>validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="hljs-string">&quot;val&quot;</span>),<br>                                  transform=data_transform[<span class="hljs-string">&quot;val&quot;</span>])<br>batch_size = <span class="hljs-number">32</span><br><br>train_loader = DataLoader(<br>    train_dataset,<br>    batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>,<br>     num_workers=<span class="hljs-number">0</span>)<br><br>validate_loader = DataLoader(<br>    validate_dataset,<br>    batch_size=<span class="hljs-number">4</span>, shuffle=<span class="hljs-literal">False</span>,<br>    num_workers=<span class="hljs-number">0</span>)<br><br>train_num = <span class="hljs-built_in">len</span>(train_dataset)<br>val_num = <span class="hljs-built_in">len</span>(validate_dataset)<br><span class="hljs-comment"># &#123;&#x27;daisy&#x27;:0, &#x27;dandelion&#x27;:1, &#x27;roses&#x27;:2, &#x27;sunflower&#x27;:3, &#x27;tulips&#x27;:4&#125;</span><br>flower_list = train_dataset.class_to_idx<br>cla_dict = <span class="hljs-built_in">dict</span>((val, key) <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> flower_list.items())<br><span class="hljs-comment"># write dict into json file</span><br>json_str = json.dumps(cla_dict, indent=<span class="hljs-number">4</span>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;class_indices.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> json_file:<br>    json_file.write(json_str)<br><br><span class="hljs-comment"># nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8])  # number of workers</span><br><span class="hljs-comment"># print(&#x27;Using &#123;&#125; dataloader workers every process&#x27;.format(nw))</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;using &#123;&#125; images for training, &#123;&#125; images for validation.&quot;</span>.<span class="hljs-built_in">format</span>(train_num,<br>                                                                       val_num))<br><span class="hljs-comment"># test_data_iter = iter(validate_loader)</span><br><span class="hljs-comment"># test_image, test_label = next(test_data_iter)</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># def imshow(img):</span><br><span class="hljs-comment">#     img = img / 2 + 0.5  # unnormalize</span><br><span class="hljs-comment">#     npimg = img.numpy()</span><br><span class="hljs-comment">#     plt.imshow(np.transpose(npimg, (1, 2, 0)))</span><br><span class="hljs-comment">#     plt.show()</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># print(&#x27; &#x27;.join(&#x27;%5s&#x27; % cla_dict[test_label[j].item()] for j in range(4)))</span><br><span class="hljs-comment"># imshow(utils.make_grid(test_image))</span><br><br>net = AlexNet(num_classes=<span class="hljs-number">5</span>, init_weights=<span class="hljs-literal">True</span>)<br>net.to(device)<br>loss_function = nn.CrossEntropyLoss()<br><span class="hljs-comment"># pata = list(net.parameters())</span><br>optimizer = optim.Adam(net.parameters(), lr=<span class="hljs-number">0.0002</span>)<br><br>epochs = <span class="hljs-number">10</span><br>save_path = <span class="hljs-string">&#x27;./AlexNet.pth&#x27;</span><br>best_acc = <span class="hljs-number">0.0</span><br>train_steps = <span class="hljs-built_in">len</span>(train_loader)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-comment"># train</span><br>    net.train()  <span class="hljs-comment"># 启用Dropout方法</span><br>    running_loss = <span class="hljs-number">0.0</span><br>    train_bar = tqdm(train_loader, file=sys.stdout)<br>    <span class="hljs-keyword">for</span> step, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_bar):<br>        images, labels = data<br>        optimizer.zero_grad()<br>        outputs = net(images.to(device))<br>        loss = loss_function(outputs, labels.to(device))<br>        loss.backward()<br>        optimizer.step()<br>        <span class="hljs-comment"># print statistics</span><br>        running_loss += loss.item()<br><br>        train_bar.desc = <span class="hljs-string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch + <span class="hljs-number">1</span>,<br>                                                                 epochs,<br>                                                                 loss)<br>    <span class="hljs-comment"># validate</span><br>    net.<span class="hljs-built_in">eval</span>()<br>    acc = <span class="hljs-number">0.0</span>  <span class="hljs-comment"># accumulate accurate number / epoch</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        val_bar = tqdm(validate_loader, file=sys.stdout)<br>        <span class="hljs-keyword">for</span> val_data <span class="hljs-keyword">in</span> val_bar:<br>            val_images, val_labels = val_data<br>            outputs = net(val_images.to(device))<br>            predict_y = torch.<span class="hljs-built_in">max</span>(outputs, dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]<br>            acc += torch.eq(predict_y, val_labels.to(device)).<span class="hljs-built_in">sum</span>().item()<br><br>    val_accurate = acc / val_num<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %<br>          (epoch + <span class="hljs-number">1</span>, running_loss / train_steps, val_accurate))<br><br>    <span class="hljs-keyword">if</span> val_accurate &gt; best_acc:<br>        best_acc = val_accurate<br>        torch.save(net.state_dict(), save_path)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished Training&#x27;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>AlexNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LeNet：第一个卷积神经网络</title>
    <link href="/2024/07/25/LeNet%EF%BC%9A%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/07/25/LeNet%EF%BC%9A%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="lenet官网demo示例">LeNet官网Demo示例</h2><p>比较基础，不多赘述~~</p><h3 id="模型构建">模型构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LeNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(LeNet,<span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 输入特征层的channel（in_channels）与卷积核的channel相同：就是说卷积核中有几个矩阵与输入特征的channel数量保持一致</span><br>        <span class="hljs-comment"># 输出的特征矩阵channel（out_channels）与卷积核个数相同</span><br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(in_channels=<span class="hljs-number">3</span>,out_channels=<span class="hljs-number">16</span>,kernel_size=<span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.pool1 = nn.MaxPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(in_channels=<span class="hljs-number">16</span>,out_channels=<span class="hljs-number">32</span>,kernel_size=<span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.pool2 = nn.MaxPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 经卷积后的矩阵尺寸大小：N = (W - F + 2 * P) / S + 1</span><br>        <span class="hljs-comment"># 输入图片大小： W * W</span><br>        <span class="hljs-comment"># FilterSize大小为 F * F</span><br>        <span class="hljs-comment"># 步长为 S</span><br>        <span class="hljs-comment"># padding 的像素数为 P</span><br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>,<span class="hljs-number">120</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">120</span>,<span class="hljs-number">84</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(<span class="hljs-number">84</span>,<span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = F.relu(<span class="hljs-variable language_">self</span>.conv1(x))<br>        x = <span class="hljs-variable language_">self</span>.pool1(x)<br>        x = F.relu(<span class="hljs-variable language_">self</span>.conv2(x))<br>        x = <span class="hljs-variable language_">self</span>.pool2(x)<br>        x = x.view(-<span class="hljs-number">1</span>,<span class="hljs-number">32</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>)<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h3 id="模型训练">模型训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># !/usr/bin/env python3</span><br><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># ********************************************************************************************************************</span><br><span class="hljs-comment">#       Created:     2024/07/24</span><br><span class="hljs-comment">#       Filename:    train.py</span><br><span class="hljs-comment">#       Author:        ___     ___     ___     ___     ___      ___       _   ___</span><br><span class="hljs-comment">#                     / __|   / _ \   |   \   | __|   | _ \    | __|   _ | | / _ \</span><br><span class="hljs-comment">#                    | (__   | (_) |  | |) |  | _|    |   /    | _|   | || || (_) |</span><br><span class="hljs-comment">#                     \___|   \___/   |___/   |___|   |_|_\   _|_|_   _\__/  \__\_\</span><br><span class="hljs-comment">#                   _|&quot;&quot;&quot;&quot;&quot;|_|&quot;&quot;&quot;&quot;&quot;|_|&quot;&quot;&quot;&quot;&quot;|_|&quot;&quot;&quot;&quot;&quot;|_|&quot;&quot;&quot;&quot;&quot;|_| &quot;&quot;&quot; |_|&quot;&quot;&quot;&quot;&quot;|_|&quot;&quot;&quot;&quot;&quot;|</span><br><span class="hljs-comment">#                   &quot;`-0-0-&#x27;&quot;`-0-0-&#x27;&quot;`-0-0-&#x27;&quot;`-0-0-&#x27;&quot;`-0-0-&#x27;&quot;`-0-0-&#x27;&quot;`-0-0-&#x27;&quot;`-0-0-&#x27;</span><br><span class="hljs-comment">#       Email:       72110902110jq@gmail.com</span><br><span class="hljs-comment">#       Group:       GUET</span><br><span class="hljs-comment">#       Create By:   coderfjq</span><br><span class="hljs-comment">#       Purpose:</span><br><span class="hljs-comment">#       Copyright:   GUET-406 - All Rights Reserved</span><br><span class="hljs-comment">#       LastModify:  2024/07/24</span><br><span class="hljs-comment"># ********************************************************************************************************************</span><br><span class="hljs-comment"># This code sucks, you know it and I know it.  </span><br><span class="hljs-comment"># Move on and call me an idiot later.</span><br><br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">from</span> 深度学习基础学习.day04.LeNet_model <span class="hljs-keyword">import</span> LeNet<br><br>transform = transforms.Compose([<br>    transforms.ToTensor(),<br>    <span class="hljs-comment"># transforms：包含图像变换方法，例如归一化、调整尺寸等。这里我们将图像归一化到均值为 0.5，标准差为 0.5。</span><br>    transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))<br>])<br><br><span class="hljs-comment"># 加载CIFAR-10训练和测试数据集</span><br><span class="hljs-comment"># trainloader和testloader：用于批量加载 CIFAR-10 数据集，提高效率</span><br>trainset = torchvision.datasets.CIFAR10(<br>    root=<span class="hljs-string">&quot;./data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">False</span>,<br>    transform=transform<br>)<br><br>testset = torchvision.datasets.CIFAR10(<br>    root=<span class="hljs-string">&quot;./data&quot;</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">False</span>,<br>    transform=transform<br>)<br><br>trainloader = torch.utils.data.DataLoader(<br>    trainset,<br>    batch_size=<span class="hljs-number">36</span>,<br>    shuffle=<span class="hljs-literal">True</span>,<br>    num_workers=<span class="hljs-number">0</span><br>)<br><br><br>testloader = torch.utils.data.DataLoader(<br>    testset,<br>    batch_size=<span class="hljs-number">10000</span>,<br>    shuffle=<span class="hljs-literal">False</span>,<br>    num_workers=<span class="hljs-number">0</span><br>)<br><br><span class="hljs-comment"># 将 testloader 转化成可迭代的迭代器</span><br>test_data_iter = <span class="hljs-built_in">iter</span>(testloader)<br>test_images,test_label = <span class="hljs-built_in">next</span>(test_data_iter)<br><br><span class="hljs-comment"># CIFAR-10的类名</span><br>classes = (<span class="hljs-string">&#x27;飞机&#x27;</span>, <span class="hljs-string">&#x27;汽车&#x27;</span>, <span class="hljs-string">&#x27;鸟&#x27;</span>, <span class="hljs-string">&#x27;猫&#x27;</span>, <span class="hljs-string">&#x27;鹿&#x27;</span>,<br>           <span class="hljs-string">&#x27;狗&#x27;</span>, <span class="hljs-string">&#x27;青蛙&#x27;</span>, <span class="hljs-string">&#x27;马&#x27;</span>, <span class="hljs-string">&#x27;船&#x27;</span>, <span class="hljs-string">&#x27;卡车&#x27;</span>)<br><br><span class="hljs-comment"># def imshow(img):</span><br><span class="hljs-comment">#     img = img / 2 +0.5</span><br><span class="hljs-comment">#     npimg = img.numpy()</span><br><span class="hljs-comment">#     plt.imshow(np.transpose(npimg,(1,2,0)))</span><br><span class="hljs-comment">#     plt.show()</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # print labels</span><br><span class="hljs-comment"># print(&#x27;&#x27;.join(&#x27;%5s&#x27; % classes[test_label[j]] for j in range(4)))</span><br><span class="hljs-comment"># # show images</span><br><span class="hljs-comment"># imshow(torchvision.utils.make_grid(test_images))</span><br><br>net = LeNet()<br><br>loss_func = nn.CrossEntropyLoss()<br>optimizer = optim.Adam(net.parameters(),lr = <span class="hljs-number">0.001</span>)<br><br>epochs = <span class="hljs-number">5</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,epochs+<span class="hljs-number">1</span>):<br><br>    running_loss = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">for</span> step,data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(trainloader,start = <span class="hljs-number">0</span>):<br><br>        inputs, labels = data<br><br>        <span class="hljs-comment"># 梯度清零（可以看作初始化）</span><br>        optimizer.zero_grad()<br><br>        <span class="hljs-comment"># 前向传播</span><br>        output = net(inputs)<br>        loss = loss_func(output,labels)<br><br>        <span class="hljs-comment"># 反向传播</span><br>        loss.backward()<br><br>        <span class="hljs-comment">#</span><br>        optimizer.step()<br><br>        running_loss += loss.item()<br>        <span class="hljs-keyword">if</span> step % <span class="hljs-number">500</span> == <span class="hljs-number">499</span> :<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                output1 = net(test_images) <span class="hljs-comment"># output1 = [batch,10]</span><br>                predict_y = torch.<span class="hljs-built_in">max</span>(output1,dim = <span class="hljs-number">1</span>)[<span class="hljs-number">1</span>] <span class="hljs-comment"># [1]中的1表示这是它的标签类别，就是index，[0]表示它确切的预测概率值</span><br>                <span class="hljs-comment"># (predict_y == test_label).sum()是一个tensor</span><br>                accurancy = (predict_y == test_label).<span class="hljs-built_in">sum</span>().item() / test_label.size(<span class="hljs-number">0</span>)<br><br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[%d, %5d] train_loss: %.3f  test_accuracy: %.3f&#x27;</span>%(epoch,step + <span class="hljs-number">1</span>,running_loss /<span class="hljs-number">500</span>,accurancy))<br>                running_loss = <span class="hljs-number">0</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练结束&quot;</span>)<br><br>save_path = <span class="hljs-string">&#x27;./LeNet.pth&#x27;</span><br>torch.save(net.state_dict(),save_path)<br><br></code></pre></td></tr></table></figure><blockquote><p>注意： *图片输入时的格式为：<code>[H,W,C]</code>,通过transforms.ToTensor()方法将其转变为<code>[C,H,W]</code>*该数据集为彩色图片，最后一层之所以没有调用softmax函数是因为在训练过程中调用损失函数nn.CrossEntropyLoss()时，内部已经实现了softmax的效果，比直接调用效果更好* 卷积层需要注意两点：1.卷积核的channel与输入特征层的channel相同，见下图，就是说输入特征矩阵彩色图片in_channel=3，则一个卷积核也有三个特征矩阵2.输出的特征矩阵channel.与卷积核个数相同，见下图，就是说卷积核个数有两个，生成的输出特征矩阵也有两个<a href="https://imgse.com/i/pkbd236"><imgsrc="https://s21.ax1x.com/2024/07/25/pkbd236.png"alt="pkbd236.png" /></a> <ahref="https://fu-jingqi.github.io/2024/07/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E4%B9%8B%E4%BD%BF%E7%94%A8CNN%E5%9C%A8CIFAR-10%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E8%AF%86%E5%88%AB/">代码详解</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Deep_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>LeNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之支持向量机实现与应用</title>
    <link href="/2024/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <url>/2024/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="支持向量机">支持向量机</h2><p>从几何学上更加直观的方法进行求解，如下图所示：</p><p><a href="https://imgse.com/i/pk7IHat"><imgsrc="https://s21.ax1x.com/2024/07/22/pk7IHat.png"alt="pk7IHat.png" /></a></p><p>上图展示了支持向量机分类的过程。图中<spanclass="math inline">\(wx十b=0\)</span>为分割直线，我们通过这条直线将数据点分开。与此同时，分割时会在直线的两边再设立两个互相平行的虚线，这两条虚线与分割直线的距离一致。这里的距离往往也被我们称之为「间隔」，而支持向量机的分割特点在于，要使得分割直线和虚线之间的间隔最大化。同时也就是两虚线之间的间隔最大化。</p><p>对于线性可分的正负样本点而言，位于<spanclass="math inline">\(wc十b=1\)</span>虚线外的点就是<strong>正样本点</strong>，而位于<spanclass="math inline">\(wx十b=一1\)</span>虚线外的点就是<strong>负样本点</strong>。另外，正好位于两条虚线上方的样本点就被我们称为支持向量，这也就是支持向量机的名字来源。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_blobs<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>%matplotlib inline<br><br>x, y = make_blobs(n_samples=<span class="hljs-number">60</span>, centers=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">30</span>, cluster_std=<span class="hljs-number">0.8</span>)  <span class="hljs-comment"># 生成示例数据</span><br><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))  <span class="hljs-comment"># 绘图</span><br>plt.scatter(x[:, <span class="hljs-number">0</span>], x[:, <span class="hljs-number">1</span>], c=y, s=<span class="hljs-number">40</span>, cmap=<span class="hljs-string">&quot;bwr&quot;</span>)<br></code></pre></td></tr></table></figure><p><code>make_blobs</code> 是一个在 Python 的<code>sklearn.datasets</code>模块中定义的函数，用于生成一个二维或多维的分类数据集，这些数据集由多个圆形或球形的簇组成，通常用于测试和演示聚类算法。下面是函数<code>make_blobs</code> 的参数解释：</p><ol type="1"><li><p><code>n_samples</code>: 这是一个整数，表示每个中心点（clustercenter）周围生成的样本数量。如果提供了一个元组，第一个元素表示每个簇的样本数量，第二个元素表示生成的簇的数量。</p></li><li><p><code>centers</code>:这是一个整数或数组，表示簇的数量。如果是一个整数，表示生成的簇的总数；如果是一个数组，则数组的长度表示簇的数量，数组中的每个元素是一个簇的中心点坐标。</p></li><li><p><code>random_state</code>: 这是一个整数或<code>None</code>，用于控制随机数生成器的种子，确保每次生成的数据集是相同的。如果设置为<code>None</code>，则每次生成的数据集可能不同。</p></li><li><p><code>cluster_std</code>:这是一个浮点数，表示簇内数据点的标凊差（或方差的标准差）。它决定了簇内数据点的分布范围，值越大，簇内的数据点分布越分散。</p></li></ol><p>使用 <code>make_blobs</code>函数可以生成一个具有多个簇的数据集，每个簇的数据点围绕一个中心点分布，并且具有一定的随机性。这在机器学习中，特别是在聚类算法的测试和验证中非常有用。</p><h3id="函数间隔与几何间隔在支持向量机中的意义">函数间隔与几何间隔在支持向量机中的意义</h3><p>在支持向量机（Support Vector Machine,SVM）中，函数间隔和几何间隔是衡量数据点与决策边界（即分类面）之间关系的两个重要概念。下面是它们的定义和意义：</p><ol type="1"><li><strong>几何间隔（Geometric Margin）</strong>：<ul><li>几何间隔是指数据点到决策边界（超平面）的最短距离。对于线性可分的情况，几何间隔是数据点到超平面的垂直距离。</li><li>几何间隔是实际的物理距离，可以直观地反映数据点在空间中的位置与决策边界的关系。</li><li>几何间隔越大，表示数据点与决策边界的距离越远，模型的分类能力越强，泛化能力越好。</li></ul></li><li><strong>函数间隔（Functional Margin）</strong>：<ul><li>函数间隔是指数据点在决策函数上的值与决策边界的距离。在二元分类问题中，决策函数可以表示为<span class="math inline">\(\( f(x) = w \cdot x + b \)\)</span>，其中<span class="math inline">\(\( w \)\)</span> 是权重向量，$( b )$是偏置项。</li><li>函数间隔计算的是 <span class="math inline">\(\( y_i (w \cdot x_i +b) \)\)</span> 的值，其中 <span class="math inline">\(\( y_i \)\)</span>是第 <span class="math inline">\(\( i \)\)</span>个数据点的真实标签，<span class="math inline">\(\( x_i \)\)</span>是其特征向量。函数间隔的绝对值越大，表示数据点越容易被正确分类。</li><li>函数间隔是相对于权重向量 <span class="math inline">\(\( w\)\)</span> 的，因此它与模型的参数（权重和偏置）有关。</li></ul></li></ol><p><strong>它们之间的关系和意义</strong>： -<strong>几何间隔与函数间隔的关系</strong>：几何间隔是函数间隔的缩放版本。具体来说，函数间隔<span class="math inline">\(\( y_i (w \cdot x_i + b) \)\)</span>除以权重向量的范数 <span class="math inline">\(\( \|w\| \)\)</span>可以得到几何间隔 <span class="math inline">\(\( \frac{y_i (w \cdot x_i +b)}{\|w\|} \)\)</span>。 -<strong>优化目标</strong>：在SVM的训练过程中，目标是最大化函数间隔，而不是几何间隔。这是因为函数间隔与模型的参数直接相关，而几何间隔则依赖于权重向量的范数，这使得函数间隔更容易在优化过程中控制。-<strong>泛化能力</strong>：函数间隔越大，意味着模型对训练数据的拟合度越高，但也可能意味着过拟合。因此，SVM通过最大化函数间隔来寻找一个平衡点，从而提高模型的泛化能力。-<strong>决策边界</strong>：几何间隔直接决定了决策边界的位置。支持向量是那些几何间隔为零的数据点，它们是距离决策边界最近的点，对决策边界的形成有直接影响。</p><p>总结来说，几何间隔和函数间隔在SVM中都是衡量数据点与决策边界关系的重要指标，但它们在优化和泛化能力方面的作用有所不同。SVM的训练过程主要关注最大化函数间隔，以获得更好的泛化效果。</p><h3 id="svm的实现">SVM的实现</h3><p>scikit-learn 中的支持向量机分类器对应的类及参数为： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sklearn.svm.SVC(C=<span class="hljs-number">1.0</span>, kernel=<span class="hljs-string">&#x27;rbf&#x27;</span>, degree=<span class="hljs-number">3</span>, gamma=<span class="hljs-string">&#x27;auto&#x27;</span>, coef0=<span class="hljs-number">0.0</span>, shrinking=<span class="hljs-literal">True</span>, probability=<span class="hljs-literal">False</span>, tol=<span class="hljs-number">0.001</span>, cache_size=<span class="hljs-number">200</span>, class_weight=<span class="hljs-literal">None</span>, verbose=<span class="hljs-literal">False</span>, max_iter=-<span class="hljs-number">1</span>, decision_function_shape=<span class="hljs-string">&#x27;ovr&#x27;</span>, random_state=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure></p><blockquote><p>C: 支持向量机中对应的惩罚参数。 kernel: 核函数，linear, poly, rbf,sigmoid, precomputed 可选，下文详细介绍。 degree: poly多项式核函数的指数。 tol: 收敛停止的容许值。</p></blockquote><p>如下图所示，支持向量机中当训练数据集并非完全线性可分时，这样在保证每个点都被正确分开后会造成过拟合，为了解决过拟合问题，引入惩罚因子<span class="math inline">\(\xi\)</span> ，可以看作上面的参数 C，允许少部分的点出错。在训练集不完全线性可分情况下，我们就要使几何间隔尽可能的大，同时使误分类点的个数尽量小，C则是调合二者的系数。</p><p><a href="https://imgse.com/i/pk7o8zD"><imgsrc="https://s21.ax1x.com/2024/07/22/pk7o8zD.png"alt="pk7o8zD.png" /></a></p><p>当我们使用支持向量机求解这类问题时，就会把最大间隔称之为最大「软间隔」，而软间隔就意味着可以容许零星噪声数据被误分类。而上文中能将数据完美分开的最大间隔也就被称为「硬间隔」。</p><p>这里，我们还是使用上面生成的示例数据训练支持向量机模型。由于是线性可分数据，<code>kernel</code>参数指定为 <code>linear</code> 即可</p><h4 id="线性支持向量机">线性支持向量机</h4><ul><li>数据准备</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_blobs<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>%matplotlib inline<br><br><span class="hljs-comment"># n_samples: 这是一个整数，表示要生成的样本总数。</span><br><span class="hljs-comment"># centers: 这是一个整数或数组，表示簇的中心点数量。如果是一个整数，make_blobs 会生成相应数量的簇，每个簇的中心点是随机选择的。</span><br><span class="hljs-comment"># random_state: 这是一个整数或 None，用于控制随机数生成器的行为，以确保结果的可重复性。如果设置为 None，则每次生成的数据可能不同。</span><br><span class="hljs-comment"># cluster_std: 这是一个浮点数，表示簇内数据点的标准差。它决定了簇内数据点的分布范围。值越大，簇内的数据点分布越分散；值越小，簇内的数据点越集中。</span><br>x, y = make_blobs(n_samples=<span class="hljs-number">60</span>, centers=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">30</span>, cluster_std=<span class="hljs-number">0.8</span>)  <span class="hljs-comment"># 生成示例数据</span><br><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))  <span class="hljs-comment"># 绘图</span><br>plt.scatter(x[:, <span class="hljs-number">0</span>], x[:, <span class="hljs-number">1</span>], c=y, s=<span class="hljs-number">40</span>, cmap=<span class="hljs-string">&quot;bwr&quot;</span>)<br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/pk7j0FH"><imgsrc="https://s21.ax1x.com/2024/07/23/pk7j0FH.png"alt="pk7j0FH.png" /></a></p><p>想要将两类数据分开，通过可视化我们能够看出我们有很多种直线可以选择</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br>plt.scatter(x[:, <span class="hljs-number">0</span>], x[:, <span class="hljs-number">1</span>], c=y, s=<span class="hljs-number">40</span>, cmap=<span class="hljs-string">&quot;bwr&quot;</span>)<br><br><span class="hljs-comment"># 绘制 3 条不同的分割线</span><br>x_temp = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">6</span>)<br><span class="hljs-keyword">for</span> m, b, d <span class="hljs-keyword">in</span> [(<span class="hljs-number">1</span>, -<span class="hljs-number">8</span>, <span class="hljs-number">0.2</span>), (<span class="hljs-number">0.5</span>, -<span class="hljs-number">6.5</span>, <span class="hljs-number">0.55</span>), (-<span class="hljs-number">0.2</span>, -<span class="hljs-number">4.25</span>, <span class="hljs-number">0.75</span>)]:<br>    y_temp = m * x_temp + b<br>    plt.plot(x_temp, y_temp, <span class="hljs-string">&quot;-k&quot;</span>)<br>    plt.fill_between(x_temp, y_temp - d, y_temp + d, color=<span class="hljs-string">&quot;#f3e17d&quot;</span>, alpha=<span class="hljs-number">0.5</span>)<br>    <span class="hljs-comment"># alpha: 这是一个介于 0 和 1 之间的浮点数，表示填充区域的不透明度。值越接近 1，填充区域越不透明；值越接近 0，填充区域越透明。</span><br>    <span class="hljs-comment"># fill_between填充区域：fill_between(x,y下限,y上限)</span><br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/pk7jBYd"><imgsrc="https://s21.ax1x.com/2024/07/23/pk7jBYd.png"alt="pk7jBYd.png" /></a></p><p>使用支持向量机可以是我们找到间距最大的那条直线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br><br>linear_svc = SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>)<br>linear_svc.fit(x,y)<br><br><span class="hljs-comment"># 对于训练完成的模型，我们可以通过 support_vectors_ 属性输出它对应的支持向量</span><br><span class="hljs-comment"># linear_svc.support_vectors_</span><br><br><span class="hljs-comment"># 绘制最大间隔支持向量图</span><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br>plt.scatter(x[:, <span class="hljs-number">0</span>], x[:, <span class="hljs-number">1</span>], c=y, s=<span class="hljs-number">40</span>, cmap=<span class="hljs-string">&quot;bwr&quot;</span>)<br>svc_plot(linear_svc)<br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/pk7jsSI"><imgsrc="https://s21.ax1x.com/2024/07/23/pk7jsSI.png"alt="pk7jsSI.png" /></a></p><ul><li><p>可视化支持向量机 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">svc_plot</span>(<span class="hljs-params">model</span>):<br>    <span class="hljs-comment"># 获取到当前 Axes 子图数据，并为绘制分割线做准备</span><br>    ax = plt.gca()<br>    x = np.linspace(ax.get_xlim()[<span class="hljs-number">0</span>], ax.get_xlim()[<span class="hljs-number">1</span>], <span class="hljs-number">50</span>)<br>    y = np.linspace(ax.get_ylim()[<span class="hljs-number">0</span>], ax.get_ylim()[<span class="hljs-number">1</span>], <span class="hljs-number">50</span>)<br>    Y, X = np.meshgrid(y, x)<br>    xy = np.vstack([X.ravel(), Y.ravel()]).T<br>    P = model.decision_function(xy).reshape(X.shape)<br><br>    <span class="hljs-comment"># 使用轮廓线方法绘制分割线</span><br>    ax.contour(X, Y, P, colors=<span class="hljs-string">&quot;green&quot;</span>, levels=[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], linestyles=[<span class="hljs-string">&quot;--&quot;</span>, <span class="hljs-string">&quot;-&quot;</span>, <span class="hljs-string">&quot;--&quot;</span>])<br><br>    <span class="hljs-comment"># 标记出支持向量的位置</span><br>    ax.scatter(<br>        model.support_vectors_[:, <span class="hljs-number">0</span>], model.support_vectors_[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&quot;green&quot;</span>, s=<span class="hljs-number">100</span><br>    )<br></code></pre></td></tr></table></figure></p></li><li><p>加入噪点</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 向原数据集中加入噪声点</span><br>x = np.concatenate((x, np.array([[<span class="hljs-number">3</span>, -<span class="hljs-number">4</span>], [<span class="hljs-number">4</span>, -<span class="hljs-number">3.8</span>], [<span class="hljs-number">2.5</span>, -<span class="hljs-number">6.3</span>], [<span class="hljs-number">3.3</span>, -<span class="hljs-number">5.8</span>]])))<br>y = np.concatenate((y, np.array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]))) <br><br>linear_svc.fit(x, y)  <span class="hljs-comment"># 训练</span><br><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br>plt.scatter(x[:, <span class="hljs-number">0</span>], x[:, <span class="hljs-number">1</span>], c=y, s=<span class="hljs-number">40</span>, cmap=<span class="hljs-string">&quot;bwr&quot;</span>)<br>svc_plot(linear_svc)<br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/pk7j66P"><imgsrc="https://s21.ax1x.com/2024/07/23/pk7j66P.png"alt="pk7j66P.png" /></a></p><p>通过改变参数<code>C</code>的值，有不同的效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># linear_svc = SVC(kernel=&quot;linear&quot;,C=1)</span><br><span class="hljs-comment"># linear_svc = SVC(kernel=&quot;linear&quot;,C=10000)</span><br><span class="hljs-comment"># linear_svc = SVC(kernel=&quot;linear&quot;,C=1000000)</span><br></code></pre></td></tr></table></figure><h4 id="非线性支持向量机">非线性支持向量机</h4><ul><li>构造数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_circles<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>x2, y2 = make_circles(<span class="hljs-number">150</span>, factor=<span class="hljs-number">0.5</span>, noise=<span class="hljs-number">0.1</span>, random_state=<span class="hljs-number">30</span>)  <span class="hljs-comment"># 生成示例数据</span><br><br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))  <span class="hljs-comment"># 绘图</span><br>plt.scatter(x2[:, <span class="hljs-number">0</span>], x2[:, <span class="hljs-number">1</span>], c=y2, s=<span class="hljs-number">40</span>, cmap=<span class="hljs-string">&quot;bwr&quot;</span>)<br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/pk7jWTg"><imgsrc="https://s21.ax1x.com/2024/07/23/pk7jWTg.png"alt="pk7jWTg.png" /></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mpl_toolkits <span class="hljs-keyword">import</span> mplot3d<br><span class="hljs-keyword">from</span> ipywidgets <span class="hljs-keyword">import</span> interact, fixed<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">kernel_function</span>(<span class="hljs-params">xi, xj</span>):<br>    poly = xi**<span class="hljs-number">2</span> + xj**<span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> poly<br><br>r = kernel_function(x2[:, <span class="hljs-number">0</span>], x2[:, <span class="hljs-number">1</span>])<br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">8</span>))<br>ax = plt.subplot(projection=<span class="hljs-string">&quot;3d&quot;</span>)<br>ax.scatter3D(x2[:, <span class="hljs-number">0</span>], x2[:, <span class="hljs-number">1</span>], r, c=y2, s=<span class="hljs-number">40</span>, cmap=<span class="hljs-string">&quot;bwr&quot;</span>)<br>ax.set_xlabel(<span class="hljs-string">&quot;x&quot;</span>)<br>ax.set_ylabel(<span class="hljs-string">&quot;y&quot;</span>)<br>ax.set_zlabel(<span class="hljs-string">&quot;r&quot;</span>)<br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/pk7jhkQ"><imgsrc="https://s21.ax1x.com/2024/07/23/pk7jhkQ.png"alt="pk7jhkQ.png" /></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC<br>rbf_svc = SVC(kernel=<span class="hljs-string">&quot;rbf&quot;</span>, gamma=<span class="hljs-string">&quot;auto&quot;</span>)<br>rbf_svc.fit(x2, y2)<br><br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>plt.scatter(x2[:, <span class="hljs-number">0</span>], x2[:, <span class="hljs-number">1</span>], c=y2, s=<span class="hljs-number">40</span>, cmap=<span class="hljs-string">&quot;bwr&quot;</span>)<br><br>svc_plot(rbf_svc)<br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/pk7jopn"><imgsrc="https://s21.ax1x.com/2024/07/23/pk7jopn.png"alt="pk7jopn.png" /></a></p><h3 id="svm的应用">SVM的应用</h3><h4 id="支持向量机实现人像分类">支持向量机实现人像分类</h4>]]></content>
    
    
    <categories>
      
      <category>Machine_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>支持向量机</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之朴素贝叶斯实现与应用</title>
    <link href="/2024/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <url>/2024/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="朴素贝叶斯">朴素贝叶斯</h2><ul><li><p>一名叫做<code>lh</code>的同学买西瓜。</p></li><li><p>卖西瓜的老板<code>bjy</code>告诉<code>lh</code>说：“我的瓜起码60%都是熟瓜。”</p><ul><li><strong>先验概率</strong>是根据以往的经验得到的，其不受任何条件的影响，只根据常识</li></ul></li><li><p>此时，又有一位不讲武德的小同志<code>ccz</code>也来买瓜，<code>ccz</code>有一手看瓜绝活，他能通过观察瓜蒂是否脱落判断瓜是否是熟瓜。</p><ul><li><strong>后验概率</strong>：若将瓜蒂脱落当作一种结果，去推测西瓜成熟的概率，这个概率被称之为后验概率</li></ul></li><li><p>小同志<code>ccz</code>购买西瓜，瓜蒂脱落且成熟，就相当于可以计算联合概率</p><ul><li><span class="math inline">\(P(瓜熟，瓜蒂脱落) = P(瓜熟|瓜蒂脱落) *P(瓜蒂脱落)\)</span></li><li><span class="math inline">\(P(瓜熟，瓜蒂脱落) = P(瓜蒂脱落|瓜熟) *P(瓜熟)\)</span></li><li>==&gt; <span class="math inline">\(P(瓜熟|瓜蒂脱落) * P(瓜蒂脱落) =P(瓜蒂脱落|瓜熟) * P(瓜熟)\)</span></li><li>==&gt; <span class="math inline">\(P(瓜熟|瓜蒂脱落) =P(瓜蒂脱落|瓜熟) * P(瓜熟) / P(瓜蒂脱落)\)</span></li></ul></li></ul><blockquote><p>重点就是计算 <code>P(瓜蒂脱落)</code></p></blockquote><p>而由全概率公式得到： *P(瓜蒂脱落)=P(瓜蒂脱落|瓜熟)<em>P(瓜熟)+P(瓜蒂脱落|瓜生)</em>P(瓜生)</p><p><code>ccz</code>一定要买一个熟瓜，他在网上一查发现瓜是否为熟瓜不是一个原因决定* <spanclass="math inline">\(P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}\)</span>* 其中<code>y</code>有<span class="math inline">\(1，2\)</span>两种取值* 不同特征两者相互独立</p><p><a href="https://imgse.com/i/pk74xfS"><imgsrc="https://s21.ax1x.com/2024/07/22/pk74xfS.png"alt="pk74xfS.png" /></a></p><h3 id="朴素贝叶斯算法流程">朴素贝叶斯算法流程</h3><ul><li>第一步：设<spanclass="math inline">\(X=\{a_1,a_2,a_3,\ldots,a_n\}\)</span>为预测数据，其中<spanclass="math inline">\(a_i\)</span>是预测数据的特征值。</li><li>第二步：设<spanclass="math inline">\(Y=\{y_1,y_2,y_3,\ldots,y_m\}\)</span>为类别集合。</li><li>第三步：计算$P(y_1x) ,P(y_2x) ,P(y_3x) ,,P(y_mx) $</li><li>第四步：寻找<span class="math inline">\(P(y_1\mid x) ,P(y_2\mid x),P(y_3\mid x) ,\ldots,P(y_m\mid x) 。\)</span>中的最大概率<spanclass="math inline">\(P(y_k\mid x)\)</span>则<code>x</code>属于类别<spanclass="math inline">\(y_k\)</span></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>朴素贝叶斯</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之k近邻回归算法实现与应用</title>
    <link href="/2024/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8Bk%E8%BF%91%E9%82%BB%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <url>/2024/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8Bk%E8%BF%91%E9%82%BB%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>在解决分类问题的过程中，K近邻算法（简称：KNN）是一种简单而且实用的方法。但KNN不仅可以实现分类问题也可以实现回归问题</p><p>K-最近邻（K-NearestNeighbors，KNN）算法是一种基本的分类和回归方法，它的工作原理非常直观：通过测量不同特征值之间的距离来进行预测。</p><h3 id="基本概念">基本概念</h3><ul><li><strong>距离度量</strong>：KNN算法通常使用欧氏距离来衡量样本之间的相似度，但也可以采用曼哈顿距离、切比雪夫距离等其他距离度量方式。</li><li><strong>K 值</strong>：K值决定了在进行决策时考虑的最近邻居的数量。较小的 K值意味着噪声对预测结果的影响更大，而较大的 K值可能导致模型过于平滑，无法捕捉数据的局部变化。</li></ul><h3 id="分类问题中的-knn">分类问题中的 KNN</h3><p>在分类问题中，KNN 的工作流程如下：</p><ol type="1"><li><strong>确定 K 值</strong>：选择一个正整数 K 作为邻居的数量。</li><li><strong>计算距离</strong>：对于每个测试数据点，计算它与训练集中每个点的距离。</li><li><strong>找到 K个最近邻居</strong>：根据计算得到的距离，找出距离最近的 K个训练数据点。</li><li><strong>进行决策</strong>：在 K个最近邻居中，根据多数投票原则（即出现次数最多的类别）来预测测试数据点的类别。</li></ol><h3 id="回归问题中的-knn">回归问题中的 KNN</h3><p>在回归问题中，KNN 的工作流程略有不同：</p><ol type="1"><li><strong>确定 K 值</strong>：同样选择一个正整数 K。</li><li><strong>计算距离</strong>：计算测试数据点与训练集中每个点的距离。</li><li><strong>找到 K 个最近邻居</strong>：找出距离最近的 K个训练数据点。</li><li><strong>预测结果</strong>：计算这 K个邻居的目标值的平均值（或加权平均值），作为测试数据点的预测结果。</li></ol><h3 id="优点">优点</h3><ul><li><strong>简单易懂</strong>：KNN 算法的原理简单，易于理解和实现。</li><li><strong>无需训练</strong>：KNN是一种惰性学习算法，所有的计算都是在预测时进行，不需要在训练阶段花费时间。</li><li><strong>可用于非线性问题</strong>：KNN不需要假设数据的分布，因此可以用于非线性问题的分类和回归。</li></ul><h3 id="缺点">缺点</h3><ul><li><strong>计算成本高</strong>：由于在预测时需要计算测试数据点与所有训练数据点之间的距离，因此计算成本较高，尤其是在大数据集上。</li><li><strong>存储成本高</strong>：KNN需要存储全部数据集，因此存储成本较高。</li><li><strong>对异常值敏感</strong>：KNN算法对噪声和异常值敏感，这可能会影响模型的预测性能。</li><li><strong>需要合适的距离度量和 K 值</strong>：选择合适的距离度量和 K值对模型的性能至关重要，但找到最佳参数可能需要大量的实验。</li></ul><h3 id="实现">实现</h3><p>KNN 算法可以使用各种编程语言和库实现，如 Python 中的<code>scikit-learn</code> 库提供了 <code>KNeighborsClassifier</code> 和<code>KNeighborsRegressor</code> 类来分别处理分类和回归问题。</p><p>KNN是一种非常灵活的算法，适用于多种类型的数据和问题，尽管它有其局限性，但在许多实际应用中仍然是一种有效的机器学习算法。</p><h2 id="knn算法实现">KNN算法实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">knn_classify</span>(<span class="hljs-params">test_data, train_data, labels, k</span>):<br>    <span class="hljs-comment"># KNN 方法完整实现</span><br>    distances = np.array([])  <span class="hljs-comment"># 创建一个空的数组用于存放距离</span><br><br>    <span class="hljs-keyword">for</span> each_data <span class="hljs-keyword">in</span> train_data:  <span class="hljs-comment"># 使用欧式距离计算数据相似度</span><br>        d = d_euc(test_data, each_data)<br>        distances = np.append(distances, d)<br><br>    sorted_distance_index = distances.argsort()  <span class="hljs-comment"># 获取按距离从小到大排序后的索引</span><br>    sorted_distance = np.sort(distances)<br>    r = (sorted_distance[k] + sorted_distance[k - <span class="hljs-number">1</span>]) / <span class="hljs-number">2</span>  <span class="hljs-comment"># 计算</span><br><br>    class_count = &#123;&#125;<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):  <span class="hljs-comment"># 多数表决</span><br>        vote_label = labels[sorted_distance_index[i]]<br>        class_count[vote_label] = class_count.get(vote_label, <span class="hljs-number">0</span>) + <span class="hljs-number">1</span><br><br>    final_label = majority_voting(class_count)<br>    <span class="hljs-keyword">return</span> final_label, r<br><br></code></pre></td></tr></table></figure><h3 id="knn解决分类问题">KNN解决分类问题：</h3><p>使用sklearn库： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier<br><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br><span class="hljs-comment"># 得到 lilac 数据集中 feature 的全部序列: sepal_length,sepal_width,petal_length,petal_width</span><br>feature_data = lilac_data.iloc[:, :-<span class="hljs-number">1</span>]<br>label_data = lilac_data[<span class="hljs-string">&quot;labels&quot;</span>]  <span class="hljs-comment"># 得到 lilac 数据集中 label 的序列</span><br><br>X_train, X_test, y_train, y_test = train_test_split(<br>    feature_data, label_data, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">2</span><br>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sklearn_classify</span>(<span class="hljs-params">train_data, label_data, test_data, k_num</span>):<br>    <span class="hljs-comment"># 使用 sklearn 构建 KNN 预测模型</span><br>    knn = KNeighborsClassifier(n_neighbors=k_num)<br>    <span class="hljs-comment"># 训练数据集</span><br>    knn.fit(train_data, label_data)<br>    <span class="hljs-comment"># 预测</span><br>    predict_label = knn.predict(test_data)<br>    <span class="hljs-comment"># 返回预测值</span><br>    <span class="hljs-keyword">return</span> predict_label<br><br><span class="hljs-comment"># 使用测试数据进行预测</span><br>y_predict = sklearn_classify(X_train, y_train, X_test, <span class="hljs-number">3</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_accuracy</span>(<span class="hljs-params">test_labels, pred_labels</span>):<br>    <span class="hljs-comment"># 准确率计算函数</span><br>    correct = np.<span class="hljs-built_in">sum</span>(test_labels == pred_labels)  <span class="hljs-comment"># 计算预测正确的数据个数</span><br>    n = <span class="hljs-built_in">len</span>(test_labels)  <span class="hljs-comment"># 总测试集数据个数</span><br>    accur = correct / n<br>    <span class="hljs-keyword">return</span> accur<br>    <br>get_accuracy(y_test, y_predict)<br><br></code></pre></td></tr></table></figure></p><p>k值选择： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">normal_accuracy = []  <span class="hljs-comment"># 建立一个空的准确率列表</span><br>k_value = <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, <span class="hljs-number">11</span>)<br><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> k_value:<br>    y_predict = sklearn_classify(X_train, y_train, X_test, k)<br>    accuracy = get_accuracy(y_test, y_predict)<br>    normal_accuracy.append(accuracy)<br><br>plt.xlabel(<span class="hljs-string">&quot;k&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>new_ticks = np.linspace(<span class="hljs-number">0.6</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">10</span>)  <span class="hljs-comment"># 设定 y 轴显示，从 0.6 到 0.9</span><br>plt.yticks(new_ticks)<br>plt.plot(k_value, normal_accuracy, c=<span class="hljs-string">&quot;r&quot;</span>)<br>plt.grid(<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 给画布增加网格</span><br></code></pre></td></tr></table></figure></p><h3 id="knn解决回归问题">KNN解决回归问题</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsRegressor<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">knn_regression</span>(<span class="hljs-params">train_data, train_labels, test_data, k</span>):<br>    <span class="hljs-comment">### 代码开始 ### (≈ 10 行代码)</span><br>    <span class="hljs-comment"># 默认情况下 KNeighborsClassifier 也会使用欧式距离</span><br>    <span class="hljs-comment"># test_labels = None</span><br>    model = KNeighborsRegressor(n_neighbors=k)<br>    model.fit(train_data,train_labels)<br>    test_labels = model.predict(test_data)<br>    <span class="hljs-comment">### 代码结束 ###</span><br>    <span class="hljs-keyword">return</span> test_labels<br><br><span class="hljs-comment"># 训练样本特征</span><br>train_data = np.array(<br>    [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>], [<span class="hljs-number">6</span>, <span class="hljs-number">6</span>], [<span class="hljs-number">7</span>, <span class="hljs-number">7</span>], [<span class="hljs-number">8</span>, <span class="hljs-number">8</span>], [<span class="hljs-number">9</span>, <span class="hljs-number">9</span>], [<span class="hljs-number">10</span>, <span class="hljs-number">10</span>]]<br>)<br><br><span class="hljs-comment"># 训练样本目标值</span><br>train_labels = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>])<br><br><span class="hljs-comment"># 测试样本特征</span><br>test_data = np.array([[<span class="hljs-number">1.2</span>, <span class="hljs-number">1.3</span>], [<span class="hljs-number">3.7</span>, <span class="hljs-number">3.5</span>], [<span class="hljs-number">5.5</span>, <span class="hljs-number">6.2</span>], [<span class="hljs-number">7.1</span>, <span class="hljs-number">7.9</span>]])<br><br><span class="hljs-comment"># 测试样本目标值</span><br>pre = knn_regression(train_data, train_labels, test_data, k=<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br>X_train,X_test, y_train, y_test =train_test_split(train_data,train_target,test_size=<span class="hljs-number">0.4</span>, random_state=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>其中： *<code>X_train,X_test, y_train, y_test</code>分别表示，切分后的特征的训练集，特征的测试集，标签的训练集，标签的测试集；其中特征和标签的值是一一对应的。</p><ul><li><p><code>train_data,train_target</code>分别表示为待划分的特征集和待划分的标签集。</p></li><li><p><code>test_size</code>：测试样本所占比例。</p></li><li><p><code>random_state</code>：随机数种子,在需要重复实验时，保证在随机数种子一样时能得到一组一样的随机数</p></li></ul><blockquote><p>sklearn.neighbors.KNeighborsClassifier((n_neighbors=5,weights='uniform', algorithm='auto') * n_neighbors : k值，表示邻近个数，默认为 5。</p></blockquote><ul><li><p>weights :决策规则选择，多数表决或加权表决，可用参数（'uniform','distance'）</p></li><li><p>algorithm : 搜索算法选择（auto，kd_tree,ball_tree），包括逐一搜索，kd 树搜索或 ball 树搜索</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Machine_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>KNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之逻辑回归实现与应用</title>
    <link href="/2024/07/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <url>/2024/07/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p><strong>逻辑回归是一种分类方法，而并不是回归方法。</strong></p><p>逻辑回归（LogisticRegression）是一种广泛使用的线性分类算法，主要用于二分类问题。尽管它的名字中包含“回归”二字，但它实际上是一种分类算法。逻辑回归通过预测一个事件发生的概率，来决定该事件是否发生。</p><blockquote><p>数据集：https://cdn.aibydoing.com/hands-on-ai/files/course-8-data.csv</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">&#x27;./course-8-data.csv&#x27;</span>)<br>x = df[[<span class="hljs-string">&quot;X0&quot;</span>, <span class="hljs-string">&quot;X1&quot;</span>]].values<br>y = df[<span class="hljs-string">&quot;Y&quot;</span>].values<br>lr = <span class="hljs-number">0.01</span>  <span class="hljs-comment"># 学习率</span><br>num_iter = <span class="hljs-number">30000</span>  <span class="hljs-comment"># 迭代次数</span><br><br>model = LogisticRegression(<br>    tol=<span class="hljs-number">0.001</span>,<br>    max_iter=<span class="hljs-number">10000</span>,<br>    solver=<span class="hljs-string">&#x27;liblinear&#x27;</span><br>)<br>model.fit(x,y)<br><span class="hljs-comment"># print(model.coef_,model.intercept_)</span><br><br>plt.scatter(df[<span class="hljs-string">&#x27;X0&#x27;</span>],df[<span class="hljs-string">&#x27;X1&#x27;</span>],c=y)<br>plt.xlabel(<span class="hljs-string">&#x27;X0&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;X1&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="机器学习之逻辑回归实现与应用/pic_1.jpg" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(model.coef_.shape)<br><span class="hljs-built_in">print</span>(model.intercept_.shape)<br><span class="hljs-built_in">print</span>(x.shape)<br><span class="hljs-built_in">print</span>(y.shape)<br></code></pre></td></tr></table></figure><p>输出如下： <figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs clojure">(<span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span>)<br>(<span class="hljs-number">1</span><span class="hljs-punctuation">,</span>)<br>(<span class="hljs-number">150</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span>)<br>(<span class="hljs-number">150</span><span class="hljs-punctuation">,</span>)<br></code></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.scatter(df[<span class="hljs-string">&quot;X0&quot;</span>], df[<span class="hljs-string">&quot;X1&quot;</span>], c=df[<span class="hljs-string">&quot;Y&quot;</span>])<br><br>x1_min, x1_max = df[<span class="hljs-string">&quot;X0&quot;</span>].<span class="hljs-built_in">min</span>(), df[<span class="hljs-string">&quot;X0&quot;</span>].<span class="hljs-built_in">max</span>()<br>x2_min, x2_max = df[<span class="hljs-string">&quot;X1&quot;</span>].<span class="hljs-built_in">min</span>(), df[<span class="hljs-string">&quot;X1&quot;</span>].<span class="hljs-built_in">max</span>()<br><br>xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))<br><span class="hljs-built_in">print</span>(xx1.shape)<br><span class="hljs-built_in">print</span>(xx2.shape)<br><span class="hljs-built_in">print</span>(xx1.ravel().shape)<br><span class="hljs-built_in">print</span>(xx2.ravel().shape)<br>grid = np.c_[xx1.ravel(), xx2.ravel()]<br><span class="hljs-built_in">print</span>(grid.shape)<br>probs = (np.dot(grid, model.coef_.T) + model.intercept_).reshape(xx1.shape)<br><span class="hljs-comment"># 绘制等高线图</span><br><span class="hljs-comment"># probs = w1 * x1 + w2 * x2 + b</span><br>plt.contour(xx1, xx2, probs, levels=[<span class="hljs-number">0</span>], linewidths=<span class="hljs-number">1</span>, colors=<span class="hljs-string">&quot;red&quot;</span>)<br><br></code></pre></td></tr></table></figure><p>输出如下： <figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs clojure">(<span class="hljs-number">50</span><span class="hljs-punctuation">,</span> <span class="hljs-number">50</span>)<br>(<span class="hljs-number">50</span><span class="hljs-punctuation">,</span> <span class="hljs-number">50</span>)<br>(<span class="hljs-number">2500</span><span class="hljs-punctuation">,</span>)<br>(<span class="hljs-number">2500</span><span class="hljs-punctuation">,</span>)<br>(<span class="hljs-number">2500</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure> <imgsrc="机器学习之逻辑回归实现与应用/pic_2.jpg" /></p><h2 id="知识点">知识点</h2><h3 id="np.meshgrid">np.meshgrid</h3><p><code>np.meshgrid</code>是NumPy库中的一个函数，它用于生成坐标网格。在多维空间中，当你需要对一个区域进行网格化，以便进行计算或绘图时，<code>np.meshgrid</code>非常有用。</p><p>具体来说，<code>np.meshgrid</code>可以接收两个一维数组，分别表示x轴和y轴的坐标点，并返回两个二维数组，这两个二维数组的行和列分别对应输入数组的坐标点。这样，你可以很容易地对这些坐标点进行矩阵运算或在这些点上进行函数评估。</p><p>下面是<code>np.meshgrid</code>的基本用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>x = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>y = np.array([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>])<br><br>X, Y = np.meshgrid(x, y)<br></code></pre></td></tr></table></figure><p>在这个例子中，<code>X</code>和<code>Y</code>将是这样的二维数组：</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs inform7">X = <span class="hljs-comment">[<span class="hljs-comment">[1, 2, 3]</span>,</span><br><span class="hljs-comment">     <span class="hljs-comment">[1, 2, 3]</span>,</span><br><span class="hljs-comment">     <span class="hljs-comment">[1, 2, 3]</span>,</span><br><span class="hljs-comment">     <span class="hljs-comment">[1, 2, 3]</span>]</span><br><br>Y = <span class="hljs-comment">[<span class="hljs-comment">[4, 4, 4]</span>,</span><br><span class="hljs-comment">     <span class="hljs-comment">[5, 5, 5]</span>,</span><br><span class="hljs-comment">     <span class="hljs-comment">[6, 6, 6]</span>,</span><br><span class="hljs-comment">     <span class="hljs-comment">[7, 7, 7]</span>]</span><br></code></pre></td></tr></table></figure><p>这样，<code>X[i, j]</code>和<code>Y[i, j]</code>就代表了网格中的一个点，其中<code>i</code>是行索引，<code>j</code>是列索引。</p><p><code>np.meshgrid</code>函数还有几个可选参数，例如：</p><ul><li><code>indexing</code>：默认值为'xy'，表示第一个参数对应x轴，第二个参数对应y轴。如果设置为'ij'，则表示第一个参数对应行索引，第二个参数对应列索引。</li><li><code>sparse</code>：默认为False，如果设置为True，则返回稀疏数组，这在处理大型网格时可以节省内存。</li></ul><p><code>np.meshgrid</code>在科学计算和数据可视化中非常常见，特别是在需要对函数进行二维或多维插值、计算或绘制图形时。</p><h3 id="np.c_">np.c_</h3><p><code>np.c_</code> 是 NumPy库中的一个函数，用于沿着列方向（即沿着第二个轴）堆叠数组。它是用于列向量或列矩阵的堆叠的快捷方式。当你想要将多个数组并排放置，形成一个新的二维数组时，可以使用<code>np.c_</code>。</p><p><code>np.c_</code> 的使用示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 创建两个一维数组</span><br>a = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>b = np.array([<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br><br><span class="hljs-comment"># 使用 np.c_ 将数组 a 和 b 沿着列方向堆叠</span><br>c = np.c_[a, b]<br></code></pre></td></tr></table></figure><p>执行上述代码后，<code>c</code> 将变成：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs lua">array(<span class="hljs-string">[[1, 4],</span><br><span class="hljs-string">       [2, 5],</span><br><span class="hljs-string">       [3, 6]]</span>)<br></code></pre></td></tr></table></figure><p>这里，<code>np.c_[a, b]</code> 等价于<code>np.column_stack((a, b))</code>，<code>np.column_stack</code>函数是 <code>np.c_</code>的等效函数，用于将多个一维数组或列向量堆叠成一个二维数组。</p><p><code>np.c_</code> 也可以用于堆叠更多的数组，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">d = np.array([<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>])<br>e = np.c_[a, b, d]  <span class="hljs-comment"># 将 a, b, d 堆叠成列</span><br></code></pre></td></tr></table></figure><p>这将得到：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs lua">array(<span class="hljs-string">[[1, 4, 7],</span><br><span class="hljs-string">       [2, 5, 8],</span><br><span class="hljs-string">       [3, 6, 9]]</span>)<br></code></pre></td></tr></table></figure><p><code>np.c_</code>通常用于数据预处理或在进行矩阵运算时需要将多个数组组合成一个矩阵的场景。</p><h3 id="ravel方法">.ravel()方法</h3><p>在NumPy中，<code>.ravel()</code>方法用于将多维数组（ndarray）降维成一维数组。如果数组已经是一维的，<code>.ravel()</code>方法将返回其自身的视图。如果数组是多维的，<code>.ravel()</code>会返回数组的一维副本，但不会改变原始数组的形状。</p><p>使用<code>.ravel()</code>方法时，可以指定一个可选的<code>order</code>参数，它控制数组的展开顺序。默认情况下，<code>order</code>参数是<code>'C'</code>，表示按C语言的行优先顺序展开数组。如果设置为<code>'F'</code>或<code>'A'</code>，则会按列优先顺序（即Fortran顺序）展开数组。</p><p>下面是<code>.ravel()</code>方法的使用示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 创建一个二维数组</span><br>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br><br><span class="hljs-comment"># 使用 ravel() 将数组降维成一维数组</span><br>b = a.ravel()<br><br><span class="hljs-built_in">print</span>(b)  <span class="hljs-comment"># 输出: [1 2 3 4 5 6]</span><br><br><span class="hljs-comment"># 如果数组已经是一维的，ravel() 将返回其自身的视图</span><br>c = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>d = c.ravel()<br><br><span class="hljs-built_in">print</span>(d <span class="hljs-keyword">is</span> c)  <span class="hljs-comment"># 输出: True，表示 d 和 c 是同一个对象</span><br></code></pre></td></tr></table></figure><p>使用<code>order='F'</code>参数时：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建一个二维数组</span><br>e = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])<br><br><span class="hljs-comment"># 使用 ravel() 并指定 order=&#x27;F&#x27; 按列优先顺序展开数组</span><br>f = e.ravel(order=<span class="hljs-string">&#x27;F&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(f)  <span class="hljs-comment"># 输出: [1 4 2 5 3 6]</span><br></code></pre></td></tr></table></figure><p><code>.ravel()</code>方法在需要将多维数组转换为一维数组进行操作时非常有用，例如在机器学习算法中对特征向量进行处理。</p><h3 id="plt.contour">plt.contour</h3><p><code>plt.contour</code> 是 Matplotlib库中的一个函数，用于绘制等高线图（contourplot），这种图表可以展示数据的三维形状在二维平面上的投影。</p><p>等高线图通常用于表示一个二维区域内的数值变化，例如地形图、温度分布图或任何二维函数的等值线图。在机器学习中，等高线图常用于可视化决策边界。</p><p>下面是 <code>plt.contour</code> 函数的一些常用参数及其作用：</p><ul><li><p><code>X</code> 和 <code>Y</code>:这两个参数是可选的，它们分别代表数据点的 x 轴和 y 轴坐标网格。如果提供了<code>X</code> 和 <code>Y</code>，则 <code>Z</code> 参数必须是一个与<code>X</code> 和 <code>Y</code> 形状相同的二维数组。如果没有提供<code>X</code> 和 <code>Y</code>，则 <code>Z</code> 必须是二维数组，并且<code>plt.contour</code> 会隐式地使用 <code>Z</code>的行索引和列索引作为坐标。</p></li><li><p><code>Z</code>:这是一个二维数组，包含了要绘制的数值数据。等高线图将展示这些数值的等值线。</p></li><li><p><code>levels</code>:这个参数定义了要绘制的等值线的具体数值。如果未指定，<code>plt.contour</code>将自动选择等值线的数量和位置。如果指定为一个数组，将绘制数组中定义的特定等值线。</p></li><li><p><code>linewidths</code>:这个参数控制等高线的宽度。它可以是一个数值，表示所有等高线使用相同的线宽，或者是一个与<code>levels</code> 数量相同的数组，表示每条等高线的具体线宽。</p></li><li><p><code>colors</code>:这个参数定义了等高线的颜色。它可以是一个颜色名称、十六进制颜色代码，或者是一个颜色映射表的名称。</p></li><li><p><code>alpha</code>: 这个参数控制等高线的透明度。</p></li><li><p><code>label</code>: 用于图例的标签。</p></li><li><p><code>norm</code>: 一个归一化对象，它将 <code>Z</code>中的值映射到 <code>colors</code> 参数指定的颜色。</p></li><li><p><code>extent</code>: 用于指定 <code>X</code> 和 <code>Y</code>轴的范围，如果没有提供 <code>X</code> 和<code>Y</code>，则这个参数特别有用。</p></li><li><p><code>transform</code>: 用于指定坐标轴的变换。</p></li></ul><p>等高线图的绘制通常需要三个主要步骤：</p><ol type="1"><li>准备数值数据（Z）。</li><li>使用 <code>np.meshgrid</code> 生成坐标网格（如果需要）。</li><li>调用 <code>plt.contour</code> 绘制等高线。</li></ol><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 创建数据</span><br>x = np.linspace(-<span class="hljs-number">3.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">100</span>)<br>y = np.linspace(-<span class="hljs-number">3.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">100</span>)<br>X, Y = np.meshgrid(x, y)<br>Z = np.sin(X) * np.cos(Y)<br><br><span class="hljs-comment"># 绘制等高线图</span><br>plt.contour(X, Y, Z, levels=<span class="hljs-number">14</span>, linewidths=<span class="hljs-number">1</span>, colors=<span class="hljs-string">&#x27;red&#x27;</span>)<br>plt.colorbar()  <span class="hljs-comment"># 显示颜色条</span><br>plt.show()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Machine_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>逻辑回归</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习实践之使用CNN在CIFAR-10数据集上进行识别</title>
    <link href="/2024/07/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E4%B9%8B%E4%BD%BF%E7%94%A8CNN%E5%9C%A8CIFAR-10%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E8%AF%86%E5%88%AB/"/>
    <url>/2024/07/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E4%B9%8B%E4%BD%BF%E7%94%A8CNN%E5%9C%A8CIFAR-10%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h2id="利用cifar-10数据彩色图像集实现卷积神经网络cnn进行图像分类">利用CIFAR-10数据（彩色图像）集实现卷积神经网络（CNN）进行图像分类</h2><h3 id="数据集准备">1. 数据集准备</h3><p><strong>CIFAR-10</strong> 数据集包括 <spanclass="math inline">\(60,000\)</span> 张 <span class="math inline">\(32\times 32\)</span> 的彩色图像，分成 <spanclass="math inline">\(10\)</span> 类，每类 <spanclass="math inline">\(6,000\)</span>张。其中 <spanclass="math inline">\(50,000\)</span> 张用于训练，<spanclass="math inline">\(10,000\)</span> 张用于测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 为训练和测试集定义数据变换</span><br>transform = transforms.Compose([<br>    transforms.ToTensor(),<br>    <span class="hljs-comment"># transforms：包含图像变换方法，例如归一化、调整尺寸等。这里我们将图像归一化到均值为 0.5，标准差为 0.5。</span><br>    transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))<br>])<br><br><span class="hljs-comment"># 加载CIFAR-10训练和测试数据集</span><br><span class="hljs-comment"># trainloader和testloader：用于批量加载 CIFAR-10 数据集，提高效率</span><br>trainset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./data&quot;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">False</span>, transform=transform)<br>trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br><br>testset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./data&quot;</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">False</span>, transform=transform)<br>testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">False</span>, num_workers=<span class="hljs-number">2</span>)<br><span class="hljs-comment"># CIFAR-10的类名</span><br>classes = (<span class="hljs-string">&#x27;飞机&#x27;</span>, <span class="hljs-string">&#x27;汽车&#x27;</span>, <span class="hljs-string">&#x27;鸟&#x27;</span>, <span class="hljs-string">&#x27;猫&#x27;</span>, <span class="hljs-string">&#x27;鹿&#x27;</span>,<br>           <span class="hljs-string">&#x27;狗&#x27;</span>, <span class="hljs-string">&#x27;青蛙&#x27;</span>, <span class="hljs-string">&#x27;马&#x27;</span>, <span class="hljs-string">&#x27;船&#x27;</span>, <span class="hljs-string">&#x27;卡车&#x27;</span>)<br></code></pre></td></tr></table></figure><p><strong>可视化数据集</strong></p><p><a href="https://imgse.com/i/pkoEqYT"><imgsrc="https://s21.ax1x.com/2024/07/17/pkoEqYT.jpg"alt="pkoEqYT.jpg" /></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 显示一批图像的函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">imshow</span>(<span class="hljs-params">img</span>):<br>    img = img / <span class="hljs-number">2</span> + <span class="hljs-number">0.5</span>  <span class="hljs-comment"># 反归一化</span><br>    npimg = img.numpy()<br>    plt.imshow(np.transpose(npimg, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<br>    plt.show()<br><br><span class="hljs-comment"># 显示训练集中的前16个图像</span><br>dataiter = <span class="hljs-built_in">iter</span>(trainloader)<br>images, labels = <span class="hljs-built_in">next</span>(dataiter)<br><br><span class="hljs-comment"># 只显示前16个图像和标签</span><br>imshow(torchvision.utils.make_grid(images[:<span class="hljs-number">16</span>]))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;classes[labels[j]]:5s&#125;</span>&#x27;</span> <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">16</span>)))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_and_evaluate</span>(<span class="hljs-params">model, train_loader, val_loader, epochs</span>):<br>    criterion = nn.CrossEntropyLoss()<br>    optimizer = optim.Adam(model.parameters(), lr=<span class="hljs-number">0.001</span>)<br>    model.train()<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        running_loss = <span class="hljs-number">0.0</span><br>        <span class="hljs-keyword">for</span> i, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>            <span class="hljs-comment"># data, target = data.to(device), target.to(device)</span><br>            optimizer.zero_grad()<br>            output = model(data)<br>            output = output.squeeze()<br>            target = target.squeeze()<br>            loss = criterion(output, target)<br>            loss.backward()<br>            optimizer.step()<br>            running_loss += loss.item()<br>            <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">99</span>:  <span class="hljs-comment"># 每100个小批量打印一次</span><br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;[Epoch <span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>, Batch <span class="hljs-subst">&#123;i + <span class="hljs-number">1</span>&#125;</span>] loss: <span class="hljs-subst">&#123;running_loss / <span class="hljs-number">100</span>:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br>                running_loss = <span class="hljs-number">0.0</span><br><br>    <span class="hljs-comment"># 评估模型性能</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        correct = <span class="hljs-number">0</span><br>        total = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> val_loader:<br>            <span class="hljs-comment"># data, target = data.to(device), target.to(device)</span><br>            output = model(data)<br>            _, predicted = torch.<span class="hljs-built_in">max</span>(output, <span class="hljs-number">1</span>)<br>            total += target.size(<span class="hljs-number">0</span>)<br>            correct += (predicted == target).<span class="hljs-built_in">sum</span>().item()<br>        accuracy = correct / total<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;在10,000张测试图像上的准确率：<span class="hljs-subst">&#123;<span class="hljs-number">100</span> * accuracy:<span class="hljs-number">.2</span>f&#125;</span>%&#x27;</span>)<br>    <span class="hljs-keyword">return</span> accuracy<br></code></pre></td></tr></table></figure><h3 id="构建cnn模型">3. 构建CNN模型</h3><p>卷积神经网络由<strong>卷积层</strong>、<strong>池化层</strong>和<strong>全连接层</strong>构成。</p><h4 id="法一自定义构建法">法一：自定义构建法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型定义</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CNN</span>(nn.Module):<br>    <span class="hljs-comment"># 定义模型属性</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels</span>):<br>        <span class="hljs-built_in">super</span>(CNN, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 输入到隐层 1</span><br>        <span class="hljs-comment"># in_channels: 输入通道数。这是输入数据的通道数，例如，对于彩色图像，输入通道数通常是 3（RGB）。</span><br>        <span class="hljs-comment"># out_channels: 输出通道数。这是卷积层输出的通道数。通常，输出通道数可以增加以提取更复杂的特征。</span><br>        <span class="hljs-comment"># kernel_size: 卷积核大小。这是一个整数或元组，表示卷积核的高度和宽度。例如，(3, 3) 表示卷积核是一个 3x3 的正方形。</span><br>        <span class="hljs-comment"># padding: 填充。这是一个整数或元组，表示在输入图像的边界周围添加的填充。填充可以是零填充，也可以是反射填充等。填充可以用于控制输出特征图的尺寸。</span><br>        <span class="hljs-comment"># stride: 步长。这是一个整数或元组，表示卷积操作在每个方向上的步长。步长为 1 表示卷积核每次移动一个像素。步长增加可以减少输出特征图的尺寸。</span><br>        <span class="hljs-comment"># bias: 偏置项。这是一个布尔值，表示是否在卷积层中添加偏置项。默认为 True。</span><br>        <span class="hljs-variable language_">self</span>.hidden1 = Conv2d(in_channels, out_channels = <span class="hljs-number">32</span>, kernel_size = (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># kaiming_uniform_这种初始化方法特别适用于使用ReLU或Leaky ReLU激活函数的网络</span><br>        <span class="hljs-comment"># kaiming_uniform_ 被用于初始化卷积神经网络（CNN）中卷积层和全连接层的权重</span><br>        <span class="hljs-comment"># kaiming_uniform_(self.hidden1.weight, nonlinearity=&#x27;relu&#x27;)</span><br>        <span class="hljs-variable language_">self</span>.act1 = ReLU()<br>        <span class="hljs-comment"># 池化层 1</span><br>        <span class="hljs-comment"># kernel_size: 池化窗口的大小。这是一个整数或者一对整数 (kH, kW)，分别表示高度和宽度。例如，(2, 2) 表示一个 2x2 的池化窗口。</span><br>        <span class="hljs-comment"># stride: 池化窗口滑动的步长。这决定了相邻窗口的水平和垂直间距。步长也是一个整数或者一对整数 (sH, sW)。如果未指定，它默认与 kernel_size 相同</span><br>        <span class="hljs-comment"># padding: 输入特征图的边界周围添加的填充。这可以是一个整数或者一对整数 (padH, padW)，表示在高度和宽度方向上的填充大小。填充通常用于保持输出特征图的尺寸不变。</span><br>        <span class="hljs-comment"># dilation: 池化窗口中元素之间的间距。这可以用于控制池化操作的“感受野”大小。默认值为 1。</span><br>        <span class="hljs-comment"># ceil_mode: 一个布尔值，当设置为 True 时，它会使用 ceil 函数而不是 floor 函数来计算输出尺寸。这意味着当输入尺寸不能被步长整除时，输出尺寸会向上取整。</span><br>        <span class="hljs-comment"># return_indices: 在某些情况下，可能需要知道池化过程中每个输出值来自输入中的哪个位置。如果设置为 True，则在进行最大池化时返回最大值的索引。</span><br>        <span class="hljs-variable language_">self</span>.pool1 = MaxPool2d(kernel_size = (<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br><br>        <span class="hljs-comment"># 隐层 2</span><br>        <span class="hljs-variable language_">self</span>.hidden2 = Conv2d(<span class="hljs-number">32</span>, out_channels = <span class="hljs-number">64</span>, kernel_size = (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># kaiming_uniform_(self.hidden2.weight, nonlinearity=&#x27;relu&#x27;)</span><br>        <span class="hljs-variable language_">self</span>.act2 = ReLU()<br>        <span class="hljs-comment"># 池化层 2</span><br>        <span class="hljs-variable language_">self</span>.pool2 = MaxPool2d(kernel_size = (<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br><br>        <span class="hljs-comment"># 隐层 3</span><br>        <span class="hljs-variable language_">self</span>.hidden3 = Conv2d(<span class="hljs-number">64</span>, out_channels = <span class="hljs-number">128</span>, kernel_size = (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>),padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># kaiming_uniform_(self.hidden3.weight, nonlinearity=&#x27;relu&#x27;)</span><br>        <span class="hljs-variable language_">self</span>.act3 = ReLU()<br>        <span class="hljs-comment"># 池化层 3</span><br>        <span class="hljs-variable language_">self</span>.pool3 = MaxPool2d(kernel_size = (<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br><br>        <span class="hljs-comment"># 全连接层</span><br>        <span class="hljs-variable language_">self</span>.hidden4 = Linear(<span class="hljs-number">128</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">512</span>)<br>        <span class="hljs-comment"># kaiming_uniform_(self.hidden3.weight, nonlinearity=&#x27;relu&#x27;)</span><br>        <span class="hljs-variable language_">self</span>.act4 = ReLU()<br><br>        <span class="hljs-comment"># 全连接层</span><br>        <span class="hljs-variable language_">self</span>.hidden5 = Linear(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>)<br>        <span class="hljs-comment"># kaiming_uniform_(self.hidden5.weight, nonlinearity=&#x27;relu&#x27;)</span><br>        <span class="hljs-variable language_">self</span>.act5 = ReLU()<br><br>        <span class="hljs-comment"># 输出层</span><br>        <span class="hljs-variable language_">self</span>.hidden6 = Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>)<br>        <span class="hljs-comment"># xavier_uniform_(self.hidden6.weight)</span><br>        <span class="hljs-comment"># self.act6 = Softmax(dim=1)</span><br><br>    <span class="hljs-comment"># 前向传播</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-comment"># 输入到隐层 1</span><br>        X = <span class="hljs-variable language_">self</span>.hidden1(X)<br>        X = <span class="hljs-variable language_">self</span>.act1(X)<br>        X = <span class="hljs-variable language_">self</span>.pool1(X)<br>        <span class="hljs-comment"># 隐层 2</span><br>        X = <span class="hljs-variable language_">self</span>.hidden2(X)<br>        X = <span class="hljs-variable language_">self</span>.act2(X)<br>        X = <span class="hljs-variable language_">self</span>.pool2(X)<br>        <span class="hljs-comment"># 隐层 3</span><br>        X = <span class="hljs-variable language_">self</span>.hidden3(X)<br>        X = <span class="hljs-variable language_">self</span>.act3(X)<br>        X = <span class="hljs-variable language_">self</span>.pool3(X)<br>        <span class="hljs-comment"># 扁平化</span><br>        X = X.view(-<span class="hljs-number">1</span>, <span class="hljs-number">128</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>)<br>        <span class="hljs-comment"># 隐层 4</span><br>        X = <span class="hljs-variable language_">self</span>.hidden4(X)<br>        X = <span class="hljs-variable language_">self</span>.act4(X)<br>        <span class="hljs-comment"># 隐层 5</span><br>        X = <span class="hljs-variable language_">self</span>.hidden5(X)<br>        X = <span class="hljs-variable language_">self</span>.act5(X)<br>        <span class="hljs-comment"># 隐层 6(输出)</span><br>        X = <span class="hljs-variable language_">self</span>.hidden6(X)<br>        <span class="hljs-comment"># X = self.act6(X)</span><br>        <span class="hljs-keyword">return</span> X<br><br><span class="hljs-comment"># 实例化模型</span><br><span class="hljs-comment"># 如果你的输入是灰度图像，你可以这样实例化模型</span><br><span class="hljs-comment"># model = CNN(in_channels=1)</span><br><span class="hljs-comment"># 如果你的输入是彩色图像，你可以这样实例化模型</span><br><span class="hljs-comment"># model = CNN(in_channels=3)</span><br><span class="hljs-comment"># net.to(device)</span><br>net1 = CNN(in_channels=<span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(net1)<br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/pkoEjl4"><imgsrc="https://s21.ax1x.com/2024/07/17/pkoEjl4.jpg"alt="pkoEjl4.jpg" /></a></p><p>将模型进行训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">accuracy = train_and_evaluate(net1, trainloader, testloader, <span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>在上述代码片段中，对定义的卷积神经网络各个层的详细解释：</p><ul><li><p><font color = 'red'>卷积层 (Conv2d)</font>:每个卷积层都使用一个3x3的卷积核，输入通道数为3（对于彩色图像），输出通道数分别为32、64和128。padding=1确保输出特征图的尺寸与输入相同。</p><ul><li><p>卷积层的输出维度可以通过以下公式计算：</p><ul><li><p><span class="math inline">\(\text{OutputHeight}=\left[\frac{\text{InputHeight}+2\times\text{Padding}-\text{Kernel Size}}{\text{Stride}}+ 1\right]\)</span></p></li><li><p><span class="math inline">\(\text{OutputWidth}=\left[\frac{\text{Input Width}+2\times\text{Padding}-\text{KernelSize}}{\text{Stride}}+1\right]\)</span></p></li></ul></li><li><p>在代码中，卷积层的 padding=1 和stride=1（默认值），因此输出尺寸大致与输入尺寸相同（实际会稍有不同，取决于卷积核的覆盖范围）。</p></li></ul></li><li><p><font color = 'red'>激活层 (ReLU)</font>: 使用 ReLU激活函数。</p></li><li><p><font color = 'red'>池化层 (MaxPool2d)</font>: 使用 2x2的最大池化，步长为2。</p><ul><li>池化层会进一步减小特征图的尺寸。在代码中，池化层的 kernel_size=2 和stride=2，这意味着特征图的尺寸会减半。</li></ul></li><li><p><font color = 'red'>扁平化层 (Flatten)</font>:将多维特征图展平成一维向量，以便输入到全连接层。</p></li><li><p><font color = 'red'>全连接层 (Linear)</font>:包含两个全连接层，第一个将特征图的维度从 128 * 4 * 4减少到512，第二个将维度从512减少到256。</p></li><li><p><font color = 'red'>输出层 (Linear)</font>:最后一个全连接层将特征从256减少到10，对应于10个类别。</p></li></ul><h4 id="训练迭代过程以及训练准确率">训练迭代过程以及训练准确率</h4><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">1.899</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">1.511</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">1.392</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">1.288</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">1.197</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">1.094</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">1.036</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">1.000</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.962</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.915</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.832</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.837</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.799</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.779</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.755</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.647</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.658</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.662</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.658</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.659</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.523</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.546</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.546</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.552</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.551</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.411</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.431</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.445</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.439</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.452</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.317</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.317</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.345</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.363</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.385</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.231</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.246</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.256</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.277</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.275</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.157</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.189</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.195</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.215</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.208</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.107</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.138</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.146</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.159</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.189</span><br>在<span class="hljs-number">10</span>,<span class="hljs-number">000</span>张测试图像上的准确率：<span class="hljs-number">76.05</span><span class="hljs-comment">%</span><br></code></pre></td></tr></table></figure><h4 id="法二快速搭建法">法二：快速搭建法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># easy and fast way to build your network</span><br>net2 = torch.nn.Sequential(<br>    torch.nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>    torch.nn.ReLU(),<br>    torch.nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>),<br><br>    torch.nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>    torch.nn.ReLU(),<br>    torch.nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>),<br><br>    torch.nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>    torch.nn.ReLU(),<br>    torch.nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>),<br><br>    torch.nn.Flatten(),                    <span class="hljs-comment"># 扁平化层</span><br><br>    torch.nn.Linear(<span class="hljs-number">128</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">512</span>),<br>    torch.nn.ReLU(),<br><br>    torch.nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>),<br>    torch.nn.ReLU(),<br><br>    torch.nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>),<br>)<br><span class="hljs-built_in">print</span>(net2)<br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/pkoEv6J"><imgsrc="https://s21.ax1x.com/2024/07/17/pkoEv6J.jpg"alt="pkoEv6J.jpg" /></a></p><p>将模型进行训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">accuracy = train_and_evaluate(net2, trainloader, testloader, <span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><h4 id="训练迭代过程以及训练准确率-1">训练迭代过程以及训练准确率</h4><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">1.871</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">1.527</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">1.379</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">1.257</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">1.189</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">1.076</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">1.033</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.991</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.961</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.927</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.822</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.817</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.786</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.783</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.756</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.663</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.655</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.642</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.631</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.639</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.516</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.507</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.521</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.530</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.553</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.389</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.415</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.419</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.428</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.425</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.290</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.288</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.326</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.344</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.358</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.198</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.213</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.259</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.253</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.267</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.135</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.155</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.177</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.215</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.190</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.102</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.125</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.148</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.146</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.168</span><br>在<span class="hljs-number">10</span>,<span class="hljs-number">000</span>张测试图像上的准确率：<span class="hljs-number">75.35</span><span class="hljs-comment">%</span><br></code></pre></td></tr></table></figure><h4id="法三自定义构建法的另一种写法">法三：自定义构建法的另一种写法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Net, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.conv3 = nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.pool = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">128</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">512</span>)<br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">10</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv1(x)))<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv2(x)))<br>        x = <span class="hljs-variable language_">self</span>.pool(F.relu(<span class="hljs-variable language_">self</span>.conv3(x)))<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">128</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>)<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = F.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = <span class="hljs-variable language_">self</span>.fc3(x)<br>        <span class="hljs-keyword">return</span> x<br>net3 = Net()<br><span class="hljs-built_in">print</span>(net3)<br></code></pre></td></tr></table></figure><p><a href="https://imgse.com/i/pkoExX9"><imgsrc="https://s21.ax1x.com/2024/07/17/pkoExX9.jpg"alt="pkoExX9.jpg" /></a></p><p>将模型进行训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">accuracy = train_and_evaluate(net3, trainloader, testloader, <span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><h4 id="训练迭代过程以及训练准确率-2">训练迭代过程以及训练准确率</h4><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">1.896</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">1.507</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">1.385</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">1.277</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">1</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">1.215</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">1.078</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">1.047</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.995</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.970</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">2</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.915</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.811</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.786</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.785</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.771</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">3</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.774</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.650</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.637</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.644</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.635</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">4</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.643</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.516</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.521</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.524</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.528</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">5</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.512</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.399</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.412</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.428</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.439</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">6</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.424</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.288</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.314</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.325</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.348</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">7</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.351</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.222</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.233</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.263</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.272</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">8</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.273</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.147</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.180</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.194</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.213</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">9</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.235</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">100</span>] loss: <span class="hljs-number">0.125</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">200</span>] loss: <span class="hljs-number">0.120</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">300</span>] loss: <span class="hljs-number">0.145</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">400</span>] loss: <span class="hljs-number">0.166</span><br>[<span class="hljs-symbol">Epoch</span> <span class="hljs-number">10</span>, <span class="hljs-symbol">Batch</span> <span class="hljs-number">500</span>] loss: <span class="hljs-number">0.181</span><br>在<span class="hljs-number">10</span>,<span class="hljs-number">000</span>张测试图像上的准确率：<span class="hljs-number">76.03</span><span class="hljs-comment">%</span><br></code></pre></td></tr></table></figure><h3 id="测试模型预测效果">测试模型预测效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-comment"># 显示一批图像的函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">imshow</span>(<span class="hljs-params">img</span>):<br>    img = img / <span class="hljs-number">2</span> + <span class="hljs-number">0.5</span>  <span class="hljs-comment"># 反归一化</span><br>    npimg = img.numpy()<br>    plt.imshow(np.transpose(npimg, (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)))<br>    plt.show()<br><br><span class="hljs-comment"># 显示训练集中的前16个图像</span><br>dataiter = <span class="hljs-built_in">iter</span>(trainloader)<br>images, labels = <span class="hljs-built_in">next</span>(dataiter)<br><br><span class="hljs-comment"># 只显示前16个图像和标签</span><br>imshow(torchvision.utils.make_grid(images[:<span class="hljs-number">16</span>]))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;classes[labels[j]]:5s&#125;</span>&#x27;</span> <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">16</span>)))<br><br><span class="hljs-comment"># 可视化预测结果</span><br>dataiter = <span class="hljs-built_in">iter</span>(testloader)<br>images, labels = <span class="hljs-built_in">next</span>(dataiter)<br><span class="hljs-comment"># images, labels = images.to(device), labels.to(device)</span><br>outputs = net1(images)<br>_, predicted = torch.<span class="hljs-built_in">max</span>(outputs, <span class="hljs-number">1</span>)<br><br>newimage = images.cpu().detach()<br><br>imshow(torchvision.utils.make_grid(newimage))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;真实标签: &#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;classes[labels[j]]:5s&#125;</span>&#x27;</span> <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;预测标签:   &#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;classes[predicted[j]]:5s&#125;</span>&#x27;</span> <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)))<br></code></pre></td></tr></table></figure><p>得到预测结果如下图：</p><p><a href="https://imgse.com/i/pkoVSmR"><imgsrc="https://s21.ax1x.com/2024/07/17/pkoVSmR.jpg"alt="pkoVSmR.jpg" /></a></p><h3 id="注输出通道">注：输出通道</h3><blockquote><p>在卷积神经网络（CNN）中，输出通道数（out_channels）是一个超参数，通常由网络设计者根据需要人为设置。它不是通过计算公式自动得到的，而是基于经验、实验或特定任务的需求来选择的。</p></blockquote><h4 id="输出通道数的作用">输出通道数的作用</h4><ul><li>特征检测：每个输出通道对应一种不同的特征检测器。增加输出通道数可以帮助网络学习到更丰富的特征。</li><li>网络深度：增加输出通道数可以增加网络的深度，有助于提取更复杂的特征，但同时也会增加计算复杂度。</li></ul><h4 id="如何选择输出通道数">如何选择输出通道数</h4><ul><li>经验法则：通常从较小的输出通道数开始，例如16、32、64等，然后根据任务的复杂性逐步增加。</li><li>实验：通过实验不同的输出通道数，观察对模型性能的影响，选择最优的配置。</li><li>任务需求：对于更复杂的任务，可能需要更多的输出通道来捕捉更多的特征。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>CIFAR-10</tag>
      
      <tag>多分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DL基础之Batch、Epoch和Iteration</title>
    <link href="/2024/07/16/DL%E5%9F%BA%E7%A1%80%E4%B9%8BBatch%E3%80%81Epoch%E5%92%8CIteration/"/>
    <url>/2024/07/16/DL%E5%9F%BA%E7%A1%80%E4%B9%8BBatch%E3%80%81Epoch%E5%92%8CIteration/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="f5f9ee43f5e438ab8d638f8f03fbfae45141720c37a33c594b7f2a63ca0b3dd3">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">文章正在抢救中.....</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <categories>
      
      <category>Deep_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随手记录</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习实践之手写数字识别</title>
    <link href="/2024/07/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E4%B9%8B%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    <url>/2024/07/15/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E4%B9%8B%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h2 id="dnn实现手写数字识别具体步骤">DNN实现手写数字识别具体步骤</h2><p>深度学习在图像识别领域取得了巨大的成功，其中卷积神经网络（CNN）是最常见的模型之一，特别是在图像分类任务中。然而，即使不使用卷积神经网络，也可以通过其他类型的神经网络实现手写数字识别。以下是使用简单的多层感知器（DNN）实现手写数字识别的步骤：</p><h3 id="数据准备">数据准备：</h3><p>首先，需要一个手写数字数据集，如MNIST数据集。这个数据集包含60,000个训练样本和10,000个测试样本，每个样本是一个28x28像素的灰度图像，代表0到9的数字。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets,transforms<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment">## 固定随机数种子，使得结果可以复现</span><br>torch.manual_seed(<span class="hljs-number">1024</span>)<br><br><span class="hljs-comment"># 定义数据预处理</span><br>transform = transforms.Compose([<br>    transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.5</span>,),(<span class="hljs-number">0.5</span>,))<br>])<br><br><span class="hljs-comment"># 加载数据集 mnist数据集在pytorch中有集成，可以直接使用，放在./data目录中</span><br><span class="hljs-comment"># 加载数据集</span><br>train_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;./data&#x27;</span>,train=<span class="hljs-literal">True</span>,transform=transforms.ToTensor(),download=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># load test set</span><br>test_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;./data&#x27;</span>,train=<span class="hljs-literal">False</span>,transform=transforms.ToTensor(),download=<span class="hljs-literal">False</span>)<br><br>train_loader = DataLoader(train_dataset,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">True</span>) <span class="hljs-comment"># 对训练集进行打包，指定批次为64</span><br>test_loader = DataLoader(test_dataset,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></p><h3 id="构建dnn模型">构建DNN模型：</h3><p>构建一个卷积神经网络模型，通常包含输入层、多个隐藏层和输出层。输入层的节点数应与图像的像素总数（784个，即28x28）相匹配。隐藏层可以使用ReLU激活函数，利用池化层获取更到特征，而输出层可以使用softmax激活函数，以进行多类分类。</p><p><a href="https://imgse.com/i/pkIcbwQ"><imgsrc="https://s21.ax1x.com/2024/07/16/pkIcbwQ.jpg"alt="pkIcbwQ.jpg" /></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型定义</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CNN</span>(<span class="hljs-title class_ inherited__">Module</span>):<br>    <span class="hljs-comment"># 定义模型属性</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_channels</span>):<br>        <span class="hljs-built_in">super</span>(CNN, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-comment"># 输入到隐层 1</span><br>        <span class="hljs-variable language_">self</span>.hidden1 = Conv2d(n_channels, <span class="hljs-number">32</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br>        kaiming_uniform_(<span class="hljs-variable language_">self</span>.hidden1.weight, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>        <span class="hljs-variable language_">self</span>.act1 = ReLU()<br>        <span class="hljs-comment"># 池化层 1</span><br>        <span class="hljs-variable language_">self</span>.pool1 = MaxPool2d((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br>        <br>        <span class="hljs-comment"># 隐层 2</span><br>        <span class="hljs-variable language_">self</span>.hidden2 = Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))<br>        kaiming_uniform_(<span class="hljs-variable language_">self</span>.hidden2.weight, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>        <span class="hljs-variable language_">self</span>.act2 = ReLU()<br>        <span class="hljs-comment"># 池化层 2</span><br>        <span class="hljs-variable language_">self</span>.pool2 = MaxPool2d((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), stride=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br>        <br>        <span class="hljs-comment"># 全连接层</span><br>        <span class="hljs-variable language_">self</span>.hidden3 = Linear(<span class="hljs-number">5</span>*<span class="hljs-number">5</span>*<span class="hljs-number">32</span>, <span class="hljs-number">100</span>)<br>        kaiming_uniform_(<span class="hljs-variable language_">self</span>.hidden3.weight, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>        <span class="hljs-variable language_">self</span>.act3 = ReLU()<br>        <span class="hljs-comment"># 全连接层不需要再池化</span><br>        <br>        <span class="hljs-comment"># 输出层</span><br>        <span class="hljs-variable language_">self</span>.hidden4 = Linear(<span class="hljs-number">100</span>, <span class="hljs-number">10</span>)<br>        xavier_uniform_(<span class="hljs-variable language_">self</span>.hidden4.weight)<br>        <span class="hljs-variable language_">self</span>.act4 = Softmax(dim=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 前向传播</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-comment"># 输入到隐层 1</span><br>        X = <span class="hljs-variable language_">self</span>.hidden1(X)<br>        X = <span class="hljs-variable language_">self</span>.act1(X)<br>        X = <span class="hljs-variable language_">self</span>.pool1(X)<br>        <span class="hljs-comment"># 隐层 2</span><br>        X = <span class="hljs-variable language_">self</span>.hidden2(X)<br>        X = <span class="hljs-variable language_">self</span>.act2(X)<br>        X = <span class="hljs-variable language_">self</span>.pool2(X)<br>        <span class="hljs-comment"># 扁平化</span><br>        X = X.view(-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>*<span class="hljs-number">4</span>*<span class="hljs-number">50</span>)<br>        <span class="hljs-comment"># 隐层 3</span><br>        X = <span class="hljs-variable language_">self</span>.hidden3(X)<br>        X = <span class="hljs-variable language_">self</span>.act3(X)<br>        <span class="hljs-comment"># 输出层</span><br>        X = <span class="hljs-variable language_">self</span>.hidden4(X)<br>        X = <span class="hljs-variable language_">self</span>.act4(X)<br>        <span class="hljs-keyword">return</span> X<br></code></pre></td></tr></table></figure><h3 id="模型训练">模型训练：</h3><ul><li>使用交叉熵损失函数来衡量预测值与实际标签之间的差异。</li><li>选择合适的优化器，如Adam或SGD，并设置学习率。</li><li>在训练数据上训练模型，并在验证数据上评估其性能。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python">net = Net()<br><span class="hljs-built_in">print</span>(net)<br><br><span class="hljs-comment"># 定义损失函数和优化器</span><br><span class="hljs-comment"># 损失函数：交叉损失函数， CrossEntropyLoss</span><br>criterion = nn.CrossEntropyLoss()<br><span class="hljs-comment"># 优化： 使用Adam优化器</span><br>optimizer = optim.Adam(net.parameters(), lr=<span class="hljs-number">0.001</span>)<br><br><span class="hljs-comment"># 用于保存训练过程中的损失和准确率</span><br>train_losses = []<br>train_accuracies = []<br>test_accuracies = []<br><br><span class="hljs-comment"># 训练模型部分</span><br>epochs = <span class="hljs-number">10</span><br>best_accuracy = <span class="hljs-number">0.0</span><br>best_model_path = <span class="hljs-string">&#x27;best_mnist_path.pth&#x27;</span> <span class="hljs-comment"># 最佳权重路径</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    running_loss = <span class="hljs-number">0.0</span><br>    correct_train = <span class="hljs-number">0</span>  <span class="hljs-comment"># 正确预测的数量</span><br>    total_train = <span class="hljs-number">0</span>    <span class="hljs-comment"># 样本总数</span><br><br>    <span class="hljs-comment"># 训练过程</span><br>    net.train()<br>    <span class="hljs-keyword">for</span> inputs,labels <span class="hljs-keyword">in</span> train_loader:<br>        optimizer.zero_grad()             <span class="hljs-comment"># 梯度清零</span><br>        outputs = net(inputs)             <span class="hljs-comment"># 前向传播</span><br>        loss = criterion(outputs,labels)  <span class="hljs-comment"># 计算损失</span><br>        loss.backward()                   <span class="hljs-comment"># 反向传播</span><br>        optimizer.step()                  <span class="hljs-comment"># 更新参数</span><br>        running_loss += loss.item()       <span class="hljs-comment"># 累加损失</span><br><br>        <span class="hljs-comment"># 计算训练集的准确率</span><br>        _,predicted = torch.<span class="hljs-built_in">max</span>(outputs,<span class="hljs-number">1</span>) <span class="hljs-comment"># 获取预测结果及其下标 【0,0,0.1,0.2,0.2,0,0,0.5,0,0】</span><br>        total_train += labels.size(<span class="hljs-number">0</span>)      <span class="hljs-comment"># 累加样本数量</span><br>        correct_train += (predicted == labels).<span class="hljs-built_in">sum</span>().item() <span class="hljs-comment"># 累加正确预测的数量</span><br>        <span class="hljs-comment"># 计算训练集上的准确率</span><br>    train_accuracy = correct_train / total_train<br>    train_losses.append(running_loss / <span class="hljs-built_in">len</span>(train_loader))<br>    train_accuracies.append((train_accuracy))<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span> / <span class="hljs-subst">&#123;epochs&#125;</span>, Loss: <span class="hljs-subst">&#123;running_loss / <span class="hljs-built_in">len</span>(train_loader):<span class="hljs-number">.4</span>f&#125;</span>,Train Accuracy :<span class="hljs-subst">&#123;train_accuracy:<span class="hljs-number">.2</span>%&#125;</span>&quot;</span>)<br>    net.<span class="hljs-built_in">eval</span>()<br>    correct = <span class="hljs-number">0</span><br>    total = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> inputs,labels <span class="hljs-keyword">in</span> test_loader:<br>            outputs = net(inputs)<br>            _,predicted = torch.<span class="hljs-built_in">max</span>(outputs,<span class="hljs-number">1</span>) <span class="hljs-comment"># 获取预测结果及其下标 【0,0,0.1,0.2,0.2,0,0,0.5,0,0】</span><br>            total += labels.size(<span class="hljs-number">0</span>)      <span class="hljs-comment"># 累加样本数量</span><br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br>    test_accuracy = correct / total<br>    test_accuracies.append(test_accuracy)<span class="hljs-comment">#记录每个epoch的测试集准确率</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>/<span class="hljs-subst">&#123;epochs&#125;</span>,Test Accuracy:<span class="hljs-subst">&#123;test_accuracy:<span class="hljs-number">.2</span>%&#125;</span>&quot;</span>)<br><br>    <span class="hljs-comment">#如果测试集准确率提高，保存当前模型的权重</span><br>    <span class="hljs-keyword">if</span> test_accuracy &gt; best_accuracy:<br>        best_accuracy = test_accuracy<br>        torch.save(net.state_dict(),best_model_path)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Best model saved with accuracy:<span class="hljs-subst">&#123;best_accuracy:<span class="hljs-number">.2</span>%&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="模型评估与可视化">模型评估与可视化：</h3><p>在测试集上评估模型的准确性，以确保模型具有良好的泛化能力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Best Accuracy on test set:<span class="hljs-subst">&#123;best_accuracy:<span class="hljs-number">.2</span>%&#125;</span>&quot;</span>)<br><span class="hljs-comment">#绘制并保存损失和准确率曲线</span><br>plt.figure(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">5</span>))<br><span class="hljs-comment">#绘制损失曲线</span><br>plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)                              <span class="hljs-comment"># 选择第一个子图</span><br>plt.plot(train_losses,label=<span class="hljs-string">&#x27;Training Loss&#x27;</span>)    <span class="hljs-comment"># 传入数据、设置标签为Training Loss</span><br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)                             <span class="hljs-comment"># x轴数据</span><br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training Loss over Epochs&#x27;</span>)          <span class="hljs-comment"># 设置标签</span><br>plt.legend()                                    <span class="hljs-comment"># 添加图例</span><br>plt.grid(<span class="hljs-literal">True</span>)                                  <span class="hljs-comment"># 添加网格</span><br><span class="hljs-comment">#绘制训练集和测试集准确率曲线</span><br>plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>plt.plot(train_accuracies,label=<span class="hljs-string">&#x27;Train Accuracy&#x27;</span>)<br>plt.plot(test_accuracies,label=<span class="hljs-string">&#x27;Test Accuracy&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Accuracy&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Train and Test Accuracy over Epochs&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br><span class="hljs-comment">#保存图像</span><br>plt.tight_layout()<br>plt.savefig(<span class="hljs-string">&#x27;loss_and_accuracy_curves.png&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep_Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多分类</tag>
      
      <tag>mnist手写数字识别</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>assignment_3_transformer</title>
    <link href="/2024/07/13/assignment-3-transformer/"/>
    <url>/2024/07/13/assignment-3-transformer/</url>
    
    <content type="html"><![CDATA[<h1 id="transformer">Transformer</h1><p><a href="https://imgse.com/i/pk5NGOx"><imgsrc="https://s21.ax1x.com/2024/07/15/pk5NGOx.png"alt="pk5NGOx.png" /></a></p><h2 id="输入部分">输入部分</h2><p><a href="https://imgse.com/i/pk50vOs"><imgsrc="https://s21.ax1x.com/2024/07/15/pk50vOs.jpg"alt="pk50vOs.jpg" /></a></p><h3 id="输入向量化input-embedding">输入向量化(input embedding)</h3><ul><li>嵌入算法将每个输入词转换为向量,编码器接收向量列表作为输入。它通过将这些向量传递到“自我注意”层来处理此列表，然后传递到前馈神经网络，然后将输出向上发送到下一个编码器。</li></ul><p><a href="https://imgse.com/i/pk5DY2F"><imgsrc="https://s21.ax1x.com/2024/07/15/pk5DY2F.jpg"alt="pk5DY2F.jpg" /></a></p><h3 id="位置编码position-embedding">位置编码(position embedding)</h3><h4 id="位置编码的数学表达">位置编码的数学表达</h4><p>对于每个位置 <span class="math inline">\(p\)</span>的编码，可以使用以下公式生成：</p><p><spanclass="math inline">\(\begin{aligned}PE(pos,2i)&amp;=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\\\PE(pos,2i+1)&amp;=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\end{aligned}\)</span></p><p>其中： * <span class="math inline">\(PE\)</span> 是位置编码矩阵。 *<span class="math inline">\(pos\)</span>是词在序列中的位置。 * <spanclass="math inline">\(i\)</span> 是维度索引。 * <spanclass="math inline">\(d_model\)</span> 是模型的维度。</p><h4 id="位置编码的基本原理">位置编码的基本原理</h4><p>位置编码通常添加到词嵌入（WordEmbedding）之上，确保模型能够利用每个单词在序列中的位置信息。位置编码通常使用正弦和余弦函数的组合来实现，这样可以保证编码的平滑性和周期性。</p><h4 id="位置编码的作用">位置编码的作用</h4><p>位置编码使得Transformer模型能够：</p><ul><li>捕捉长距离依赖关系，即使在很长的序列中也能保持有效。</li><li>理解序列中单词的相对位置，这对于语言的语义理解至关重要</li></ul><h4 id="位置编码的实现">位置编码的实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionalEncoding</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, dropout, max_len=<span class="hljs-number">5000</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        执行论文的PE方程:</span><br><span class="hljs-string">            PE(pos,2i) = sin(pos/10000^(2i/d_model))</span><br><span class="hljs-string">            PE(pos,2i+1) = cos(pos/10000^(2i/d_model))</span><br><span class="hljs-string">        由于Transformer不包含递归和卷积，为了让模型利用序列的顺序，必须注入一些关于标记在序列中的相对或绝对位置的信息。</span><br><span class="hljs-string">        为此，在编码器和解码器堆栈底部的输入嵌入中添加了“位置编码”，作者使用不同频率的正弦和余弦函数实现。</span><br><span class="hljs-string">        :param d_model: (int) embedding后词向量的维度</span><br><span class="hljs-string">        :param dropout: (int) 丢弃机制</span><br><span class="hljs-string">        :param max_len: (int) 最大长度</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(PositionalEncoding, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(p=dropout)<br>        <span class="hljs-comment"># 在对数空间计算一次位置编码.</span><br>        pe = torch.zeros(max_len, d_model)<br>        position = torch.arange(<span class="hljs-number">0</span>, max_len).unsqueeze(<span class="hljs-number">1</span>)<br>        div_term = torch.exp(torch.arange(<span class="hljs-number">0</span>, d_model, <span class="hljs-number">2</span>) * -(math.log(<span class="hljs-number">10000.0</span>) / d_model))<br>        pe[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = torch.sin(position * div_term)<br>        pe[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = torch.cos(position * div_term)<br>        pe = pe.unsqueeze(<span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&#x27;pe&#x27;</span>, pe)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = x + Variable(<span class="hljs-variable language_">self</span>.pe[:, :x.size(<span class="hljs-number">1</span>)], requires_grad=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.dropout(x)<br><br></code></pre></td></tr></table></figure><h2 id="编码层encoder-layer">编码层(Encoder layer)</h2><h3 id="注意力机制attention">注意力机制：Attention</h3><ul><li><p>注意力机制（AttentionMechanism）是深度学习领域中一种重要的模型结构，它模仿了人类的注意力行为。它的名字来源于人类的注意力，指的人能够有意识和主动地关注某个对象。</p></li><li><p>人的注意力是可以自下而上，也可以绑定一个任务至上而下。如我们看下一张图片的，注意力一般会优先集中在桌面和柜子的物体上，这是自下而上的，但如果我们有饮食的目的，注意力会进一步聚焦到红框所展示的食物饮料上，这是自上而下的。而深度学习中的attention模仿的是后者。</p></li></ul><p><a href="https://imgse.com/i/pk5NYm6"><imgsrc="https://s21.ax1x.com/2024/07/15/pk5NYm6.jpg"alt="pk5NYm6.jpg" /></a></p><ul><li>从这个例子可以引申出attention中比较抽象的三个定义，我们以饮食为目的，想找寻“食物”，这里“食物”就可以看作query，图片各个区域的像素就是value，它们有自己对应的物品key。<strong>注意力机制就是优先关注key与query更加相似的value</strong>，比如在这个场景下，就是图片中以水果、蛋糕为代表的下午茶部分，而且最终提取出的信息（attentionvalue）着重来自于这几个关键信息的加权。</li></ul><p>在人类的注意力行为中，我们可以有意识地关注某个对象，这种注意力可以是自下而上的，也可以是自上而下的。例如，在观看一张图片时，我们的注意力可能会首先集中在图片中的某些显著物体上（自下而上），而如果我们有特定的目标，比如寻找食物，我们的注意力就会进一步聚焦到与食物相关的物体上（自上而下）。</p><p>在深度学习中，注意力机制主要模仿的是自上而下的过程。具体来说，注意力机制涉及到以下几个关键概念：</p><ul><li>Query（查询）：这是我们想要关注的目标或对象，比如在寻找食物的场景中，"食物"就是Query。</li><li>Value（值）：这是输入数据中的元素，它们各自拥有对应的信息。在图片中，Value可以是图片的各个区域的像素。</li><li>Key（键）：与Value相对应，每个Value都有一个Key，它们用来标识Value。</li></ul><p>注意力机制的核心思想是，系统会优先关注那些与Query更加相似的Value。这种相似度是通过计算Query和Key之间的相似度来确定的，通常使用向量之间的点积来衡量。两个向量的方向越相近，夹角越小，它们的相似度就越大。这个相似度可以用来作为Value的权重，通过加权求和的方式，生成一个新的输出向量，这个输出向量可以看作是Query在当前场景下更具体的指代。#### attention 的计算 *向量内积可以解释为key向量在query向量方向的投影距离与query向量长度的乘积，这在很大程度上反义了向量相似度。因为两个向量越相关投影就会越长，完全不相关时夹角为90度，对应内积为0。* 除以$ <spanclass="math inline">\(以消除计算内积时的求和对方差的扩大效应,其中\)</span>d_k$为维度 * 归一。Softmax的公式为 $ (x_{i})= $可以将向量中各元素变为和为1的权重，实现对value向量的加权求和。 *乘以V矩阵，此矩阵由 <span class="math inline">\(m\)</span> 个长度为<spanclass="math inline">\(d_k\)</span>的value向量组成<spanclass="math inline">\(m*d_k\)</span>的矩阵，作为attention的最终输出。</p><p>注意力机制的数学表达可以通过以下公式来描述：</p><p>$ (Q, K, V)=() V $</p><p>注意力机制在自然语言处理、图像处理、机器翻译等多个领域都有广泛的应用，它能够使模型更加灵活地处理序列数据，提高模型的性能和泛化能力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">attention</span>(<span class="hljs-params">query, key, value, mask=<span class="hljs-literal">None</span>, dropout=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算&#x27;Scaled Dot Product Attention&#x27;，输入由维度为dk的query,key以及维度为dv的values组​​成</span><br><span class="hljs-string">    # 注意力函数可以描述为将一个query和一组key对映射到一个output，其中query、keys、values和output都是向量。</span><br><span class="hljs-string">    # 输出为values的加权总和，其中分配给每个值的权重由query与相应key的相关性函数计算。</span><br><span class="hljs-string">    # 计算key和query的点积，将每个key除以√dk，并应用 softmax 函数来获得值的权重。</span><br><span class="hljs-string">    # 在实践中，同时计算一组query的注意力函数，打包成一个矩阵Q</span><br><span class="hljs-string">    :param query: (Tensor)query矩阵</span><br><span class="hljs-string">    :param key: (Tensor)key矩阵</span><br><span class="hljs-string">    :param value: (Tensor)value矩阵</span><br><span class="hljs-string">    :param mask:</span><br><span class="hljs-string">    :param dropout: (int)丢弃机制</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    d_k = query.size(-<span class="hljs-number">1</span>)<br>    scores = torch.matmul(query, key.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)) \<br>             / math.sqrt(d_k)<br>    <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        scores = scores.masked_fill(mask == <span class="hljs-number">0</span>, -<span class="hljs-number">1e9</span>)<br>    p_attn = F.softmax(scores, dim=-<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> dropout <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        p_attn = dropout(p_attn)<br>    <span class="hljs-keyword">return</span> torch.matmul(p_attn, value), p_attn<br><br></code></pre></td></tr></table></figure><h4 id="自注意力机制self-attention">自注意力机制：Self-Attention</h4><p>在自注意力机制(<spanclass="math inline">\(self-attention\)</span>)中，<spanclass="math inline">\(Q、K、V\)</span>全都来自原始文本<spanclass="math inline">\(X\)</span>，之前会先乘以一个可训练的矩阵作为缓冲，将此映射到不同的空间提升模型的拟合能力。</p><p>换句话说，对于一段文本，我们以续写此文本为目的，关注每一个词在整段语境下的<spanclass="math inline">\(attentionvalue\)</span>，我们以下面一段话为例模拟一下自然语言大模型的的学习过程：</p><blockquote><p>你是信的开头，诗的内容，童话的结尾。你是理所当然的奇迹，你是月色真美。你是圣诞老人送给我好孩子的礼物，你是三千美丽世界里，我的一瓢水。”</p></blockquote><p>在自然语言处理的大型模型中，最小的语义单元被称为token。例如，"结尾"这个词可以被视为一个token，它会被赋予一个特定的数值标记，并转化为一个查询向量。在之前的讨论中，我们了解到注意力机制的输出向量可以代表这个场景下query的具体含义。如果模型识别出"结尾"是用来修饰另一个token"童话"的，那么它们之间就会因为相似性而建立联系，"童话"的词向量会以一定的权重被加到query"结尾"上，使得"结尾"的词向量从原本的中性含义向正面含义倾斜。因为童话的结局通常都是美好的，所以"结尾"的词向量在这种语境下会带有积极的色彩。</p><p>然而，大型模型的分析并没有就此停止。模型可以再次检查这些经过调整的词向量，并进行新一轮的注意力计算。在这个过程中，模型可能发现原本中性的词汇如"开头"、"内容"、"结尾"现在都带有了正面的色彩，它们之间的相似度因此增加，模型通过这种新的注意力机制赋予了它们一种排比的修辞效果。</p><p>经过多轮的迭代计算，模型最终识别出这些排比句共同隐含的主题——美好的爱情。基于这一发现，模型接着生成了下一个token，以延续这个主题。</p><h4id="多头注意力机制multiheadattention">多头注意力机制：MultiHeadAttention</h4><p>一组 <span class="math inline">\(Q,K,V\)</span>得到了一组当前词的特征表达,类似卷积神经网络中的filter,提取更多种特征</p><p>每一个词向量有多组<spanclass="math inline">\(Q_i,K_i,V_i\)</span>,分别计算并拼接</p><ul><li>multi-headed结果：不同的注意力结果，得到的特征向量表达也不相同<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadedAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, h, d_model, dropout=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        计算多头注意力机制，其实就是将&#x27;Scaled Dot Product Attention&#x27;重复h次</span><br><span class="hljs-string">        # 两个最常用的注意力函数是加法注意力 （cite）和点积（乘法）注意力。点积注意力与我们的算法相同</span><br><span class="hljs-string">        # 多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。</span><br><span class="hljs-string">        :param h: (int)重复次数</span><br><span class="hljs-string">        :param d_model: (int) embedding后词向量维度</span><br><span class="hljs-string">        :param dropout: (int)丢弃机制</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(MultiHeadedAttention, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-keyword">assert</span> d_model % h == <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 假设 d_v 总是等于 d_k</span><br>        <span class="hljs-variable language_">self</span>.d_k = d_model // h<br>        <span class="hljs-variable language_">self</span>.h = h<br>        <span class="hljs-variable language_">self</span>.linears = clones(nn.Linear(d_model, d_model), <span class="hljs-number">4</span>)<br>        <span class="hljs-variable language_">self</span>.attn = <span class="hljs-literal">None</span><br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(p=dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, query, key, value, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        前向传播，对应论文多头注意力那个图</span><br><span class="hljs-string">        :param query: (Tensor)query矩阵</span><br><span class="hljs-string">        :param key: (Tensor)key矩阵</span><br><span class="hljs-string">        :param value: (Tensor)value矩阵</span><br><span class="hljs-string">        :param mask:</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            mask = mask.unsqueeze(<span class="hljs-number">1</span>)<br>        nbatches = query.size(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 1) 做线性变换获取query, key, value，模型维度（d_model） 为 h x d_k</span><br>        query, key, value = [l(x).view(nbatches, -<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.h, <span class="hljs-variable language_">self</span>.d_k).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>                             <span class="hljs-keyword">for</span> l, x <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-variable language_">self</span>.linears, (query, key, value))]<br><br>        <span class="hljs-comment"># 2) 对每个batch所有向量施加注意力.</span><br>        x, <span class="hljs-variable language_">self</span>.attn = attention(query, key, value, mask=mask, dropout=<span class="hljs-variable language_">self</span>.dropout)<br><br>        <span class="hljs-comment"># 3) 使用view函数和全连接层实现&quot;Concat&quot;.</span><br>        x = x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous().view(nbatches, -<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.h * <span class="hljs-variable language_">self</span>.d_k)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.linears[-<span class="hljs-number">1</span>](x)<br></code></pre></td></tr></table></figure></li><li>每个位置上的单词都会经过一个自注意力过程。然后，它们会分别经过一个前馈神经网络——完全相同的网络，每个向量都会分别流过它。</li></ul><h3 id="求和与归一化add-norm">求和与归一化(Add &amp; Norm)</h3><p>Add和Norm层是由Add和Norm两部分构成的计算公式如下：</p><p><spanclass="math display">\[LayerNorm(X+MultiHeadAttention(X))\]</span></p><p><span class="math display">\[LayerNorm(X+FeedForward(X))\]</span></p><ul><li>X表示多头或者前馈神经网络的输入，MultiHeadAttention(X)和FeedForward(X)表示输出（输出与输入X维度是一样的，所以可以相加)。</li><li>X+MultiHeadAttention(X)是一种残差连接，通常用于解决多层网络梯度消失和梯度爆炸的问题，可以让网格只关注当前差异的部分，在ResNet中经常用到，公式简单表达就是:</li></ul><p><span class="math display">\[输出=输入十F(输入)\]</span></p><p><a href="https://imgse.com/i/pk5UlE8"><imgsrc="https://s21.ax1x.com/2024/07/15/pk5UlE8.png"alt="pk5UlE8.png" /></a></p><h3 id="feedforward前馈神经网络">Feedforward前馈神经网络</h3><p>前馈神经网络层结构相对比较简单，是一个两层的全连接层，第一层的激活函数为Relu,第二层不使用激活函数，公式如下</p><p><span class="math display">\[output = max(0,X*W_1 + b_1)*W_2 +b_2\]</span></p><p><a href="https://imgse.com/i/pk56nJ0"><imgsrc="https://s21.ax1x.com/2024/07/15/pk56nJ0.png"alt="pk56nJ0.png" /></a></p><h2 id="解码层decoder-layer">解码层(Decoder Layer)</h2><p>decode部分和encode部分相似，但是也存在一些区别 *decode侧有两个multi--head attention层 *decode的第一个多头采用了mask操作，而第二个多头的k,v矩阵使用的是encode侧编码信息矩阵c(encode的输出)计算，而q则是第一个decode侧的多头的输出计算 *最后包含一个softmax层计算下一个翻译单词的概率（假设是翻译任务）</p><h3 id="第一个多头注意力带有mask">第一个多头注意力（带有mask）</h3><p><imgsrc="https://pic.imgdb.cn/item/6694858fd9c307b7e9c0505b.png" /></p><ul><li>流程就是：首先输入<Begin>作为decode侧的开始标志预测第一个单词，得到I后，然后将"<Begin>"作为下一次预测的输入，以此类推</li><li>具体操作其实是加了一个与输入矩阵形状相同的ask矩阵，这样就可以把每一次计算当前时间后的信息盖住；</li><li>然后的操作就是和之前的自注意力一样，通过输入矩阵×计算得到q,k,v矩阵然后计算q和k转置的乘积，但是再softmax之前需要乘以mask矩阵，如下图所示</li><li>得到的maskQKAT进行softmax就会发现单词0那一行1234的位置都是0，因为我们设置的是负无穷映射后就成0了，再去与矩阵V得到矩阵Z</li></ul><p><a href="https://imgse.com/i/pk5aJIO"><imgsrc="https://s21.ax1x.com/2024/07/15/pk5aJIO.png"alt="pk5aJIO.png" /></a></p><p><a href="https://imgse.com/i/pk5atiD"><imgsrc="https://s21.ax1x.com/2024/07/15/pk5atiD.png"alt="pk5atiD.png" /></a></p><p><a href="https://imgse.com/i/pk5aNJe"><imgsrc="https://s21.ax1x.com/2024/07/15/pk5aNJe.png"alt="pk5aNJe.png" /></a></p><p><strong>此处我有一个一直没看懂的疑问，为啥要加mask,我查阅了很多文章都是说为了让模型预测第i+1的时候只能使用+1前的信息，但是场景类似于完型填空，因为selfattention是使用的上下文信息是全文的，所以它是提前知道信息的，所以需要加mask,但是比如这个翻译，是怎么提前看到的呢？搞了半天才明白一个重点，就是上面讲述的decode侧的输入，train的时候decode侧输入的是正确答案，所以要mask</strong></p><h3 id="第二个多头注意力">第二个多头注意力</h3><ul><li>这个多头的区别就是输入，自注意力的k,v矩阵使用的encode侧输出的编码信息矩阵c,第一个多头提供的输出计算q,(train的时候)此处的q就是加了掩码的正确信息，而kv是原始信息，好处就是q这边的每一个信息都可以利用上encode侧的所有信息，这些信息是不需要mask的 ### softmax预测输出 <ahref="https://imgse.com/i/pk5aBLt"><imgsrc="https://s21.ax1x.com/2024/07/15/pk5aBLt.png"alt="pk5aBLt.png" /></a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderDecoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        标准的encoder-decoder架构</span><br><span class="hljs-string">        :param encoder: (nn.Module) transformer编码器模型</span><br><span class="hljs-string">        :param decoder: (nn.Module) transformer解码器模型</span><br><span class="hljs-string">        :param src_embed: (nn.Module) 输入词向量（embedding层）</span><br><span class="hljs-string">        :param tgt_embed: (nn.Module) 目标词向量（embedding层）</span><br><span class="hljs-string">        :param generator: (nn.Module) 生成器，实现transformer的decoder最后的linear+softmax</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(EncoderDecoder, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.encoder = encoder<br>        <span class="hljs-variable language_">self</span>.decoder = decoder<br>        <span class="hljs-variable language_">self</span>.src_embed = src_embed<br>        <span class="hljs-variable language_">self</span>.tgt_embed = tgt_embed<br>        <span class="hljs-variable language_">self</span>.generator = generator<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, src, tgt, src_mask, tgt_mask</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        喂入和处理masked src和目标序列.</span><br><span class="hljs-string">        decode的输入是encode输出，目标词向量</span><br><span class="hljs-string">        :param src: (Tensor) 输入词向量</span><br><span class="hljs-string">        :param tgt: (Tensor) 输出词向量</span><br><span class="hljs-string">        :param src_mask:</span><br><span class="hljs-string">        :param tgt_mask:</span><br><span class="hljs-string">        :return: (nn.Module) 整个transformer模型</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.decode(<span class="hljs-variable language_">self</span>.encode(src, src_mask), src_mask, tgt, tgt_mask)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, src, src_mask</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.encoder(<span class="hljs-variable language_">self</span>.src_embed(src), src_mask)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, memory, src_mask, tgt, tgt_mask</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.decoder(<span class="hljs-variable language_">self</span>.tgt_embed(tgt), memory, src_mask, tgt_mask)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Generator</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;定义标准的linear + softmax生成器.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, vocab</span>):<br>        <span class="hljs-built_in">super</span>(Generator, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.proj = nn.Linear(d_model, vocab)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> F.log_softmax(<span class="hljs-variable language_">self</span>.proj(x), dim=-<span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_model</span>(<span class="hljs-params">src_vocab, tgt_vocab, N=<span class="hljs-number">6</span>, d_model=<span class="hljs-number">512</span>, d_ff=<span class="hljs-number">2048</span>, h=<span class="hljs-number">8</span>, dropout=<span class="hljs-number">0.1</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    构建整体模型</span><br><span class="hljs-string">    :param src_vocab: (int) 输入词向量尺寸</span><br><span class="hljs-string">    :param tgt_vocab: (int) 输出词向量尺寸</span><br><span class="hljs-string">    :param N: (int, default=6) 编解码层的重复次数</span><br><span class="hljs-string">    :param d_model: (int, default=512) embedding后词向量维度</span><br><span class="hljs-string">    :param d_ff: (int, default=2048) 编解码器内层维度</span><br><span class="hljs-string">    :param h: (int, default=8) &#x27;Scaled Dot Product Attention&#x27;，使用的次数</span><br><span class="hljs-string">    :param dropout: (int, default=0.1) 丢弃机制，正则化的一种方式，默认为0.1</span><br><span class="hljs-string">    :return: (nn.Module) 整个transformer模型</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    c = copy.deepcopy<br>    attn = MultiHeadedAttention(h, d_model)<br>    ff = PositionwiseFeedForward(d_model, d_ff, dropout)<br>    position = PositionalEncoding(d_model, dropout)<br>    model = EncoderDecoder(<br>        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),<br>        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),<br>        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),<br>        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),<br>        Generator(d_model, tgt_vocab))<br><br>    <span class="hljs-comment"># 下面这部分非常重要，模型参数使用Xavier初始化方式，基本思想是输入和输出的方差相同，包括前向传播和后向传播</span><br>    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters():<br>        <span class="hljs-keyword">if</span> p.dim() &gt; <span class="hljs-number">1</span>:<br>            nn.init.xavier_uniform(p)<br>    <span class="hljs-keyword">return</span> model<br><br></code></pre></td></tr></table></figure><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1"class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/342235515">参考资料1</a><a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2"class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/398039366">参考资料2</a><a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><ahref="https://hwcoder.top/Manual-Coding-3#transformer">参考资料3</a><a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><ahref="https://blog.csdn.net/shizheng_Li/article/details/131721198?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522172085940116800188568245%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=172085940116800188568245&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-131721198-null-null.142%5Ev100%5Epc_search_result_base1&amp;utm_term=transformer%E4%BB%A3%E7%A0%81&amp;spm=1018.2226.3001.4187">参考资料4</a><a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:5"class="footnote-text"><span><a href="https://github.com/msyJY/attention_relate">参考资料5</a><a href="#fnref:5" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><ahref="http://jalammar.github.io/illustrated-transformer/">参考资料6</a><a href="#fnref:6" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span><ahref="https://blog.csdn.net/Lian_Ge_Blog/article/details/132783696?spm=1001.2014.3001.5501">参考资料7</a><a href="#fnref:7" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Assignments</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>transformer</tag>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>assignment_2_DNN网络搭建</title>
    <link href="/2024/07/12/assignment-2-DNN%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA/"/>
    <url>/2024/07/12/assignment-2-DNN%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="dnn的定义">DNN的定义</h2><p>深度神经网络（Deep NeuralNetwork，简称DNN）是一种包含多个隐藏层的神经网络。与传统的浅层神经网络相比，DNN能够学习更复杂的数据特征和模式，这使得它们在处理高维度数据和解决复杂问题方面表现出色。</p><p><a href="https://imgse.com/i/pk4Bk8g"><imgsrc="https://s21.ax1x.com/2024/07/13/pk4Bk8g.png"alt="pk4Bk8g.png" /></a></p><h2 id="dnn原理">DNN原理</h2><p>DNN的原理基于以下几个关键概念：</p><ol type="1"><li><strong>层次结构</strong>：DNN由多个层次组成，包括输入层、多个隐藏层和输出层。每个层次包含多个神经元。<ul><li>多层次非线性变换：每一层神经元通过接收前一层的输出，并通过非线性激活函数（如sigmoid、ReLU等）进行处理，将原始数据逐步转化为更抽象和复杂的特征表示。在图像识别任务中，底层神经元可能学习到边缘、纹理等低级特征，而高层神经元则能够捕捉到更高级别的特征，如物体的部件和整体形态。</li></ul></li><li><strong>参数化</strong>：每个神经元与前一个层次的神经元通过权重连接，并且每个神经元有一个偏置项。</li><li><strong>前向传播</strong>：输入数据在网络中前向传递，每层的输出是下一层的输入。</li><li><strong>激活函数</strong>：在每层的输出上应用非线性激活函数，以引入非线性特性，使网络能够学习复杂的函数。</li><li><strong>损失函数</strong>：定义一个损失函数来衡量模型预测与真实值之间的差异，常见的损失函数包括均方误差（MSE）和交叉熵损失。</li><li><strong>反向传播</strong>：利用链式法则计算损失函数对每个参数的梯度，从而更新网络的权重和偏置。<ul><li>反向传播与优化：DNN的训练采用反向传播算法（Backpropagation）配合梯度下降（GradientDescent）或其他优化算法（如Adam、Adagrad等），通过计算损失函数相对于权重的梯度，更新网络权重以最小化损失函数，从而实现对训练数据的良好拟合。</li></ul></li><li><strong>优化算法</strong>：使用梯度下降或其变体（如Adam、RMSprop等）来更新网络的参数，以最小化损失函数。</li></ol><h2 id="dnn模型构建步骤">DNN模型构建步骤</h2><p>构建DNN模型通常遵循以下步骤：</p><ol type="1"><li><strong>定义网络结构</strong>：确定网络的层数、每层的神经元数量以及激活函数。</li><li><strong>初始化参数</strong>：为网络中的权重和偏置分配初始值，通常使用随机初始化。</li><li><strong>选择损失函数</strong>：根据任务类型（如分类或回归）选择合适的损失函数。</li><li><strong>选择优化器</strong>：选择一个优化算法来更新网络的参数。</li><li><strong>前向传播</strong>：在训练数据上执行前向传播，计算预测输出。</li><li><strong>计算损失</strong>：使用损失函数计算预测输出与真实标签之间的差异。</li><li><strong>反向传播</strong>：执行反向传播算法，计算损失相对于每个参数的梯度。</li><li><strong>参数更新</strong>：根据计算出的梯度和选择的学习率更新网络的参数。</li><li><strong>迭代训练</strong>：重复步骤5到8，直到模型在验证集上的性能不再显著提升或达到预定的迭代次数。</li><li><strong>评估和调优</strong>：在测试集上评估模型性能，根据需要调整网络结构或训练过程。</li><li><strong>应用模型</strong>：将训练好的模型应用于实际问题或数据。</li></ol><p>DNN模型的构建和训练是一个迭代和试错的过程，可能需要多次调整和优化以达到最佳性能。</p><h2 id="代码实现">代码实现</h2><p>下面用一个识别手写数字的例子来搭建深度神经网络（DNN）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets,transforms<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>torch.manual_seed(<span class="hljs-number">1024</span>)<br>transform = transforms.Compose([<br>    transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.5</span>,),(<span class="hljs-number">0.5</span>,))<br>])<br>train_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;./data&#x27;</span>,train=<span class="hljs-literal">True</span>,transform=transforms.ToTensor(),download=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># load test set</span><br>test_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;./data&#x27;</span>,train=<span class="hljs-literal">False</span>,transform=transforms.ToTensor(),download=<span class="hljs-literal">False</span>)<br>train_loader = DataLoader(train_dataset,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">True</span>)<br>test_loader = DataLoader(test_dataset,batch_size=<span class="hljs-number">64</span>,shuffle=<span class="hljs-literal">False</span>)<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, <span class="hljs-number">256</span>) <br>        <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>)<br>        <span class="hljs-variable language_">self</span>.fc3 = nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">64</span>)  <br>        <span class="hljs-variable language_">self</span>.fc4 = nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)   <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self,x</span>):<br>        x = torch.flatten(x,start_dim=<span class="hljs-number">1</span>)<br>        x = torch.relu(<span class="hljs-variable language_">self</span>.fc1(x))<br>        x = torch.relu(<span class="hljs-variable language_">self</span>.fc2(x))<br>        x = torch.relu(<span class="hljs-variable language_">self</span>.fc3(x))<br>        x = <span class="hljs-variable language_">self</span>.fc4(x)<br>        <span class="hljs-keyword">return</span> x<br>net = Net()<br><span class="hljs-built_in">print</span>(net)<br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.Adam(net.parameters(), lr=<span class="hljs-number">0.001</span>)<br>train_losses = []<br>train_accuracies = []<br>test_accuracies = []<br>epochs = <span class="hljs-number">10</span><br>best_accuracy = <span class="hljs-number">0.0</span><br>best_model_path = <span class="hljs-string">&#x27;best_mnist_path.pth&#x27;</span> <br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    running_loss = <span class="hljs-number">0.0</span><br>    correct_train = <span class="hljs-number">0</span> <br>    total_train = <span class="hljs-number">0</span>    <br><br>    <span class="hljs-comment"># 训练过程</span><br>    net.train()<br>    <span class="hljs-keyword">for</span> inputs,labels <span class="hljs-keyword">in</span> train_loader:<br>        optimizer.zero_grad()             <span class="hljs-comment"># 梯度清零</span><br>        outputs = net(inputs)             <span class="hljs-comment"># 前向传播</span><br>        loss = criterion(outputs,labels)  <span class="hljs-comment"># 计算损失</span><br>        loss.backward()                   <span class="hljs-comment"># 反向传播</span><br>        optimizer.step()                  <span class="hljs-comment"># 更新参数</span><br>        running_loss += loss.item()       <span class="hljs-comment"># 累加损失</span><br><br>        _,predicted = torch.<span class="hljs-built_in">max</span>(outputs,<span class="hljs-number">1</span>) <br>        total_train += labels.size(<span class="hljs-number">0</span>)      <span class="hljs-comment"># 累加样本数量</span><br>        correct_train += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br>    train_accuracy = correct_train / total_train<br>    train_losses.append(running_loss / <span class="hljs-built_in">len</span>(train_loader))<br>    train_accuracies.append((train_accuracy))<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span> / <span class="hljs-subst">&#123;epochs&#125;</span>, Loss: <span class="hljs-subst">&#123;running_loss / <span class="hljs-built_in">len</span>(train_loader):<span class="hljs-number">.4</span>f&#125;</span>,Train Accuracy :<span class="hljs-subst">&#123;train_accuracy:<span class="hljs-number">.2</span>%&#125;</span>&quot;</span>)<br>    net.<span class="hljs-built_in">eval</span>()<br>    correct = <span class="hljs-number">0</span><br>    total = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> inputs,labels <span class="hljs-keyword">in</span> test_loader:<br>            outputs = net(inputs)<br>            _,predicted = torch.<span class="hljs-built_in">max</span>(outputs,<span class="hljs-number">1</span>)<br>            total += labels.size(<span class="hljs-number">0</span>)<br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br>    test_accuracy = correct / total<br>    test_accuracies.append(test_accuracy)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>/<span class="hljs-subst">&#123;epochs&#125;</span>,Test Accuracy:<span class="hljs-subst">&#123;test_accuracy:<span class="hljs-number">.2</span>%&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">if</span> test_accuracy &gt; best_accuracy:<br>        best_accuracy = test_accuracy<br>        torch.save(net.state_dict(),best_model_path)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Best model saved with accuracy:<span class="hljs-subst">&#123;best_accuracy:<span class="hljs-number">.2</span>%&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Best Accuracy on test set:<span class="hljs-subst">&#123;best_accuracy:<span class="hljs-number">.2</span>%&#125;</span>&quot;</span>)<br>plt.figure(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">5</span>))<br>plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)                              <br>plt.plot(train_losses,label=<span class="hljs-string">&#x27;Training Loss&#x27;</span>)    <br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)                             <br>plt.ylabel(<span class="hljs-string">&#x27;Loss&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Training Loss over Epochs&#x27;</span>)          <br>plt.legend()                                   <br>plt.grid(<span class="hljs-literal">True</span>)                               <br>plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>plt.plot(train_accuracies,label=<span class="hljs-string">&#x27;Train Accuracy&#x27;</span>)<br>plt.plot(test_accuracies,label=<span class="hljs-string">&#x27;Test Accuracy&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Epoch&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Accuracy&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Train and Test Accuracy over Epochs&#x27;</span>)<br>plt.legend()<br>plt.grid(<span class="hljs-literal">True</span>)<br>plt.tight_layout()<br>plt.savefig(<span class="hljs-string">&#x27;loss_and_accuracy_curves.png&#x27;</span>)<br>plt.show()   <br></code></pre></td></tr></table></figure><p><ahref="https://fu-jingqi.github.io/2024/07/15/深度学习实践之手写数字识别/">实现细节注释</a></p><h2 id="参考">参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1"class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/693940030">参考资料1</a><a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2"class="footnote-text"><span><a href="https://hwcoder.top/Manual-Coding-2">参考资料2</a><a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Assignments</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>DNN网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>assignment_1_BP算法</title>
    <link href="/2024/07/12/assignment-1-BP%E7%AE%97%E6%B3%95/"/>
    <url>/2024/07/12/assignment-1-BP%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="算法简介">算法简介</h2><p>BP算法，全称为反向传播算法（Backpropagation），是训练人工神经网络（ArtificialNeuralNetwork，ANN）的一种有效且广泛应用的方法。它是一种监督学习算法，用于根据输入数据和对应的目标输出数据，调整神经网络的权重，使得网络能够对输入数据做出正确的预测。</p><h2 id="算法原理">算法原理</h2><p>BP算法，即误差反向传播算法（BackPropagation），是一种在神经网络中用于训练多层前馈神经网络的监督学习算法。它通过两个主要的过程来实现：正向传播和误差反向传播。</p><ul><li>正向传播：在这个阶段，输入数据在网络中向前流动，经过每一层的神经元，每一层的输出成为下一层的输入。每一层的节点输出是通过对输入进行加权求和后，通过一个激活函数进行非线性转换得到的。如果输出层的实际输出与期望输出不符，则进入下一个阶段。</li><li>误差反向传播：这个阶段是BP算法的核心，目的是调整网络中的权重和偏置，以减少输出误差。首先计算输出层的误差梯度，然后根据链式法则将这个误差反向传播回网络，逐层计算每个节点的误差贡献，并更新权重和偏置。这个过程涉及到梯度的计算，通常使用梯度下降法来最小化损失函数。</li></ul><p>BP算法的关键是确定隐含层节点的数量，这通常需要一些经验。有一个经验公式可以帮助确定隐含层节点数目，但实际应用中可能需要根据具体问题进行调整。</p><h2 id="算法步骤个人理解">算法步骤(个人理解)</h2><p>BP算法是神经网络中加速更新训练参数过程的一个算法，通过结合链式法则，从而减少计算机重复计算的次数，进而提高神经网络的训练速度，是损失函数快速收敛</p><p>假设构建一个如下图的神经网络：（第0层是输入层(2个神经元)，第1层是隐含层(3个神经元)，第2层是隐含层(2个神经元)，第3层是输出层。）</p><p><a href="https://imgse.com/i/pk4Q528"><imgsrc="https://s21.ax1x.com/2024/07/12/pk4Q528.png"alt="pk4Q528.png" /></a></p><p>我们抽出一条路径分析：</p><p><a href="https://imgse.com/i/pk4lsJ0"><imgsrc="https://s21.ax1x.com/2024/07/12/pk4lsJ0.png"alt="pk4lsJ0.png" /></a></p><ul><li>输入：<span class="math inline">\(X\)</span></li><li>线性变换：<span class="math inline">\(Y_i = f(X) = W_i * X +b_i\)</span></li><li>非线性变换：（<spanclass="math inline">\(sigmoid\)</span>函数）<spanclass="math inline">\(\sigma(x)=\frac{1}{1+e^{-x}}\)</span></li><li>损失函数<span class="math inline">\(Loss =loss(x,y)=1/n\sum(y_i-y&#39;_i)^2\)</span></li><li>真实值：<span class="math inline">\(y&#39;_i\)</span></li></ul><p>BP算法的实现涉及到多个步骤：</p><ul><li><p>初始化网络中的权重和偏置。</p></li><li><p>进行前向传播，计算输出值。</p><ul><li>input : <span class="math inline">\(X\)</span></li><li>线性变换1：<span class="math inline">\(Y_1 = W_1 * X +b_1\)</span></li><li>非线性变换1：<span class="math inline">\(Z_1 =\frac{1}{1+e^{-Y_1}}\)</span></li><li>线性变换2：<span class="math inline">\(Y_2 = W_2 * Z_1 +b_2\)</span></li><li>非线性变换2：<span class="math inline">\(Z_2 =\frac{1}{1+e^{-Y_2}}\)</span></li></ul></li><li><p>计算输出误差，并根据这个误差进行反向传播。</p><ul><li>损失函数计算：$Loss = 1/2 * (z_2-y’)^2 $</li></ul></li><li><p>使用链式法则计算每个权重对损失函数的贡献。</p><ul><li><span class="math inline">\(d(loss)/d(w_2) = d(Loss)/d(z_2) *d(z_2)/d(y_2) * d(y_2)/d(w_2)\)</span></li><li><span class="math inline">\(d(loss)/d(w_1) = ....\)</span></li><li><span class="math inline">\(d(loss)/d(b_2) = d(Loss)/d(z_2) *d(z_2)/d(y_2) * d(y_2)/d(b_2)\)</span></li><li><span class="math inline">\(d(loss)/d(b_1) = ....\)</span></li></ul></li><li><p>更新权重和偏置以减少误差。</p><ul><li><span class="math inline">\(w_i = w_i - \eta *d(loss)/d(w_i)\)</span></li><li><span class="math inline">\(b_i = b_i - \eta *d(loss)/d(b_i)\)</span></li></ul></li></ul><h2 id="算法实现">算法实现</h2><h3 id="法一模拟bp">法一：模拟BP：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment">## 随时函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Loss_func</span>(<span class="hljs-params">y1, y2</span>):<br>    <span class="hljs-keyword">return</span> np.<span class="hljs-built_in">sum</span>(np.square(y1 - y2) / <span class="hljs-number">2</span>)<br><br><span class="hljs-comment">## 激活函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-<span class="hljs-number">1</span> * x))<br><br><span class="hljs-comment">## 激活函数导数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid_ds</span>(<span class="hljs-params">x</span>):<br>    s = sigmoid(x)<br>    <span class="hljs-keyword">return</span> s * (np.ones(s.shape) - s)<br><br><span class="hljs-comment">## 损失函数导数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">LossFunc_ds</span>(<span class="hljs-params">y1, y2</span>):<br>    <span class="hljs-keyword">return</span> y1 - y2<br><br><span class="hljs-comment">## 前向传播过程</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">w1, w2, b1, b2, x, y</span>):<br>    y1 = np.matmul(x, w1) + b1<br>    z1 = sigmoid(y1)<br>    y2 = np.matmul(z1, w2) + b2<br>    z2 = sigmoid(y2)<br>    loss = Loss_func(z2, y)<br>    <span class="hljs-keyword">return</span> y1, z1, y2, z2, loss<br> <br><span class="hljs-comment">## BP算法</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward_update</span>(<span class="hljs-params">epochs, learn_rate</span>):<br>    x = np.random.random((<span class="hljs-number">2</span>,<span class="hljs-number">2</span>))<br>    y = np.random.randint((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))<br>    w1 = np.random.random((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>))<br>    b1 = np.random.random((<span class="hljs-number">1</span>,<span class="hljs-number">3</span>))<br>    w2 = np.random.random((<span class="hljs-number">3</span>,<span class="hljs-number">2</span>))<br>    b2 = np.random.random((<span class="hljs-number">1</span>,<span class="hljs-number">2</span>))<br>    <span class="hljs-comment"># 前向传播</span><br>    y1, z1, y2, z2, loss = forward(w1, w2, b1, b2, x, y)<br>    <span class="hljs-comment"># 开始迭代</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        ds2 = LossFunc_ds(z2, y) * sigmoid_ds(y2)<br>        <span class="hljs-comment"># 将z1看作输入dw2 = dL/dw2 = dL/dy2 * dy2/dw2 = dL/dz2 * dz2/dy2 * dy2/dw2</span><br>        dw2 = np.matmul(z1.T, ds2)<br>        <span class="hljs-comment"># db2 = dL/db2 = dL/dy2 * dy2/db2 = dL/dz2 * dz2/dy2 * dy2/db2(1)</span><br>        <span class="hljs-comment"># 偏置值求和，将每个神经元的误差合并得到总的偏置误差梯度</span><br>        db2 = np.<span class="hljs-built_in">sum</span>(ds2, axis=<span class="hljs-number">0</span>)<br>        dx = np.matmul(ds2, w2.T)<br>        ds1 = sigmoid_ds(y1) * dx<br>        dw1 = np.matmul(x.T, ds1)<br>        db1 = np.<span class="hljs-built_in">sum</span>(ds1, axis=<span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 更新偏置值和系数</span><br>        w1 = w1 - learn_rate * dw1<br>        b1 = b1 - learn_rate * db1<br>        w2 = w2 - learn_rate * dw2<br>        b2 = b2 - learn_rate * db2<br><br>        y1, z1, y2, z2, loss = forward(w1, w2, b1, b2, x, y)<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练轮数：&#x27;</span> , i / <span class="hljs-number">100</span>)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;当前损失：&#123;:.4f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(loss))<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;------&quot;</span>)<br>            <span class="hljs-built_in">print</span>(z2)<br>            <span class="hljs-comment"># sigmoid激活函数将结果大于0.5的值分为正类，小于0.5的值分为负类</span><br>            z2[z2 &gt; <span class="hljs-number">0.5</span>] = <span class="hljs-number">1</span><br>            z2[z2 &lt; <span class="hljs-number">0.5</span>] = <span class="hljs-number">0</span><br>            <span class="hljs-built_in">print</span>(z2)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;------------------------------&quot;</span>)<br>            <br><span class="hljs-comment">## 测试算法</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    backward_update(<span class="hljs-number">30001</span>, <span class="hljs-number">0.02</span>)<br></code></pre></td></tr></table></figure><h3id="法二使用类来定义一个神经网络并实现bp">法二：使用类来定义一个神经网络并实现BP</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BackPropagation</span>:<br>    <span class="hljs-comment"># 初始化各层数量</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_n, hidden_n, output_n</span>):<br>        <span class="hljs-variable language_">self</span>.input_n = input_n<br>        <span class="hljs-variable language_">self</span>.hidden_n = hidden_n<br>        <span class="hljs-variable language_">self</span>.output_n = output_n<br>        <span class="hljs-comment"># 随机初始化权重和阈值</span><br>        <span class="hljs-variable language_">self</span>.i_h_weight = np.random.rand(<span class="hljs-variable language_">self</span>.input_n, <span class="hljs-variable language_">self</span>.hidden_n)<br>        <span class="hljs-variable language_">self</span>.h_o_weight = np.random.rand(<span class="hljs-variable language_">self</span>.hidden_n, <span class="hljs-variable language_">self</span>.output_n)<br>        <span class="hljs-variable language_">self</span>.h_threshold = np.random.rand(<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.hidden_n)<br>        <span class="hljs-variable language_">self</span>.o_threshold = np.random.rand(<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.output_n)<br> <br>    <span class="hljs-comment"># 激活函数sigmoid function</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br> <br>    <span class="hljs-comment"># sigmiod函数求导</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid_derivative</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> - x)<br> <br>    <span class="hljs-comment"># 向前传播forward propagation</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_propagation</span>(<span class="hljs-params">self, data_train</span>):<br>        <span class="hljs-comment"># 输入层到隐藏层</span><br>        hiddens_input = np.dot(data_train, <span class="hljs-variable language_">self</span>.i_h_weight) - <span class="hljs-variable language_">self</span>.h_threshold<br>        hiddens_output = <span class="hljs-variable language_">self</span>.sigmoid(hiddens_input)<br>        <span class="hljs-comment"># 隐藏层到输出层</span><br>        outputs_input = np.dot(hiddens_output, <span class="hljs-variable language_">self</span>.h_o_weight) - <span class="hljs-variable language_">self</span>.o_threshold<br>        outputs_output = <span class="hljs-variable language_">self</span>.sigmoid(outputs_input)<br>        <span class="hljs-keyword">return</span> hiddens_output, outputs_output<br> <br> <br>    <span class="hljs-comment"># 向后传播back propagation</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backpropagation</span>(<span class="hljs-params">self, hiddens_output, outputs_output, data_train, data_labels, learning_rate</span>):<br>        <span class="hljs-comment"># 计算输出层的误差</span><br>        output_error = data_labels - outputs_output<br>        output_delta = output_error * <span class="hljs-variable language_">self</span>.sigmoid_derivative(outputs_output)<br> <br>        <span class="hljs-comment"># 计算隐藏层的误差</span><br>        hidden_error = output_delta.dot(<span class="hljs-variable language_">self</span>.h_o_weight.T)<br>        hidden_delta = hidden_error * <span class="hljs-variable language_">self</span>.sigmoid_derivative(hiddens_output)<br> <br>        <span class="hljs-comment"># 更新隐藏层到输出层的权重和阈值</span><br>        <span class="hljs-variable language_">self</span>.h_o_weight += hiddens_output.T.dot(output_delta) * learning_rate<br>        <span class="hljs-variable language_">self</span>.o_threshold -= np.<span class="hljs-built_in">sum</span>(output_delta, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">True</span>) * learning_rate<br> <br>        <span class="hljs-comment"># 更新输入层到隐藏层的权重和阈值</span><br>        <span class="hljs-variable language_">self</span>.i_h_weight += np.dot(data_train.T, hidden_delta) * learning_rate<br>        <span class="hljs-variable language_">self</span>.h_threshold -= np.<span class="hljs-built_in">sum</span>(hidden_delta, axis=<span class="hljs-number">0</span>, keepdims=<span class="hljs-literal">True</span>) * learning_rate<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fit</span>(<span class="hljs-params">self, data_train, data_labels, epochs=<span class="hljs-number">10</span>, learning_rate=<span class="hljs-number">0.01</span></span>):<br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>            hiddens_output, outputs_output = <span class="hljs-variable language_">self</span>.forward_propagation(data_train)<br>            <span class="hljs-variable language_">self</span>.backpropagation(hiddens_output, outputs_output, data_train, data_labels, learning_rate)<br>        _, output = <span class="hljs-variable language_">self</span>.forward_propagation(data_train)<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><h2 id="bp算法的局限性">BP算法的局限性</h2><h3 id="收敛时是局部最小值">收敛时是局部最小值</h3><p>当有不止一个极小值时，参数调整到最近的极小值便停止更新了，而该极小值未必全局最优</p><h3 id="梯度消失">梯度消失</h3><p>梯度消失原因是链式求导，导致梯度逐层递减，我们BP第一节推倒公式时就是通过链式求导把各层连接起来的，但是因为激活函数是sigmod函数，取值在1和-1之间，因此每次求导都会比原来小，当层次较多时，就会导致求导结果也就是梯度接近于0</p><h2 id="参考">参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><ahref="https://blog.csdn.net/weixin_42398658/article/details/83929474?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522172078496016800207024926%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=172078496016800207024926&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-7-83929474-null-null.142%5Ev100%5Epc_search_result_base1&amp;utm_term=BP%E7%AE%97%E6%B3%95&amp;spm=1018.2226.3001.4187">参考资料1</a><a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><ahref="https://blog.csdn.net/m0_74086448/article/details/132007507?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522172078496016800207024926%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=172078496016800207024926&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-132007507-null-null.142%5Ev100%5Epc_search_result_base1&amp;utm_term=BP%E7%AE%97%E6%B3%95&amp;spm=1018.2226.3001.4187">参考资料2</a><a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Assignments</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>BP算法</tag>
      
      <tag>算法理解</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/07/12/hello-world/"/>
    <url>/2024/07/12/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
